---
layout: post
title: Norma L1 y L2
date: 2023-06-10 20:30
description: Explicación de norma L1 y L2, y su uso en Linear Regression
categories: algoritmos
giscus_comments: true
related_posts: false
---

# Regularización y las normas L1 y L2

*Información obtenida de este artículo de Chiara Campagnola https://towardsdatascience.com/visualizing-regularization-and-the-l1-and-l2-norms-d962aa769932*

## Cómo minimizar una norma produce la regularización

En linear regression, el modelo es el siguiente:
$$
\hat{y} = w \cdot x + b
$$
y la función de costo a minimizar es MSE:
$$
MSE = \frac{1}{m}\sum^{m}_{i=1}(y-\hat{y})^2
$$

El modelo que usaremos para hacer la predicción tiene que ser lo suficientemente complejo para ajustarse a la data, pero no tanto como para que produzca overfitting. Al añadir un término de regularización, prevenimos que los parámetros $$w$$ sean muy grandes (en valor absoluto), simplificando el modelo.

El término de regularización puede ser la norma $$L1$$ o $$L2$$ al cuadrado, y se agrega a la función de costo, por lo que durante el entrenamiento, el algoritmo de aprendizaje (linear regression en este caso), intentará minimizar la función de costo con el término de regularización.

$$
\text{norma L1:} ||w||_1 = \sum^{n}_{i}|w_i|
$$
$$
\text{normal L2 al cuadrado:} ||w||_2^2 = \sum^{n}_{i}w_i^2
$$


**Minimizar la norma incentiva a la función a ser menos compleja**.

Matemáticamente, las normas $$L1$ y $$L2$$ son medidas de la magnitud de los weights: la suma de los valores absolutos (Norma $$L1$$) y la suma de los valores al cuadrado (Norma $$L2$$). Valores más grandes de weights dan una norma más grande. Esto significa que, minimizar la norma incentiva a los weights a ser más pequeños (en valor absoluto), lo que a su vez da funciones más simples.

* Ejemplos de modelos:
- polinomio grado 8 se ajusta muy bien a la data, obtiene error 0, sin embargo, sus normas son bastantes altas (L2=32 y L1 = 70)
    - Esto se refleja en el modelo: $$f(x)=0.04+0.04x+0.9x^2$$
    - Este modelo producirá overfitting
- polinomio grado 2 se ajusta bien a la data, obtiene un error dde 0.006, pero sus normas son muy pequeñas (L2=0.9 y L1=0.98).
    - Esto se refleja en el modelo: $$f(x)=-0.01+0.57x+2.67x^2-4.08x^3-12.25x^4+7.41x^5+24.87x^6-3.79x^7-14.38x^8$$
    - Este modelo generalizará mejor.

Con esto se concluye que mientras menor sea la norma, más pequeños los parámetros y más simple el modelo.

Dependiendo de la norma que usemos, habrán distintos efectos, ya sea, eliminar algunos weights completamente (estableciéndolos a 0), o haciendo que todos los weights sean lo más pequeños posibles. El primero lo genera la norma $$L1$$, mientras que el segundo lo hará la norma $$L2$$ al cuadrado.

To understand how they operate differently, let’s have a look at how they change depending on the value of the weights.

Veamos como cambian las normas dependiendo del valor de los weights.
<div class="row mt-3">
    <div class="col-sm mt-3 mt-md-0">
        {% include figure.html path="assets/img/derivada_l1.png" class="img-fluid rounded z-depth-1" %}
    </div>
</div>
<div class="caption">
    Gráfica que muestra a $$|w|$$ y la derivada de $$|w|=\frac{w \cdot w'}{|w|}$$. Podemos observar que la pendiente es siempre la misma (un valor constante).
</div>

<div class="row mt-3">
    <div class="col-sm mt-3 mt-md-0">
        {% include figure.html path="assets/img/derivada_l2.png" class="img-fluid rounded z-depth-1" %}
    </div>
</div>
<div class="caption">
    Gráfica que muestra $$w^2$$ y la derivada de ésta que es $$2w$$. Recordar que es la norma L2 al cuadrado la que se utiliza. Podemos observar que la pendiente es muy grande al principio, pero a medida que $$w$$ disminuye, la pendiente se hace cada vez más pequeña.
</div>

Las normas $$L1$$ y $$L2$$ aumentan a medida que aumenta el valor absoluto de $$w$$. Sin embargo, mientras que la norma $$L1$$ aumenta a un valor constante, la norma $$L2$$ aumenta más rápidamente.

Cuando utilizamos el gradient descent actualizaremos nuestros weights basándonos en la derivada la función de costo. Entnces si incluimos una norma en nuestra función de costo, la derivada de la norma determinará cómo serán actualizados los weights.
- Con la norma $$L2$$ a medida que $$w$$ se vuelve más pequeño, también lo hace la pendiente de la norma, lo que significa que las actualizaciones también serán cada vez más pequeñas. Cuando los weigths estén cerca de 0, las actualizaciones se habrán vuelto tan pequeñas que serán casi insignificantes, por lo que es poco probable que los pesos lleguen a ser 0.
- Con la norma $$L1$$ la pendiente es constante. Esto significa que a medida que w se reduce, las actualizaciones no cambian. Por consiguiente, es mucho más probable que la norma $$L1$$ reduzca algunos weights a 0.

---
*Obtenido de artículo escrito por Saptashwa Bhattacharyya https://towardsdatascience.com/ridge-and-lasso-regression-a-complete-guide-with-python-scikit-learn-e20e34bcbf0b* 

Ridge y Lasso regression son técnicas simples para reducir la complejidad del modelo y prevenir el overfitting.

## Ridge regression
En Ridge regression, la función de costo es alterada agregando un castigo equivalente al cuadrado de la magnitud de los coeficientes. En otras palabras, si agregamos la norma $$L2$$ al cuadrado a la loss y la minimizamos, obtendremos la función de costo de Ridge regression:
$$
min L(w) = min(\frac{1}{m}\sum^{m}_{i=1}(y-\hat{y})^2 + \lambda ||w||_2^2)
$$
$$\lambda$$ es el coeficiente de regularización que determina cuánta regularización queremos.

Entonces, Ridge regression pone restricciones en los coeficientes $$w$$ en caso de que los coeficientes tomen valores muy grandes. **Ridge regression solamente puede reducir los coeficientes cerca de 0, pero nunca 0**.

## Lasso regression
La función de costo para Lasso (*least absolute shrinkage and selection operator*) regression es:
$$
min L(w) = min(\frac{1}{m}\sum^{m}_{i=1}(y-\hat{y})^2 + \lambda ||w||_1)
$$

Lasso regression también simplifica el modelo, la diferencia es que en lugar de tomar el cuadrado de los coeficientes, solo tomamos las magnitudes de los parámetros. Este tipo de regularización puede conducir a coeficientes con valor 0, es decir, tienen nulo aporte en la evaluación de la salida. **Lasso regression no solo ayuda en reducir el overfitting sino que también puede ayudarnos en feature selection**. Feature selection es cuando queremos seleccionar ciertas features, sólo las más importantes.
