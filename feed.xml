<?xml version="1.0" encoding="utf-8"?><feed xmlns="http://www.w3.org/2005/Atom" xml:lang="en"><generator uri="https://jekyllrb.com/" version="4.3.2">Jekyll</generator><link href="https://squirogar.github.io/feed.xml" rel="self" type="application/atom+xml" /><link href="https://squirogar.github.io/" rel="alternate" type="text/html" hreflang="en" /><updated>2023-06-19T01:28:25+00:00</updated><id>https://squirogar.github.io/feed.xml</id><title type="html">blank</title><subtitle>A simple, whitespace theme for academics. Based on [*folio](https://github.com/bogoli/-folio) design.
</subtitle><entry><title type="html">Reinforcement Learning y control</title><link href="https://squirogar.github.io/blog/2023/reinforcement-learning/" rel="alternate" type="text/html" title="Reinforcement Learning y control" /><published>2023-06-10T20:30:00+00:00</published><updated>2023-06-10T20:30:00+00:00</updated><id>https://squirogar.github.io/blog/2023/reinforcement-learning</id><content type="html" xml:base="https://squirogar.github.io/blog/2023/reinforcement-learning/"><![CDATA[<p><em>Material obtenido del e-book de matlab de Reinforcement Learning</em></p>

<h1 id="reinforcement-learning-rl">Reinforcement Learning (RL)</h1>
<p>Permite resolver problemas muy difíciles de control. Por ejemplo, un robot caminante. Un robot caminante es muy difícil de lograr con el enfoque de control, ya que necesitaríamos muchos loops de control, sensores, controladores de motor, etc.</p>

<p>En RL, trabajamos con un entorno dinámico, a diferencia del Supervised learning y el Unsupervised learning que son datasets estáticos.</p>

<div class="row mt-3">
    <div class="col-sm mt-3 mt-md-0">
        <figure>

  <picture>
    
    <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/ul_sl_rl-480.webp" />
    <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/ul_sl_rl-800.webp" />
    <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/ul_sl_rl-1400.webp" />
    

    <!-- Fallback to the original file -->
    <img src="/assets/img/ul_sl_rl.png" class="img-fluid rounded z-depth-1" width="auto" height="auto" onerror="this.onerror=null; $('.responsive-img-srcset').remove();" />

  </picture>

</figure>

    </div>
</div>
<div class="caption">
    Unsupervised learning, Supervised learning y Reinforcement Learning se utilizan para problemas diferentes. El primero para encontrar patrones ocultos interesantes, el segundo para etiquetar correctamente los datos, y el tercero se utiliza para encontrar la mejor secuencia de acciones que generan la salida óptima.
</div>

<p>RL consiste en un agente que se relaciona con un entorno y va tomando acciones que afectan a ese entorno. Dicho entorno cambia de estado tras cada acción que tome el agente y además le proporciona una recompensa al agente por cada acción que ejecute. Estas acciones o decisiones pueden ser buenas o malas, y dependiendo de ésto va a recibir un recompensa buena o mala respectivamente. Cabe destacar que <strong>al agente nunca se le dice qué hacer, sino que él mismo debe descubrir qué acciones tomar, de tal forma que produzca la mayor recompensa a largo plazo</strong>.</p>

<blockquote>
  <p>Con RL, queremos encontrar la mejor secuencia de acciones que generarán la salida óptima, o sea, la que genera la mejor recompensa final. La idea es maximizar esta recompensa a largo plazo.</p>
</blockquote>

<blockquote>
  <p>El agente es un software que explora, interactúa y aprende del entorno</p>
</blockquote>

<p>Ejemplo de reinforcement learning:</p>
<ol>
  <li>agente: ser humano</li>
  <li>entorno: ciudad en la que vive el agente</li>
  <li>acción 1: el agente mira a ambos lados para cruzar la calle</li>
  <li>Luego de la acción 1, el entorno cambia de estado y el agente obtiene una recompensa:
    <ul>
      <li>observaciones de estado: el agente se encuentra en la otra vereda de la calle de la ciudad</li>
      <li>recompensa: no resultó herido al cruzar la calle.</li>
    </ul>
  </li>
</ol>

<h2 id="i-funcionamiento-de-rl">I. Funcionamiento de RL</h2>
<p>El procedimiento es el siguiente:</p>

<ul>
  <li>Paso 1: Agente observa el estado actual del entorno</li>
</ul>
<div class="row mt-3">
    <div class="col-sm mt-3 mt-md-0">
        <figure>

  <picture>
    
    <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/paso1-480.webp" />
    <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/paso1-800.webp" />
    <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/paso1-1400.webp" />
    

    <!-- Fallback to the original file -->
    <img src="/assets/img/paso1.png" class="img-fluid rounded z-depth-1" width="auto" height="auto" onerror="this.onerror=null; $('.responsive-img-srcset').remove();" />

  </picture>

</figure>

    </div>
</div>

<ul>
  <li>Paso 2: Decide cual acción tomar dependiendo del estado</li>
</ul>
<div class="row mt-3">
    <div class="col-sm mt-3 mt-md-0">
        <figure>

  <picture>
    
    <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/paso2-480.webp" />
    <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/paso2-800.webp" />
    <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/paso2-1400.webp" />
    

    <!-- Fallback to the original file -->
    <img src="/assets/img/paso2.png" class="img-fluid rounded z-depth-1" width="auto" height="auto" onerror="this.onerror=null; $('.responsive-img-srcset').remove();" />

  </picture>

</figure>

    </div>
</div>

<ul>
  <li>Paso 3: Entorno cambia de estado y produce una recompensa por la acción ejecutada</li>
</ul>
<div class="row mt-3">
    <div class="col-sm mt-3 mt-md-0">
        <figure>

  <picture>
    
    <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/paso3-480.webp" />
    <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/paso3-800.webp" />
    <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/paso3-1400.webp" />
    

    <!-- Fallback to the original file -->
    <img src="/assets/img/paso3.png" class="img-fluid rounded z-depth-1" width="auto" height="auto" onerror="this.onerror=null; $('.responsive-img-srcset').remove();" />

  </picture>

</figure>

    </div>
</div>

<ul>
  <li>Paso 4: ¿Fueron buenas las acciones o malas?
    <ul>
      <li>buenas: repetirlas</li>
      <li>malas: evitarlas
  Repetir hasta terminar el aprendizaje.</li>
    </ul>
  </li>
</ul>
<div class="row mt-3">
    <div class="col-sm mt-3 mt-md-0">
        <figure>

  <picture>
    
    <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/paso4-480.webp" />
    <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/paso4-800.webp" />
    <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/paso4-1400.webp" />
    

    <!-- Fallback to the original file -->
    <img src="/assets/img/paso4.png" class="img-fluid rounded z-depth-1" width="auto" height="auto" onerror="this.onerror=null; $('.responsive-img-srcset').remove();" />

  </picture>

</figure>

    </div>
</div>

<h2 id="ii-política">II. Política</h2>
<p>El agente internamente toma las observaciones del estado del entorno y las asigna a acciones realizando así un mapeo. En otras palabras, se puede entender este mapeo como una función que recibe entradas y genera una salida. A este mapeo se le llama <strong>política</strong>. La política es muy importante, ya que decide qué acción ejecutar dado un conjunto de observaciones de estado. Básicamente la política es el <em>cerebro</em> de nuestro agente, y le va a indicar qué hacer. La política se puede representar de varias formas, una forma muy útil es que si tenemos observaciones de estado complejas como por ejemplo, imágenes, entonces podemos usar una red neuronal para procesar dichos datos.</p>

<div class="row mt-3">
    <div class="col-sm mt-3 mt-md-0">
        <figure>

  <picture>
    
    <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/politica-480.webp" />
    <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/politica-800.webp" />
    <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/politica-1400.webp" />
    

    <!-- Fallback to the original file -->
    <img src="/assets/img/politica.png" class="img-fluid rounded z-depth-1" width="auto" height="auto" onerror="this.onerror=null; $('.responsive-img-srcset').remove();" />

  </picture>

</figure>

    </div>
</div>
<div class="caption">
    La política es la función que toma observaciones y genera acciones que influyen sobre el entorno.
</div>

<p>Ejemplo de política en un robot caminante:</p>
<ol>
  <li>Observaciones: ángulo de cada articulación, aceleración, velocida angular del tronco y los miles de pixeles de un sensor de visión.</li>
  <li>Política: toma las observaciones y genera comandos de acción que mueven los brazos y piernas del robot.</li>
  <li>Recompensa: qué tan bien funcionaron la combinación de comandos: ¿Se cayó el robot?, ¿Se desvió del camino?, ¿Se está arrastrando? ¿Esta caminando erguido? Etc.</li>
</ol>

<div class="row mt-3">
    <div class="col-sm mt-3 mt-md-0">
        <figure>

  <picture>
    
    <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/politica_robot-480.webp" />
    <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/politica_robot-800.webp" />
    <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/politica_robot-1400.webp" />
    

    <!-- Fallback to the original file -->
    <img src="/assets/img/politica_robot.png" class="img-fluid rounded z-depth-1" width="auto" height="auto" onerror="this.onerror=null; $('.responsive-img-srcset').remove();" />

  </picture>

</figure>

    </div>
</div>
<div class="caption">
    RL en un robot caminante.
</div>

<h2 id="iii-algoritmo-de-reinforcement-learning">III. Algoritmo de Reinforcement Learning</h2>
<p>La política debe ajustarse, no puede ser estática, ya que el entorno es dinámico; para esto existen los <strong>algoritmos de Reinforcement Learning</strong>. Un algoritmo de RL hace óptima a la política, cambiandola en función de las acciones que se tomaron, las observaciones del estado del entorno y la cantidad de recompensa recolectada.</p>

<blockquote>
  <p>Un agente de RL utiliza un algoritmo de RL para modificar su política a medida que interactúa con el entorno, de modo que eventualmente, dado cualquier estado, siempre tomará la mejor acción: la que producirá la mayor recompensa a largo plazo.</p>
</blockquote>

<div class="row mt-3">
    <div class="col-sm mt-3 mt-md-0">
        <figure>

  <picture>
    
    <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/agente_alg-480.webp" />
    <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/agente_alg-800.webp" />
    <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/agente_alg-1400.webp" />
    

    <!-- Fallback to the original file -->
    <img src="/assets/img/agente_alg.png" class="img-fluid rounded z-depth-1" width="auto" height="auto" onerror="this.onerror=null; $('.responsive-img-srcset').remove();" />

  </picture>

</figure>

    </div>
</div>
<div class="caption">
    Algoritmos RL + la política.
</div>

<h2 id="iv-cómo-un-agente-aprende-del-entorno">IV. ¿Cómo un agente aprende del entorno?</h2>
<p>Una política es una función compuesta por parámetros lógicos y ajustables. Para una estructura de política dada, existe un conjunto de parámetros que producirán una política óptima, o sea, un mapeo de los estados a las acciones que producen la recompensa más grande a largo plazo.</p>

<blockquote>
  <p>El aprendizaje es el término que se le da al proceso de ajustar sistemáticamente esos parámetros para converger en la política óptima.</p>
</blockquote>

<p>El agente usa la información que obtiene del entorno (observaciones del estado del entorno y recompensa) para ajustar sus acciones a futuro. La recompensa es muy importante porque le va a decir al agente qué tan buena fue la acción que acaba de realizar.</p>

<h2 id="v-valor-y-recompensa-value-and-reward">V. Valor y recompensa (Value and reward)</h2>
<ul>
  <li>Valor (value): recompensa total que un agente puede esperar recibir desde ese estado en adelante.</li>
  <li>Recompensa (reward): beneficio inmediato de estar en un estado y realizar una acción específica.</li>
</ul>

<h3 id="por-qué-es-importante-el-valor">¿Por qué es importante el valor?</h3>
<p>Evaluar el valor de un estado o una acción en vez de la recompensa inmediata ayuda al agente a elegir la acción que obtendrá la mayor recompensa a lo largo del tiempo, en vez de un beneficio a corto plazo.</p>

<p>Supongamos que tenemos la siguiente situación:</p>
<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>+1  agente  -1  -1  +10
</code></pre></div></div>
<p>¿Dónde debe ir el agente? Si va al estado de la izquierda obtendrá un beneficio instantáneo. Pero, si va a la derecha obtendrá un beneficio a futuro mayor de +8.</p>

<p>Ahora, tenemos la siguiente situación en la que sólo podemos dar 2 pasos:</p>
<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>                    agente
                      |
Valor             0   |     +4
Recompensa   -1  +1   0     -1    +5
Estado       s0  s1   s2    s3    s4
</code></pre></div></div>
<p>El valor para el estado s3 es +4 porque se espera recibir desde aquí en adelante +4 de recompensa total.</p>

<ul>
  <li>
    <p>Si se elige en base a la recompensa:</p>

    <div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>  1. agente va a s1 ---&gt; recompensa_total = +1
  2. agente vuelve a s2 ---&gt; recompensa_total = +1
</code></pre></div>    </div>
  </li>
  <li>
    <p>Si se elige en base a al valor estimado de un estado:</p>

    <div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>  1. Agente va a s3 ---&gt; recompensa_total = -1
  2. Agente va a s4 ---&gt; recompensa_total = +4
</code></pre></div>    </div>
  </li>
</ul>

<p>No obstante, elegir a corto plazo igual puede servir:</p>
<ul>
  <li>Recompensa inmediata puede ser mejor que esperar por una futura que implique una secuencia de varios pasos.</li>
  <li>Predicción de recompensas puede fallar y esa recompensa alta puede que no esté cuando lleguemos a esos estados, o sea, existe mayor incertidumbre.</li>
  <li>La solución a esto es descontar recompensas mientras más lejos estén el futuro. Esto se hace estableciendo el factor de descuento \(\gamma\) entre 0 y 1:</li>
</ul>

<div class="row mt-3">
    <div class="col-sm mt-3 mt-md-0">
        <figure>

  <picture>
    
    <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/rec_descontada-480.webp" />
    <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/rec_descontada-800.webp" />
    <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/rec_descontada-1400.webp" />
    

    <!-- Fallback to the original file -->
    <img src="/assets/img/rec_descontada.png" class="img-fluid rounded z-depth-1" width="auto" height="auto" onerror="this.onerror=null; $('.responsive-img-srcset').remove();" />

  </picture>

</figure>

    </div>
</div>
<div class="caption">
    Descuento de recompensas futuras.
</div>

<p>¿Y qué pasa cuando hay estados que no se conocen? Cabe la posibilidad de que existan estados que sean desconocidos para el agente, pero éstos pueden contener recompensas mayores a las de los estados que actualmente conocemos. Es aquí donde entran los dos enfoques que puede aplicar el agente.</p>

<h3 id="enfoque-muy-codicioso-explotación-del-entorno">Enfoque muy codicioso: explotación del entorno</h3>
<p>Recolectamos la mayor cantidad de recompensas que se conozcan, o sea, las más cercanas. Le damos más relevancia al beneficio inmediato que al futuro.</p>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>+1  agente  -1  ?  ?
</code></pre></div></div>
<p>El agente irá a la izquierda.</p>

<h3 id="enfoque-poco-codicioso-exploración-del-entorno">Enfoque poco codicioso: exploración del entorno</h3>
<p>Exploramos estados desconocidos del entorno con la esperanza de obtener mejores recompensas y por consiguiente, una mejor recompensa a largo plazo. Sin embargo, corremos el riesgo de recolectar peores recompensas por algún tiempo, o que incluso descubramos que estas recompensas no sean tan buenas como las que conocíamos.</p>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>+1  agente  -1  -1  -1   10
</code></pre></div></div>
<p>El agente irá a la derecha.</p>

<h3 id="explotación-vs-exploración">Explotación vs Exploración</h3>
<p>El algoritmo de RL explorará o explotará el espacio de estados, convirtiéndose esto en un problema de optimización</p>

<p>Si bien explorar para obtener una gran recompensa en el futuro puede ser muy tentador a elegir, puede que no sea tan buena opción, esto se debe a que es posible que:</p>
<ol>
  <li>La recompensa actual es mayor que la recompensa después</li>
  <li>Las recompensas en el futuro son menos confiables, ya que podrían no estar allí cuando se alcancen, por lo que no hay que confiar 100% en la predicción de recompensa.</li>
</ol>

<blockquote>
  <p>RL descuenta las recompensas en una cantidad mayor cuanto más lejos estén en el futuro.</p>
</blockquote>

<blockquote>
  <p>El algoritmo de RL establece un equilibrio entre exploración y explotación. Este trade-off se da mientras el agente interactúa con el entorno.</p>
</blockquote>

<h2 id="vi-control-de-sistemas-y-rl">VI. Control de sistemas y RL</h2>
<p>Tenemos un sistema o proceso industrial que queremos controlar. Controlamos las entradas del sistema (acciones) para intentar generar las salidas deseadas (comportamientos).</p>

<div class="row mt-3">
    <div class="col-sm mt-3 mt-md-0">
        <figure>

  <picture>
    
    <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/control-480.webp" />
    <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/control-800.webp" />
    <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/control-1400.webp" />
    

    <!-- Fallback to the original file -->
    <img src="/assets/img/control.png" class="img-fluid rounded z-depth-1" width="auto" height="auto" onerror="this.onerror=null; $('.responsive-img-srcset').remove();" />

  </picture>

</figure>

    </div>
</div>
<div class="caption">
    Sistema de control.
</div>

<p>Controlamos las entradas mediante un software controlador.</p>

<p>Utilizamos las observaciones del estado (retroalimentación) para mejorar el rendimiento y corregir errores y perturbaciones aleatorias.</p>

<div class="row mt-3">
    <div class="col-sm mt-3 mt-md-0">
        <figure>

  <picture>
    
    <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/lazo_cerrado-480.webp" />
    <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/lazo_cerrado-800.webp" />
    <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/lazo_cerrado-1400.webp" />
    

    <!-- Fallback to the original file -->
    <img src="/assets/img/lazo_cerrado.png" class="img-fluid rounded z-depth-1" width="auto" height="auto" onerror="this.onerror=null; $('.responsive-img-srcset').remove();" />

  </picture>

</figure>

    </div>
</div>
<div class="caption">
    Un sistema de control en lazo cerrado utiliza la retroalimentación para ajustar las acciones.
</div>

<p>Si el problema es muy complejo, utilizamos múltiples loops de control andidados, motores, sensores y controladores, lo que es muy difícil de implementar y modificar.</p>

<div class="row mt-3">
    <div class="col-sm mt-3 mt-md-0">
        <figure>

  <picture>
    
    <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/loops-480.webp" />
    <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/loops-800.webp" />
    <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/loops-1400.webp" />
    

    <!-- Fallback to the original file -->
    <img src="/assets/img/loops.png" class="img-fluid rounded z-depth-1" width="auto" height="auto" onerror="this.onerror=null; $('.responsive-img-srcset').remove();" />

  </picture>

</figure>

    </div>
</div>
<div class="caption">
    Este sería el controlador para un robot. Estos loops interactúan entre sí.
</div>

<p>El controlador tiene que encontrar la combinación de comandos o acciones que hacen que el sistema de comporte correctamente, incluso frente a perturbarciones.</p>

<div class="row mt-3">
    <div class="col-sm mt-3 mt-md-0">
        <figure>

  <picture>
    
    <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/robot_control-480.webp" />
    <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/robot_control-800.webp" />
    <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/robot_control-1400.webp" />
    

    <!-- Fallback to the original file -->
    <img src="/assets/img/robot_control.png" class="img-fluid rounded z-depth-1" width="auto" height="auto" onerror="this.onerror=null; $('.responsive-img-srcset').remove();" />

  </picture>

</figure>

    </div>
</div>
<div class="caption">
    Para el caso de un robot caminante, el controlador tendría que mover cada articulación del robot para que camine y no tropiece, incluso frente a obstáculos.
</div>

<p>En control, el diseñador es el que se preocupa de modificar explícitamente el sistema para que tenga el comportamiento idóneo. No obstante, podemos utilizar RL para resolver los problemas de control. En RL, el software por sí mismo intenta aprender el comportamiento óptimo a lo largo del tiempo. La idea es tomar las observaciones del estado del sistema y el agente generará las mejores acciones para controlar el sistema. <em>El agente sería el controlador</em>.</p>

<div class="row mt-3">
    <div class="col-sm mt-3 mt-md-0">
        <figure>

  <picture>
    
    <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/rl_ayuda_control-480.webp" />
    <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/rl_ayuda_control-800.webp" />
    <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/rl_ayuda_control-1400.webp" />
    

    <!-- Fallback to the original file -->
    <img src="/assets/img/rl_ayuda_control.png" class="img-fluid rounded z-depth-1" width="auto" height="auto" onerror="this.onerror=null; $('.responsive-img-srcset').remove();" />

  </picture>

</figure>

    </div>
</div>
<div class="caption">
    RL permite generar una sola función matemática que reemplace a todas esos loops complicados de control.
</div>

<p>Estrictamente hablando, el controlador sería la política (recordar que la política es el cerebro del agente), ya que va a mapear las observaciones del estado del sistema a comandos de acción que permitan generar el comportamiento que queremos que este sistema tenga. Nótese que <em>el entorno sería el sistema a controlar</em>.</p>

<blockquote>
  <p>El controlador influye sobre el sistema cambiando su estado.</p>
</blockquote>

<h2 id="vii-uso-de-rl-en-control">VII. Uso de RL en control</h2>
<p>Para utilizar RL en control tenemos que:</p>
<ol>
  <li>Establecer la estructura del controlador: definir la política</li>
  <li>Definir ¿qué es un resultado exitoso? Y establecer recompensas cuando se consiga: Establecer una función de recompensa que el indique al algoritmo si está mejorando o no.</li>
  <li>Aplicar un algoritmo de aprendizaje eficiente que sepa cómo ajustar los parámetros para que el proceso converja en un tiempo razonable: Elegir un algoritmo de RL</li>
</ol>

<div class="row mt-3">
    <div class="col-sm mt-3 mt-md-0">
        <figure>

  <picture>
    
    <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/uso_rl_control-480.webp" />
    <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/uso_rl_control-800.webp" />
    <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/uso_rl_control-1400.webp" />
    

    <!-- Fallback to the original file -->
    <img src="/assets/img/uso_rl_control.png" class="img-fluid rounded z-depth-1" width="auto" height="auto" onerror="this.onerror=null; $('.responsive-img-srcset').remove();" />

  </picture>

</figure>

    </div>
</div>
<div class="caption">
    Uso de RL en un sistema de control.
</div>

<h2 id="viii-workflow-de-rl">VIII. Workflow de RL</h2>
<p>El flujo de trabajo de RL consiste en:</p>
<ul>
  <li>Paso 1: Establecer un entorno: qué debe existir en ese entorno. Además debemos decidir: ¿Durante el entrenamiento probamos con un entorno real (hardware real) o simulado (uso de modelos matemáticos del sistema)?</li>
</ul>
<div class="row mt-3">
    <div class="col-sm mt-3 mt-md-0">
        <figure>

  <picture>
    
    <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/1_entorno-480.webp" />
    <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/1_entorno-800.webp" />
    <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/1_entorno-1400.webp" />
    

    <!-- Fallback to the original file -->
    <img src="/assets/img/1_entorno.png" class="img-fluid rounded z-depth-1" width="auto" height="auto" onerror="this.onerror=null; $('.responsive-img-srcset').remove();" />

  </picture>

</figure>

    </div>
</div>

<ul>
  <li>Paso 2: Definir la señal de recompensa: qué debe hacer el agente, cómo debe llegar al objetivo, diseñar la función de recompensa que incentive al agente.</li>
</ul>
<div class="row mt-3">
    <div class="col-sm mt-3 mt-md-0">
        <figure>

  <picture>
    
    <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/2_recompensa-480.webp" />
    <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/2_recompensa-800.webp" />
    <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/2_recompensa-1400.webp" />
    

    <!-- Fallback to the original file -->
    <img src="/assets/img/2_recompensa.png" class="img-fluid rounded z-depth-1" width="auto" height="auto" onerror="this.onerror=null; $('.responsive-img-srcset').remove();" />

  </picture>

</figure>

    </div>
</div>

<ul>
  <li>Paso 3: Establecer la política: cómo representamos la política: estructuración de los parámetros y la lógica de la toma de decisiones del agente. ¿Cuál es la información que recibe el agente? ¿Cuál debe ser la salida que genera el agente? ¿De qué tipo son estas entradas? ¿Voy a representar la política con una red neuronal? Etc.</li>
</ul>
<div class="row mt-3">
    <div class="col-sm mt-3 mt-md-0">
        <figure>

  <picture>
    
    <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/3_politica-480.webp" />
    <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/3_politica-800.webp" />
    <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/3_politica-1400.webp" />
    

    <!-- Fallback to the original file -->
    <img src="/assets/img/3_politica.png" class="img-fluid rounded z-depth-1" width="auto" height="auto" onerror="this.onerror=null; $('.responsive-img-srcset').remove();" />

  </picture>

</figure>

    </div>
</div>

<ul>
  <li>Paso 4: Entrenamiento: necesitamos escoger un algoritmo de RL que permita obtener la política óptima. Se debe elegir el mejor de acuerdo a nuestro caso. Los algoritmos de RL dependen de la estructuración de la política.</li>
</ul>
<div class="row mt-3">
    <div class="col-sm mt-3 mt-md-0">
        <figure>

  <picture>
    
    <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/4_entrenamiento-480.webp" />
    <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/4_entrenamiento-800.webp" />
    <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/4_entrenamiento-1400.webp" />
    

    <!-- Fallback to the original file -->
    <img src="/assets/img/4_entrenamiento.png" class="img-fluid rounded z-depth-1" width="auto" height="auto" onerror="this.onerror=null; $('.responsive-img-srcset').remove();" />

  </picture>

</figure>

    </div>
</div>

<ul>
  <li>Paso 5: Deploy/Implementación: se implementa la política en un entorno real y se verfican los resultados.</li>
</ul>
<div class="row mt-3">
    <div class="col-sm mt-3 mt-md-0">
        <figure>

  <picture>
    
    <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/5_deploy-480.webp" />
    <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/5_deploy-800.webp" />
    <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/5_deploy-1400.webp" />
    

    <!-- Fallback to the original file -->
    <img src="/assets/img/5_deploy.png" class="img-fluid rounded z-depth-1" width="auto" height="auto" onerror="this.onerror=null; $('.responsive-img-srcset').remove();" />

  </picture>

</figure>

    </div>
</div>

<h2 id="1-entorno">1. Entorno</h2>
<p>En RL, el entorno es de dónde el agente aprende. El entorno es todo lo que está <strong>afuera</strong> del agente. Es donde el agente envía acciones, y es lo que genera recomepensas y observaciones.</p>

<div class="row mt-3">
    <div class="col-sm mt-3 mt-md-0">
        <figure>

  <picture>
    
    <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/entorno_rl-480.webp" />
    <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/entorno_rl-800.webp" />
    <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/entorno_rl-1400.webp" />
    

    <!-- Fallback to the original file -->
    <img src="/assets/img/entorno_rl.png" class="img-fluid rounded z-depth-1" width="auto" height="auto" onerror="this.onerror=null; $('.responsive-img-srcset').remove();" />

  </picture>

</figure>

    </div>
</div>
<div class="caption">
    Entorno en RL.
</div>

<p>En RL, el entorno es todo menos el agente. Esto llevado a control sería todo lo que no es el controlador: lazo de retroalimentación, sistema, señal de referencia, etc.</p>

<div class="row mt-3">
    <div class="col-sm mt-3 mt-md-0">
        <figure>

  <picture>
    
    <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/entorno_rl_control-480.webp" />
    <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/entorno_rl_control-800.webp" />
    <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/entorno_rl_control-1400.webp" />
    

    <!-- Fallback to the original file -->
    <img src="/assets/img/entorno_rl_control.png" class="img-fluid rounded z-depth-1" width="auto" height="auto" onerror="this.onerror=null; $('.responsive-img-srcset').remove();" />

  </picture>

</figure>

    </div>
</div>
<div class="caption">
    Entorno RL en control.
</div>

<p>Un agente realiza <strong>acciones</strong> que influyen sobre el entorno. El entorno cambia de estado, al hacerlo, informa al agente entregándole <strong>observaciones de estado</strong> y una <strong>recompensa</strong>.</p>

<p>Existen dos tipos de RL:</p>
<h3 id="11-rl-sin-modelo-model-free-rl">1.1. RL sin modelo (model-free RL)</h3>
<p>El agente aún no sabiendo nada sobre el entorno es capaz de aprender la política óptima. Por ejemplo, el agente no necesita saber las leyes que rigen al robot caminante ni como se mueven las articulaciones. Se coloca un agente RL en cualquier sistema y asumiendo que a la política se le da acceso a las observaciones, acciones y estados internos, el agente obtendrá la mayor recompensa por sí solo. A esto se le llama <strong>RL sin modelo</strong>.</p>

<div class="row mt-3">
    <div class="col-sm mt-3 mt-md-0">
        <figure>

  <picture>
    
    <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/rl_no_modelo-480.webp" />
    <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/rl_no_modelo-800.webp" />
    <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/rl_no_modelo-1400.webp" />
    

    <!-- Fallback to the original file -->
    <img src="/assets/img/rl_no_modelo.png" class="img-fluid rounded z-depth-1" width="auto" height="auto" onerror="this.onerror=null; $('.responsive-img-srcset').remove();" />

  </picture>

</figure>

    </div>
</div>
<div class="caption">
    RL sin un modelo que lo ayude.
</div>

<p>Como el agente no tiene conocimiento del entorno, debe explorar todas las áreas del espacio de estados para averiguar cómo recolectar la mayor recompensa, lo que conlleva más tiempo en aprender la política óptima.</p>

<p>El RL sin modelo se utiliza cuando es difícil generar un modelo. Ej: controlar un auto, robot caminante.</p>

<h3 id="12-rl-basado-en-modelo-model-based-rl">1.2. RL basado en modelo (Model-based RL)</h3>
<p>Sin ninguna comprensión del entorno el agente deberá explorar todas las áreas del espacio de estados, por lo que gastará tiempo explorando áreas de bajas recompensas mientras está aprendiendo. Al proporcionar un modelo del entorno, o parte de él, proporcionamos al agente información de zonas del espacio de estado que no valen la pena explorar. Entonces, el modelo puede complementar el proceso de aprendizaje evitando áreas que sabe que son malas y el agente aprenderá mejor.</p>

<p>Este modelo sirve de guía para el agente, complementando el aprendizaje de este último. El tiempo es menor en aprender la política óptima.</p>

<div class="row mt-3">
    <div class="col-sm mt-3 mt-md-0">
        <figure>

  <picture>
    
    <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/rl_con_modelo-480.webp" />
    <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/rl_con_modelo-800.webp" />
    <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/rl_con_modelo-1400.webp" />
    

    <!-- Fallback to the original file -->
    <img src="/assets/img/rl_con_modelo.png" class="img-fluid rounded z-depth-1" width="auto" height="auto" onerror="this.onerror=null; $('.responsive-img-srcset').remove();" />

  </picture>

</figure>

    </div>
</div>
<div class="caption">
    RL con un modelo que lo ayude.
</div>

<h3 id="13-entorno-real-físico-o-simulado">1.3. ¿Entorno real (físico) o simulado?</h3>
<p>Cuando puede ocurrir daño en el hardware o a las personas se ocupa un entorno simulado, debido a los costos. Además, la simulación es más rápida que en tiempo real y se pueden ejecutar en paralelo.</p>

<p>Comparación entorno real vs simulado:</p>
<ol>
  <li>Real
    <ul>
      <li>Preciso</li>
      <li>Simple: no se gasta tiempo creando y validando el modelo</li>
      <li>Necesario: algunas veces necesario (difícil de hacer modelo)</li>
    </ul>
  </li>
  <li>Simulado
    <ul>
      <li>Veloz y paralelizable</li>
      <li>Fácil de modelar condiciones de testeo</li>
      <li>Seguro</li>
    </ul>
  </li>
</ol>

<h4 id="rl-en-matlab">RL en matlab</h4>
<p>Si ya tenemos un modelo en Matlab de nuestro sistema, reemplazamos el controlador existente con un agente RL, y añadimos una función de recompensa al entorno, y así empezamos el proceso de aprendizaje.</p>

<div class="row mt-3">
    <div class="col-sm mt-3 mt-md-0">
        <figure>

  <picture>
    
    <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/rl_matlab-480.webp" />
    <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/rl_matlab-800.webp" />
    <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/rl_matlab-1400.webp" />
    

    <!-- Fallback to the original file -->
    <img src="/assets/img/rl_matlab.png" class="img-fluid rounded z-depth-1" width="auto" height="auto" onerror="this.onerror=null; $('.responsive-img-srcset').remove();" />

  </picture>

</figure>

    </div>
</div>
<div class="caption">
    Implementación de RL con modelo Matlab.
</div>

<h2 id="2-señal-de-recompensa">2. Señal de recompensa</h2>
<p>Debemos recompensar al agente por sus acciones, para así orientarlo a alcanzar el objetivo. Esto requiere crear una función de recompensa para que el algoritmo de aprendizaje entienda cuando la política se está volviendo mejor.</p>

<h3 id="21-qué-es-una-recompensa">2.1. ¿Qué es una recompensa?</h3>
<blockquote>
  <p>La recompensa es una función que produce un número escalar que representa la <em>bondad</em> de estar en un estado particular y realizar una acción determinada.</p>
</blockquote>

\[\text{recompensa} = f(\text{estado}, \text{acción})\]

<p>En RL no hay un límite para crear una función de recompensa. Las recompensas pueden ser:</p>
<ul>
  <li>escasas</li>
  <li>en cada paso de tiempo (time step)</li>
  <li>al final de cada episodio</li>
  <li>basada en parámetros</li>
  <li>etc.</li>
</ul>

<p>Hacer una función de recompensa buena es difícil. No existe una forma sencilla de diseñar una señal o función de recompensa para garantizar que el agente converja a la solución que se quiere. Esto se debe a dos razones:</p>

<ul>
  <li>1.- A menudo el objetivo se cumple luego de una larga secuencia de acciones. Si sólo recompensamos por cumplir el objetivo, el agente se equivocará por largos periodos de tiempo sin recibir recompensas, a esto se le llama <strong>recompensas escasas</strong>. Es muy poco probable que el agente se tope al azar con la secuencia de acciones que produce la recompensa escasa. Confiar en esta exploración aleatoria es bastante ineficiente, ya que es un aprendizaje muy lento, incluso llega a no ser práctico. Este problema se puede solucionar con el <strong>reward shaping</strong> o dar forma a las recompensas. Si proporcionamos <strong>recompensas intermedias</strong> más pequeñas que induzcan al agente a seguir el camino correcto, lograremos que el agente consiga su objetivo.</li>
</ul>
<div class="row mt-3">
    <div class="col-sm mt-3 mt-md-0">
        <figure>

  <picture>
    
    <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/reward_shaping-480.webp" />
    <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/reward_shaping-800.webp" />
    <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/reward_shaping-1400.webp" />
    

    <!-- Fallback to the original file -->
    <img src="/assets/img/reward_shaping.png" class="img-fluid rounded z-depth-1" width="auto" height="auto" onerror="this.onerror=null; $('.responsive-img-srcset').remove();" />

  </picture>

</figure>

    </div>
</div>
<div class="caption">
    Reward shaping.
</div>

<ul>
  <li>2.- El reward shaping viene con su propio conjunto de problemas. Si se le da un atajo a un algoritmo de optimización, él lo tomará. Los atajos están ocultos en la función de recompensa. Una función de recompensa mal moldeada puede hacer que el agente converja a una solución no ideal, incluso si produce la mayor recompensa. Por ejemplo: tenemos un robot caminante cuyo objetivo es caminar hasta los 10 metros. Supongamos que tenemos pensado darle recompensas intermedias y 1 grande al final por llegar a los 10 metros. Además de llegar al objetivo es importante considerar el <em>cómo</em> se llega a él; para este robot desplomarse y arrastrase como una oruga hasta llegar a los 10 metros es lo mismo que caminar erguido y sin desviarse hasta los 10 metros. Al agente le da lo mismo porque al final igual va a recibir la recompensa. Sin embargo, a nosotros nos interesa que el robot cumpla su objetivo de manera idónea.</li>
</ul>
<div class="row mt-3">
    <div class="col-sm mt-3 mt-md-0">
        <figure>

  <picture>
    
    <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/reward_shaping_problema-480.webp" />
    <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/reward_shaping_problema-800.webp" />
    <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/reward_shaping_problema-1400.webp" />
    

    <!-- Fallback to the original file -->
    <img src="/assets/img/reward_shaping_problema.png" class="img-fluid rounded z-depth-1" width="auto" height="auto" onerror="this.onerror=null; $('.responsive-img-srcset').remove();" />

  </picture>

</figure>

    </div>
</div>
<div class="caption">
    El problema del reward shaping.
</div>

<p>¿Cómo solucionamos el problema de los atajos? Inyectando conocimiento específico del dominio en el agente. Vamos a recompensar al agente no solo por llegar al objetivo sino también por el <em>cómo lo hace</em>. Por ejemplo, recompensar al robot por caminar erguido, a una velocidad determinada, que no se desvíe del camino, etc.</p>

<h3 id="22-exploración-y-explotación">2.2. Exploración y explotación</h3>
<p>La idea es la siguiente:</p>
<ul>
  <li>Explotación: agente explota el entorno eligiendo las acciones que recogen la mayor cantidad de recompensas que ya conoce.</li>
  <li>Exploración: elegir acciones que exploran partes del entorno que aún no se conocen.</li>
</ul>

<p>Existe un trade-off entre exploración y explotación mientras el agente interactúa con el entorno.</p>

<p>Las acciones que hace el agente determinan la información que recibe y, por lo tanto, lo que aprende.</p>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>                    Estado actual
                          s1
         explotar    a1 /     \ a2     explorar
                      /         \
                    s2          s3
                  R. conocida     R. desconocida
</code></pre></div></div>
<p>Si ejecutamos <code class="language-plaintext highlighter-rouge">a1</code> el agente sólo recibirá información de <code class="language-plaintext highlighter-rouge">s2</code> obteniendo una recompensa conocidad y nada sobre <code class="language-plaintext highlighter-rouge">s3</code>. Si realizamos <code class="language-plaintext highlighter-rouge">a2</code> el agente sólo recibirá información de <code class="language-plaintext highlighter-rouge">s3</code> (un estado nuevo) y la recompensa puede que sea mejor que la entregada por <code class="language-plaintext highlighter-rouge">s2</code>.</p>

<h4 id="explotación-pura">Explotación pura</h4>
<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>estados conocidos      estado actual            estados desconocidos
0    +1   0    +1        agente          +1     ¿?   ¿?
s0   s1   s2   s3          s4            s5     s6   s7
      |____|____|___________v   
</code></pre></div></div>
<p>El agente nunca recibirá información adicional sobre los estados desconocidos.</p>

<p>Con explotación pura, el algoritmo de aprendizaje converge en una política subóptima. No sabremos si las áreas desconocidas poseían mayores recompensas.</p>

<h4 id="exploración-pura">Exploración pura</h4>
<p>Si bien existe riesgo de recibir menos recompensas, la exploración permite expandir la política para los nuevos estados. Tenemos la chance de recibir mejores recompensas, y por ende, más posibilidades de encontrar la solución global.</p>

<p>La exploración pura no es buena cuando se entrena con hardware físico, ya que lo puede dañar, o peor aún, puede dañar a las personas, por ejemplo, probar una entrada de volante aleatoria en un auto autónomo. Además el aprendizaje es más lento y puede que no encontremos una solución en tiempo razonable.</p>

<p>Los mejores algoritmos de aprendizaje RL son los que logran el equilibrio entre ambas. Generalmente, un agente explora más al comienzo del aprendizaje y gradualmente pasa a un rol más de explotación al final.</p>

<h2 id="3-política">3. Política</h2>
<p>El agente se compone de una política y un algoritmo de aprendizaje de RL. Muchos algoritmos de aprendizaje requieren una estructura de política específica. La elección del algoritmo también depende de la naturaleza del entorno.</p>

<p>En la política se representan la lógica y los parámetros. Esta política es una función matemática que toma las observaciones de estado y genera las acciones.</p>

\[\text{observaciones} \rightarrow f(x) \rightarrow \text{acciones}\]

<p>Existen dos enfoques para estructurar la función de política:</p>
<ol>
  <li>Directo (actores o policy-based): hace un mapeo entre observaciones y acciones específico.</li>
  <li>Indirecto (críticos o value-based): se basa en otras métricas como el valor para inferir el mapeo óptimo.</li>
</ol>

<h3 id="representamos-la-política-con-una-tabla-función-o-tabla-q">Representamos la política con una tabla: Función o Tabla Q</h3>
<p>Si los espacios de estado y acción son <strong>discretos</strong> y pequeños en número, podemos ocupar una tabla simple para representar la política.</p>

<p>La función o tabla Q es una tabla que asigna (mapea) estados y acciones a un valor.</p>

\[\begin{aligned}
&amp; \text {Tabla 1. Ejemplo de política como una Tabla Q}\\
&amp;\begin{array}{c|cc}
\hline \hline \text {  } &amp; \text { a1 } &amp; \text { a2 } &amp; \text { a3 }\\
\hline
s1 &amp; -1 &amp; 2 &amp; 0 \\
s2 &amp;  0 &amp; 1 &amp; 5 \\
s3 &amp; 3 &amp; -1 &amp; 2 \\
\hline
\end{array}
\end{aligned}\]

<p>Entonces, dado un estado <em>S</em> actual, la política sería buscar el valor de cada acción <em>A</em> posible en ese estado y elegir la acción con el valor más alto.</p>

<div class="row mt-3">
    <div class="col-sm mt-3 mt-md-0">
        <figure>

  <picture>
    
    <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/tablaq-480.webp" />
    <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/tablaq-800.webp" />
    <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/tablaq-1400.webp" />
    

    <!-- Fallback to the original file -->
    <img src="/assets/img/tablaq.png" class="img-fluid rounded z-depth-1" width="auto" height="auto" onerror="this.onerror=null; $('.responsive-img-srcset').remove();" />

  </picture>

</figure>

    </div>
</div>
<div class="caption">
    La política como tabla Q toma el estado actual en el que estamos y elige la acción con el valor más alto.
</div>

<blockquote>
  <p>Entrenar a un agente con una Tabla Q consistiría en calcular el valor para todas las acciones posibles en cada estado.</p>
</blockquote>

<blockquote>
  <p>La función Q falla cuando el número de pares estado/acción se vuelve muy grande o infinito, en otras palabras, cuando son continuos.</p>
</blockquote>

<p>Por ejemplo, imaginemos que queremos controlar un péndulo invertido. El estado del péndulo puede ser cualquier ángulo de \(-\pi\) a \(\pi\). El espacio de acción es cualquier torque de motor desde el \(-\infty\) a \(\infty\). Tratar de capturar cada combinación de cada estado y acción en una tabla es imposible.</p>

<p>Si una función o tabla Q no nos sirve, entonces necesitamos una función continua que sea capaz de representar la política, aunque es bastante difícil de diseñar. La solución a esto es usar un <strong>aproximador de función</strong> de propósito general para representar la política, algo que pueda manejar estados y acciones dentro de un espacio continuo: <strong>Las redes neuronales profundas (Deep Learning)</strong>.</p>

<h4 id="por-qué-usar-redes-neuronales-y-no-tablas-o-una-función-de-transferencia-para-la-política">¿Por qué usar redes neuronales y no tablas o una función de transferencia para la política?</h4>

<p><em>Nota: la función de transferencia es una función que relaciona las entradas con las salidas de un sistema. Se utiliza mucho en control.</em></p>

<p>Las tablas no son prácticas cuando el espacio de estados y acciones son muy grandes (continuos).</p>

<p>Para el caso de las funciones de transferencia, es difícil diseñar la estructura de estas funciones para entornos complejos.</p>

<h3 id="31-política-como-red-neuronal">3.1 Política como red neuronal</h3>
<p>Una red neurona es un grupo de nodos llamados <strong>neuronas artificiales</strong> que están interconectados de forma que se vuelven un <strong>aproximador de función universal</strong>. Esto significa que se puede configurar la red con la correcta combinación de nodos y conexiones para imitar cualquier relación de entrada y salida. La función generada por la red neuronal puede ser extremadamente compleja, sin embargo, la naturaleza de las redes neuronales asegura que toda función se puede aproximar.</p>

<p>El aprendizaje de la red consiste en el ajuste de los parámetros sistemáticamente para encontrar la relación óptima de entrada/salida.</p>

<div class="row mt-3">
    <div class="col-sm mt-3 mt-md-0">
        <figure>

  <picture>
    
    <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/red_neuronal-480.webp" />
    <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/red_neuronal-800.webp" />
    <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/red_neuronal-1400.webp" />
    

    <!-- Fallback to the original file -->
    <img src="/assets/img/red_neuronal.png" class="img-fluid rounded z-depth-1" width="auto" height="auto" onerror="this.onerror=null; $('.responsive-img-srcset').remove();" />

  </picture>

</figure>

    </div>
</div>
<div class="caption">
    Red con 2 entradas (recibe 2 valores), 2 capas ocultas con 3 neuronas cada una, y una capa de salida con las 2 salidas finales de la red.
</div>

<p>La salida de una neurona está dada por:
\(a = f(w_0\cdot x_0 + w_1\cdot x_1 + ... + w_{n-1}\cdot x_{n-1} + b)\)
donde \(b\) es el <strong>bias</strong> o sesgo, \(w\) son los <strong>weights</strong> o pesos asginados a cada entrada \(x\).</p>

<p>Sin \(f\) la salida o activación de una neurona es una operación lineal (\(w\cdot x + b\) es una suma ponderada o combinación lineal). Si ninguna neurona de nuestra red neuronal utilizara la función \(f\), la salida de la red sería lineal. El inconveniente es que los problemas lineales son simples, muy por el contrario de los problemas de la vida real que son complejos, o sea, no lineales. Es por esto que se utiliza una <strong>función de activación \(f\)</strong> para poder aproximar funciones no lineales. Esta función de activación transforma el valor de la suma ponderada a otro valor (depende de la función) que es el que finalmente sale de la neurona y sirve de entrada a las neuronas de las siguientes capas.</p>

<p>Que las funciones de activación sean no lineales es fundamental para crea una red que pueda aproximarse a cualquier función. Esto se debe a que muchas funciones no lineales se pueden dividir en una combinación ponderada de salidas de función de activación.</p>

<h3 id="funciones-de-activación">Funciones de activación</h3>
<p>Las 3 funciones de activación más populares son:</p>
<ol>
  <li>Linear: simplemente es dejar como salida de la neurona a la suma ponderada \(w\cdot x + b\).</li>
  <li>Sigmoid: se mapea \(w\cdot x + b\) a un valor entre 0 y 1.</li>
  <li>ReLU: si \(w\cdot x + b\) es positivo, se deja como salida \(w\cdot x + b\). Si es 0 o negativo, la salida será 0.</li>
</ol>

<p>Existen más funciones de activación no lineales como por ejemplo, <strong>Tanh</strong>, <strong>Leaky ReLU</strong>, <strong>Softmax</strong>, etc., que igualmente se utilizan, pero son menos populares.</p>

\[\begin{aligned}
&amp; \text {Tabla 2. Ejemplos de transformación de valores para Sigmoid y ReLU}\\
&amp;\begin{array}{cccc}
\hline \hline \text { Valor } &amp; \text { Sigmoid } &amp; \text { ReLU } \\
\hline
-2 &amp; 0.12 &amp; 0 \\
-1 &amp; 0.27 &amp; 0 \\
1 &amp; 0.73 &amp; 1 \\
2 &amp; 0.88 &amp; 2 \\
\hline
\end{array}
\end{aligned}\]

<blockquote>
  <p>Se debe tener en cuenta que la red neuronal debe ser lo suficientemente compleja como para aproximarse a la función, pero no tan compleja como para que el entrenamiento no sea posible o sea muy lento.</p>
</blockquote>

<div class="row mt-3">
    <div class="col-sm mt-3 mt-md-0">
        <figure>

  <picture>
    
    <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/red_neuronal2-480.webp" />
    <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/red_neuronal2-800.webp" />
    <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/red_neuronal2-1400.webp" />
    

    <!-- Fallback to the original file -->
    <img src="/assets/img/red_neuronal2.png" class="img-fluid rounded z-depth-1" width="auto" height="auto" onerror="this.onerror=null; $('.responsive-img-srcset').remove();" />

  </picture>

</figure>

    </div>
</div>
<div class="caption">
    Política representada como una red neuronal.
</div>

<h3 id="diseño-de-una-red-neuronal">Diseño de una red neuronal</h3>
<p>Se debe elegir lo siguiente para implementar una red neuronal:</p>
<ul>
  <li>Función de activación para las capas ocultas y la capa de salida respectivamente (pueden ser distintas o para los dos tipos de capa puede ser la misma)</li>
  <li>Número de capas ocultas</li>
  <li>Número de neurona en cada capa</li>
  <li>Estructura interna de la red: ¿Totalmente conectada (fully connected)? ¿Nos saltamos capas (red residual)? ¿La red tiene memoria interna (red recurrente)? ¿Grupos de neuronas trabajan en conjunto (red convolusional)?</li>
</ul>

<div class="row mt-3">
    <div class="col-sm mt-3 mt-md-0">
        <figure>

  <picture>
    
    <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/red_neuronal_tipos-480.webp" />
    <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/red_neuronal_tipos-800.webp" />
    <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/red_neuronal_tipos-1400.webp" />
    

    <!-- Fallback to the original file -->
    <img src="/assets/img/red_neuronal_tipos.png" class="img-fluid rounded z-depth-1" width="auto" height="auto" onerror="this.onerror=null; $('.responsive-img-srcset').remove();" />

  </picture>

</figure>

    </div>
</div>
<div class="caption">
    Tipos de redes neuronales.
</div>

<p>No existe un enfoque preestablecido para la estructura de la red, pero una idea sería comenzar con una estructura que ya ha funcionado para el tipo de problema que estamos resolviendo.</p>

<h2 id="4-entrenamiento-algortimos-de-aprendizaje-rl">4. Entrenamiento: Algortimos de aprendizaje RL</h2>
<p>La estructura de la política y el algoritmo de aprendizaje RL están íntimamente entrelazados. No se puede estructurar la política sin elegir el algoritmo de RL de aprendizaje.</p>

<p>Existen 3 formas de estructurar la política:</p>
<ol>
  <li>Policy function-based</li>
  <li>Value function-based</li>
  <li>Actor / critic</li>
</ol>

<div class="row mt-3">
    <div class="col-sm mt-3 mt-md-0">
        <figure>

  <picture>
    
    <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/policy_estructura-480.webp" />
    <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/policy_estructura-800.webp" />
    <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/policy_estructura-1400.webp" />
    

    <!-- Fallback to the original file -->
    <img src="/assets/img/policy_estructura.png" class="img-fluid rounded z-depth-1" width="auto" height="auto" onerror="this.onerror=null; $('.responsive-img-srcset').remove();" />

  </picture>

</figure>

    </div>
</div>
<div class="caption">
    Formas de estructurar la política.
</div>

<h3 id="41-policy-function-based-learning">4.1. Policy function-based learning</h3>
<p>Estos algoritmos de aprendizaje entrenan una red neuronal que toma las observaciones del estado y produce acciones. Esta red neuronal es la política completa, de ahí el nombre de algoritmos basados en función de política. Esta red neuronal se llama <strong>actor</strong> porque le dice directamente al agente qué acciones tomar.</p>

<div class="row mt-3">
    <div class="col-sm mt-3 mt-md-0">
        <figure>

  <picture>
    
    <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/actor-480.webp" />
    <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/actor-800.webp" />
    <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/actor-1400.webp" />
    

    <!-- Fallback to the original file -->
    <img src="/assets/img/actor.png" class="img-fluid rounded z-depth-1" width="auto" height="auto" onerror="this.onerror=null; $('.responsive-img-srcset').remove();" />

  </picture>

</figure>

    </div>
</div>
<div class="caption">
    Red neuronal actor.
</div>

<p>¿Cómo entrenamos una red neuronal o política del tipo actor? La entrenamos con los métodos <strong>policy gradient</strong>.</p>

<p>Veamos un ejemplo del uso de una red neuronal del tipo actor: Juego Breakout.</p>

<div class="row mt-3">
    <div class="col-sm mt-3 mt-md-0">
        <figure>

  <picture>
    
    <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/breakout-480.webp" />
    <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/breakout-800.webp" />
    <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/breakout-1400.webp" />
    

    <!-- Fallback to the original file -->
    <img src="/assets/img/breakout.png" class="img-fluid rounded z-depth-1" width="auto" height="auto" onerror="this.onerror=null; $('.responsive-img-srcset').remove();" />

  </picture>

</figure>

    </div>
</div>
<div class="caption">
    Captura de pantalla del juego.
</div>

<p>En el juego, intentamos eliminar los ladrillos usando una paleta para dirigir una pelota que rebota. El juego tiene 3 acciones: mover la paleta a la izquierda, a la derecha, o no moverla. Además, tiene un espacio de estados casi continuo que incluye: posición de la paleta, posición de la pelota, velocidad de la pelota, ubicación de los ladrillos restantes.</p>

<ul>
  <li>Entradas a la red de actor: estados de la paleta, la pelota y los bloques.</li>
  <li>Salidas de la red de actor: nodos que representan las acciones: izquierda, derecha y mantener.</li>
</ul>

<p>En lugar de calcular los estados manualmente e introducirlos en la red, podemos ingresar una captura de pantalla del juego y dejar que la red aprenda qué características de la imagen son las más importantes para basar su salida. El actor mapearía la intensidad de miles de píxeles a las tres salidas.</p>

<div class="row mt-3">
    <div class="col-sm mt-3 mt-md-0">
        <figure>

  <picture>
    
    <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/red_breakout-480.webp" />
    <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/red_breakout-800.webp" />
    <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/red_breakout-1400.webp" />
    

    <!-- Fallback to the original file -->
    <img src="/assets/img/red_breakout.png" class="img-fluid rounded z-depth-1" width="auto" height="auto" onerror="this.onerror=null; $('.responsive-img-srcset').remove();" />

  </picture>

</figure>

    </div>
</div>
<div class="caption">
    Red de actor para el juego Breakout.
</div>

<p>Una vez configurada la red, es hora de buscar enfoques para entrenarla. Un enfoque sería los <strong>métodos policy gradient</strong>. Los métodos policy gradient funcionan con una <strong>política estocástica</strong>. Este tipo de política se refiere a que en vez de entregar la salida <em>izquierda, derecha o mantener</em> determinista, la política generaría una <strong>probabilidad</strong> de mover a la izquierda, a la derecha o mantener.</p>

<div class="row mt-3">
    <div class="col-sm mt-3 mt-md-0">
        <figure>

  <picture>
    
    <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/red_estocastica-480.webp" />
    <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/red_estocastica-800.webp" />
    <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/red_estocastica-1400.webp" />
    

    <!-- Fallback to the original file -->
    <img src="/assets/img/red_estocastica.png" class="img-fluid rounded z-depth-1" width="auto" height="auto" onerror="this.onerror=null; $('.responsive-img-srcset').remove();" />

  </picture>

</figure>

    </div>
</div>
<div class="caption">
    Política estocástica.
</div>

<p>La política estocástica incorpora la exploración en las probabilidades. Cuando el agente aprende, actualiza las probabilidades, aumentando la probabilidad de la acción que produjo una buena recompensa.</p>

<p>Con el tiempo, el agente empujará estas probabilidades en la dirección que produzca la mayor recompensa. Eventualmente, la acción venajosa para cada estado tendrá una probabilidad tan alta que el agente siempre realizará esa acción.</p>

<h4 id="policy-gradient-methods">Policy gradient methods</h4>
<p>¿Cómo el agente sabe si las acciones fueron buenas o no? La idea es la siguiente: ejecutar la política actual, recolectar recompensas a lo largo del camino y luego actualizar la red para aumentar las probabilidades de acciones que llevaron a recompensas más altas.</p>

<p>El funcionamiento de los métodos de policy gradient es el siguiente:</p>

<div class="row mt-3">
    <div class="col-sm mt-3 mt-md-0">
        <figure>

  <picture>
    
    <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/policy_gradient_ciclo-480.webp" />
    <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/policy_gradient_ciclo-800.webp" />
    <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/policy_gradient_ciclo-1400.webp" />
    

    <!-- Fallback to the original file -->
    <img src="/assets/img/policy_gradient_ciclo.png" class="img-fluid rounded z-depth-1" width="auto" height="auto" onerror="this.onerror=null; $('.responsive-img-srcset').remove();" />

  </picture>

</figure>

    </div>
</div>
<div class="caption">
    Diagrama de flujo del funcionamiento de los métodos policy gradient.
</div>

<p>Estos métodos toman la derivada de cada <em>weight</em> y <em>bias</em> en la red neuronal con respecto a la recompensa y los ajusta en la dirección de un aumento de recompensa positivo. Así, el algoritmo de aprendizaje mueve los <em>bias</em> y <em>weights</em> para ascender por la pendiente de recompensa, de ahí viene el nombre de gradiente.</p>

<div class="row mt-3">
    <div class="col-sm mt-3 mt-md-0">
        <figure>

  <picture>
    
    <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/policy_gradient-480.webp" />
    <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/policy_gradient-800.webp" />
    <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/policy_gradient-1400.webp" />
    

    <!-- Fallback to the original file -->
    <img src="/assets/img/policy_gradient.png" class="img-fluid rounded z-depth-1" width="auto" height="auto" onerror="this.onerror=null; $('.responsive-img-srcset').remove();" />

  </picture>

</figure>

    </div>
</div>
<div class="caption">
    Policy gradient methods.
</div>

<h4 id="411-lo-malo-de-los-métodos-de-policy-gradient">4.1.1. Lo malo de los métodos de policy gradient</h4>
<ul>
  <li>1.- El enfoque ingenuo de simplemente seguir la dirección del ascenso más empinado puede converger a un máximo local en vez de uno global.</li>
</ul>
<div class="row mt-3">
    <div class="col-sm mt-3 mt-md-0">
        <figure>

  <picture>
    
    <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/problema1_policy-480.webp" />
    <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/problema1_policy-800.webp" />
    <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/problema1_policy-1400.webp" />
    

    <!-- Fallback to the original file -->
    <img src="/assets/img/problema1_policy.png" class="img-fluid rounded z-depth-1" width="auto" height="auto" onerror="this.onerror=null; $('.responsive-img-srcset').remove();" />

  </picture>

</figure>

    </div>
</div>
<div class="caption">
    Problema del máximo local.
</div>

<ul>
  <li>2.- Pueden converger lentamente debido a su sensibilidad a las mediciones ruidosas, o sea, cuando se necesitan muchas acciones secuenciales para recibir una recompensa y la recompensas acumulada resultante tiene gran variación entre episodios.</li>
</ul>

<p>Por ejemplo, en Breakout, el agente puede hacer muchos movimientos rápidos de paleta hacia la izquierda y hacia la derecha mientras la paleta finalmente se abre camino a través del campo para golpear la pelota y recibir la recompensa. ¿Todas esas acciones previas fueron realmente necesarias para obtener esa recompensa? El algoritmo de policy gradient tendría que tratar cada acción como si fuera necearia y ajustar las probabilidades es consecuencia.</p>
<div class="row mt-3">
    <div class="col-sm mt-3 mt-md-0">
        <figure>

  <picture>
    
    <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/problema2_policy-480.webp" />
    <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/problema2_policy-800.webp" />
    <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/problema2_policy-1400.webp" />
    

    <!-- Fallback to the original file -->
    <img src="/assets/img/problema2_policy.png" class="img-fluid rounded z-depth-1" width="auto" height="auto" onerror="this.onerror=null; $('.responsive-img-srcset').remove();" />

  </picture>

</figure>

    </div>
</div>
<div class="caption">
    Problema de la sensibilidad.
</div>

<h3 id="42-aprendizaje-basado-en-la-función-de-valor-value-function-based-learning">4.2. Aprendizaje basado en la función de valor (value function-based learning):</h3>
<p>Con un agente basado en la función de valor, una función toma el estado y una de las posibles acciones de ese estado y generaría el valor de tomar esa acción.
\(\text{valor} = f(\text{observaciones del estado}, \text{acción})\)</p>

<blockquote>
  <p>El valor va a indicar qué tan buena es la acción estando en ese estado.</p>
</blockquote>

<p>Esta función sola no es suficiente para representar la política, ya que genera un valor y la política necesita generar una acción. Por lo tanto, la política sería usar esta función para chequear el valor de cada posible acción de un estado dado y elegir la acción con el valor más alto.</p>

<div class="row mt-3">
    <div class="col-sm mt-3 mt-md-0">
        <figure>

  <picture>
    
    <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/value_function-480.webp" />
    <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/value_function-800.webp" />
    <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/value_function-1400.webp" />
    

    <!-- Fallback to the original file -->
    <img src="/assets/img/value_function.png" class="img-fluid rounded z-depth-1" width="auto" height="auto" onerror="this.onerror=null; $('.responsive-img-srcset').remove();" />

  </picture>

</figure>

    </div>
</div>
<div class="caption">
    La política en value function-based learning es elegir la acción con el valor más alto dado un estado. Este valor es calculado por la función de valor.
</div>

<p>Esta función se llama <strong>crítico</strong>, ya que critica las posbiles acciones que el agente puede elegir.</p>

<div class="row mt-3">
    <div class="col-sm mt-3 mt-md-0">
        <figure>

  <picture>
    
    <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/critico-480.webp" />
    <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/critico-800.webp" />
    <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/critico-1400.webp" />
    

    <!-- Fallback to the original file -->
    <img src="/assets/img/critico.png" class="img-fluid rounded z-depth-1" width="auto" height="auto" onerror="this.onerror=null; $('.responsive-img-srcset').remove();" />

  </picture>

</figure>

    </div>
</div>
<div class="caption">
    Crítico.
</div>

<p>Esta función se puede representar con un tabla si es que los espacios de acción y estado son discretos, o con una red neuronal si son continuos. La diferencia en este último caso, es que entran las observaciones de estado y las acciones, y salen los valores de esos pares estado/acción, y la política elige la acción con valor más alto.</p>

<div class="row mt-3">
    <div class="col-sm mt-3 mt-md-0">
        <figure>

  <picture>
    
    <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/tablaq1-480.webp" />
    <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/tablaq1-800.webp" />
    <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/tablaq1-1400.webp" />
    

    <!-- Fallback to the original file -->
    <img src="/assets/img/tablaq1.png" class="img-fluid rounded z-depth-1" width="auto" height="auto" onerror="this.onerror=null; $('.responsive-img-srcset').remove();" />

  </picture>

</figure>

    </div>
</div>
<div class="caption">
    Tabla Q.
</div>

<h4 id="tabla-q-como-crítico">Tabla Q como crítico</h4>
<p>En el caso de que el espacio de estados y de acciones fueran discretos los representamos con una tabla Q. El agente aprende estos valores a través de Q-learning:</p>
<ol>
  <li>Con Q-learning, se comienza inicializando la tabla con ceros, por lo que todas las acciones tienen el mismo aspecto para el agente.</li>
  <li>El agente realiza una acción aleatoria, llega a un nuevo estado y recoge la recompensa del entorno.</li>
  <li>El agente usa esa recompensa como nueva información para acualizar el valor del estado anterior y la acción que tomó usando la ecuación de Bellman.</li>
</ol>

<h4 id="ecuación-de-bellman">Ecuación de Bellman</h4>
<p>Permite al agente resolver la tabla Q a lo largo del tiempo dividiendo todo el problema en varios pasos más simples. En lugar de resolver el valor real de un par estado/acción en un solo paso, el agente actualizará el valor cada vez que se visite el par estado/acción a través de la programación dinámica.</p>

<p>La ecuación de Bellman es la siguiente:</p>

<div class="row mt-3">
    <div class="col-sm mt-3 mt-md-0">
        <figure>

  <picture>
    
    <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/ec_bellman-480.webp" />
    <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/ec_bellman-800.webp" />
    <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/ec_bellman-1400.webp" />
    

    <!-- Fallback to the original file -->
    <img src="/assets/img/ec_bellman.png" class="img-fluid rounded z-depth-1" width="auto" height="auto" onerror="this.onerror=null; $('.responsive-img-srcset').remove();" />

  </picture>

</figure>

    </div>
</div>
<div class="caption">
    Ecuación de Bellman.
</div>

<p>Una vez que el agente ha realizado una acción desde el estado <em>S</em>, recibe una recompensa.</p>

<div class="row mt-3">
    <div class="col-sm mt-3 mt-md-0">
        <figure>

  <picture>
    
    <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/ec_bellman2-480.webp" />
    <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/ec_bellman2-800.webp" />
    <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/ec_bellman2-1400.webp" />
    

    <!-- Fallback to the original file -->
    <img src="/assets/img/ec_bellman2.png" class="img-fluid rounded z-depth-1" width="auto" height="auto" onerror="this.onerror=null; $('.responsive-img-srcset').remove();" />

  </picture>

</figure>

    </div>
</div>

<p>El valor es más que la recompensa instantánea de una acción; es el rendimiento máximo esperado en el futuro. Por lo tanto, el valor del par estado/acción es la recompensa que el agente acaba de recibir más la recompensa que el agente espera cobrar en el futuro.</p>

<div class="row mt-3">
    <div class="col-sm mt-3 mt-md-0">
        <figure>

  <picture>
    
    <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/ec_bellman3-480.webp" />
    <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/ec_bellman3-800.webp" />
    <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/ec_bellman3-1400.webp" />
    

    <!-- Fallback to the original file -->
    <img src="/assets/img/ec_bellman3.png" class="img-fluid rounded z-depth-1" width="auto" height="auto" onerror="this.onerror=null; $('.responsive-img-srcset').remove();" />

  </picture>

</figure>

    </div>
</div>

<p>Descontamos las recompensas futuras por \(\gamma\) para que el agente no dependa demasiado de las recompensas en el futuro. \(\gamma\) es un número entre 0 (no mira recompensas futuras para evaluar el valor) y 1 (mira recompensas infinitamente lejanas en el futuro).</p>

<div class="row mt-3">
    <div class="col-sm mt-3 mt-md-0">
        <figure>

  <picture>
    
    <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/ec_bellman4-480.webp" />
    <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/ec_bellman4-800.webp" />
    <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/ec_bellman4-1400.webp" />
    

    <!-- Fallback to the original file -->
    <img src="/assets/img/ec_bellman4.png" class="img-fluid rounded z-depth-1" width="auto" height="auto" onerror="this.onerror=null; $('.responsive-img-srcset').remove();" />

  </picture>

</figure>

    </div>
</div>

<p>La suma es ahora el nuevo valor del par de estado y acción \((s,a)\), y lo comparamos con la estimación anterior para obtener el error.</p>

<div class="row mt-3">
    <div class="col-sm mt-3 mt-md-0">
        <figure>

  <picture>
    
    <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/ec_bellman5-480.webp" />
    <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/ec_bellman5-800.webp" />
    <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/ec_bellman5-1400.webp" />
    

    <!-- Fallback to the original file -->
    <img src="/assets/img/ec_bellman5.png" class="img-fluid rounded z-depth-1" width="auto" height="auto" onerror="this.onerror=null; $('.responsive-img-srcset').remove();" />

  </picture>

</figure>

    </div>
</div>

<p>El error se multiplica por una tasa de aprendizaje que le da control sobre si debe reemplazar la estimación del valor anterior con la nueva \((\alpha = 1)\) o empujar el valor anterior en la dirección del nuevo \((\alpha &lt; 1)\).</p>

<div class="row mt-3">
    <div class="col-sm mt-3 mt-md-0">
        <figure>

  <picture>
    
    <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/ec_bellman6-480.webp" />
    <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/ec_bellman6-800.webp" />
    <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/ec_bellman6-1400.webp" />
    

    <!-- Fallback to the original file -->
    <img src="/assets/img/ec_bellman6.png" class="img-fluid rounded z-depth-1" width="auto" height="auto" onerror="this.onerror=null; $('.responsive-img-srcset').remove();" />

  </picture>

</figure>

    </div>
</div>

<p>Finalmente, el valor \(\delta\) resultante se agrega a la estimación anterior y se actualiza la tabla Q.</p>

<div class="row mt-3">
    <div class="col-sm mt-3 mt-md-0">
        <figure>

  <picture>
    
    <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/ec_bellman7-480.webp" />
    <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/ec_bellman7-800.webp" />
    <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/ec_bellman7-1400.webp" />
    

    <!-- Fallback to the original file -->
    <img src="/assets/img/ec_bellman7.png" class="img-fluid rounded z-depth-1" width="auto" height="auto" onerror="this.onerror=null; $('.responsive-img-srcset').remove();" />

  </picture>

</figure>

    </div>
</div>

<h4 id="crítico-como-red-neuronal">Crítico como red neuronal</h4>
<p>Imaginemos un péndulo invertido. Hay dos estados: ángulo y velocidad angular. Ambos son continuos.</p>

<p>La función de valor o crítico se representa con una red neuronal. La idea es la misma que con una tabla:</p>
<ol>
  <li>ingresamos las observaciones de estado y una acción</li>
  <li>La red neuronal devuelve el valor de ese par de estado/acción</li>
  <li>La política es elegir la acción con el valor más alto.</li>
</ol>

<div class="row mt-3">
    <div class="col-sm mt-3 mt-md-0">
        <figure>

  <picture>
    
    <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/value_critico-480.webp" />
    <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/value_critico-800.webp" />
    <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/value_critico-1400.webp" />
    

    <!-- Fallback to the original file -->
    <img src="/assets/img/value_critico.png" class="img-fluid rounded z-depth-1" width="auto" height="auto" onerror="this.onerror=null; $('.responsive-img-srcset').remove();" />

  </picture>

</figure>

    </div>
</div>
<div class="caption">
    Red neuronal como crítico en value function-based learning.
</div>

<p>Con el tiempo, la red convergerá lentamente en una función que genera el valor real de cada acción en cualquier lugar del espacio de estado continuo.</p>

<h4 id="lo-malo-de-las-políticas-basadas-en-función-de-valor">Lo malo de las políticas basadas en función de valor</h4>
<p>Podemos utilizar una red neuronal para definir la función de valor para espacios de estado continuos. Si el péndulo invertido tiene un espacio de acción discreto, podemos alimentar las acciones discretas a la red crítica de una en una.</p>

<div class="row mt-3">
    <div class="col-sm mt-3 mt-md-0">
        <figure>

  <picture>
    
    <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/pendulo-480.webp" />
    <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/pendulo-800.webp" />
    <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/pendulo-1400.webp" />
    

    <!-- Fallback to the original file -->
    <img src="/assets/img/pendulo.png" class="img-fluid rounded z-depth-1" width="auto" height="auto" onerror="this.onerror=null; $('.responsive-img-srcset').remove();" />

  </picture>

</figure>

    </div>
</div>
<div class="caption">
    Infinitos estados, pocas acciones posibles.
</div>

<p>Las políticas basadas en la función de valor no funcionan bien para espacios de acción continuos. Esto se debe a que no hay forma de calcular el valor uno a la vez para acción infinita para encontrar el valor máximo. Incluso para un espacio de acción grande, pero no infinito, esto se vuelve computacionalmente costoso. Esto es desafortunado porque a menudo en los problemas de control tiene un espacio de acción continuo, como aplicar un rango continuo de torque a un problema de péndulo invertido.</p>

<p>¿Qué podemos hacer? Implementar un método policy-gradient vainilla (visto anteriormente en la sección pasada). Estos algoritmos pueden manear espacios de acción continua, pero tienen problemas para converger cuando hay una gran variación en las recompensas y el gradiente es ruidoso. Podemos fusionar las dos técnincas (value function-based learning y policy function-based) en una clase de algoritmos llamados actor-crítico.</p>

<h3 id="43-algoritmos-actor-crítico">4.3. Algoritmos Actor-crítico</h3>
<p>Fusión de las dos técnicas anteriores.</p>
<ul>
  <li>Actor: red que está tratando de tomar lo que cree que es la mejor acción dado el estado actual, tal cual se hacía en los algoritmos de policy function (<strong>sección 4.1.</strong>).</li>
  <li>Crítico: segunda red que está tratando de estimar el valor del par estado/acción que tomó el actor, como se hacía en los algoritmos de value function (<strong>sección 4.2.</strong>).</li>
</ul>

<p>Este enfoque funciona para espacios de acción continua porque el crítico solo necesita mirar la accción individual que realizó el actor y no necesita tratar de encontrar la mejor acción evaluándolas todas.</p>

<blockquote>
  <p>Actor-crítico funciona para espacios de acción y estados continuos.</p>
</blockquote>

<div class="row mt-3">
    <div class="col-sm mt-3 mt-md-0">
        <figure>

  <picture>
    
    <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/metodo_actor_critico-480.webp" />
    <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/metodo_actor_critico-800.webp" />
    <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/metodo_actor_critico-1400.webp" />
    

    <!-- Fallback to the original file -->
    <img src="/assets/img/metodo_actor_critico.png" class="img-fluid rounded z-depth-1" width="auto" height="auto" onerror="this.onerror=null; $('.responsive-img-srcset').remove();" />

  </picture>

</figure>

    </div>
</div>
<div class="caption">
    Algoritmos de actor-crítico.
</div>

<h4 id="funcionamiento-de-actor-crítico">Funcionamiento de actor-crítico</h4>
<ul>
  <li>Paso 1: El actor elige una acción de la misma manera que lo haría un algoritmo de policy function y se aplica al entorno.</li>
</ul>
<div class="row mt-3">
    <div class="col-sm mt-3 mt-md-0">
        <figure>

  <picture>
    
    <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/paso1_ac-480.webp" />
    <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/paso1_ac-800.webp" />
    <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/paso1_ac-1400.webp" />
    

    <!-- Fallback to the original file -->
    <img src="/assets/img/paso1_ac.png" class="img-fluid rounded z-depth-1" width="auto" height="auto" onerror="this.onerror=null; $('.responsive-img-srcset').remove();" />

  </picture>

</figure>

    </div>
</div>

<ul>
  <li>Paso 2: El crítico estima el valor para ese par estado/acción actual.</li>
</ul>
<div class="row mt-3">
    <div class="col-sm mt-3 mt-md-0">
        <figure>

  <picture>
    
    <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/paso2_ac-480.webp" />
    <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/paso2_ac-800.webp" />
    <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/paso2_ac-1400.webp" />
    

    <!-- Fallback to the original file -->
    <img src="/assets/img/paso2_ac.png" class="img-fluid rounded z-depth-1" width="auto" height="auto" onerror="this.onerror=null; $('.responsive-img-srcset').remove();" />

  </picture>

</figure>

    </div>
</div>

<ul>
  <li>Paso 3: El crítico usa la recompensa recibida para determinar la precisión de su predicción de valor.
    <ul>
      <li>El error es: \(\text{error}=\text{valor nuevo estimado del estado anterior}-\text{valor viejo para el estado anterior}\), ambos dados por la red crítica.</li>
      <li>El estado anterior es el estado desde el cual se ejecutó la acción actual.</li>
      <li>El nuevo valor estimado se basa en la recompensa recibida y el valor descontado del estado actual.</li>
      <li>El error permite que el crítico se de cuenta si las cosas salieron mejor o peor de lo esperado.</li>
    </ul>
  </li>
</ul>
<div class="row mt-3">
    <div class="col-sm mt-3 mt-md-0">
        <figure>

  <picture>
    
    <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/paso3_ac-480.webp" />
    <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/paso3_ac-800.webp" />
    <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/paso3_ac-1400.webp" />
    

    <!-- Fallback to the original file -->
    <img src="/assets/img/paso3_ac.png" class="img-fluid rounded z-depth-1" width="auto" height="auto" onerror="this.onerror=null; $('.responsive-img-srcset').remove();" />

  </picture>

</figure>

    </div>
</div>

<ul>
  <li>Paso 4: El crítico usa este error para actualizarse a sí mismo para que así tenga una mejor predicción la próxima vez que esté en ese estado.</li>
</ul>
<div class="row mt-3">
    <div class="col-sm mt-3 mt-md-0">
        <figure>

  <picture>
    
    <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/paso4_ac-480.webp" />
    <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/paso4_ac-800.webp" />
    <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/paso4_ac-1400.webp" />
    

    <!-- Fallback to the original file -->
    <img src="/assets/img/paso4_ac.png" class="img-fluid rounded z-depth-1" width="auto" height="auto" onerror="this.onerror=null; $('.responsive-img-srcset').remove();" />

  </picture>

</figure>

    </div>
</div>

<ul>
  <li>Paso 5: El actor también se actualiza con la respuesta del crítico para que pueda ajustar sus probabilidades de volver a tomar esa acción en el futuro. De esta forma, la política asciende la pendiente de la recompensa en la dirección que recomienda el crítico en lugar de usar las recompensas directamente.</li>
</ul>
<div class="row mt-3">
    <div class="col-sm mt-3 mt-md-0">
        <figure>

  <picture>
    
    <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/paso5_ac-480.webp" />
    <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/paso5_ac-800.webp" />
    <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/paso5_ac-1400.webp" />
    

    <!-- Fallback to the original file -->
    <img src="/assets/img/paso5_ac.png" class="img-fluid rounded z-depth-1" width="auto" height="auto" onerror="this.onerror=null; $('.responsive-img-srcset').remove();" />

  </picture>

</figure>

    </div>
</div>

<p>El actor y el crítico son redes neuronales que intentan aprender el comportamiento óptimo:</p>
<ul>
  <li>El actor está aprendiendo las acciones correctas usando la retroalimentación del crítico.</li>
  <li>El crítico está aprendiendo la función de valor para poder criticar correctamente la acción del actor.</li>
</ul>

<blockquote>
  <p>Los métodos de actor-crítico aprovechan lo mejor de los algoritmos de value function y policy function. Los métodos actor-crítico permiten acelerar el aprendizaje cuando hay gran variación en la recompensa recibida. Además, pueden manejar tanto espacios de estado como de acción continuos.</p>
</blockquote>

<div class="row mt-3">
    <div class="col-sm mt-3 mt-md-0">
        <figure>

  <picture>
    
    <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/red_ac-480.webp" />
    <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/red_ac-800.webp" />
    <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/red_ac-1400.webp" />
    

    <!-- Fallback to the original file -->
    <img src="/assets/img/red_ac.png" class="img-fluid rounded z-depth-1" width="auto" height="auto" onerror="this.onerror=null; $('.responsive-img-srcset').remove();" />

  </picture>

</figure>

    </div>
</div>
<div class="caption">
    En los algoritmos Actor-crítico se deben configurar dos redes neuronales al crear el agente: una para el actor y otra para el crítico.
</div>

<h2 id="5-deployment">5. Deployment</h2>
<h3 id="51-despliegue-de-política">5.1. Despliegue de política</h3>
<p>Si el aprendizaje se hizo físicamente, o sea, en un entorno real, la política aprendida ya está en el agente y puede explotarse.</p>

<p>Si el aprendizaje se hizo fuera de línea (offline), o sea, en una simulación; si la política es lo suficientemente óptima, se detiene el aprendizaje y la política estática se puede implementar en cualquier número de objetivos.</p>

<div class="row mt-3">
    <div class="col-sm mt-3 mt-md-0">
        <figure>

  <picture>
    
    <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/deploy_politica-480.webp" />
    <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/deploy_politica-800.webp" />
    <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/deploy_politica-1400.webp" />
    

    <!-- Fallback to the original file -->
    <img src="/assets/img/deploy_politica.png" class="img-fluid rounded z-depth-1" width="auto" height="auto" onerror="this.onerror=null; $('.responsive-img-srcset').remove();" />

  </picture>

</figure>

    </div>
</div>
<div class="caption">
    Despliegue de la política en hardaware objetivo.
</div>

<h3 id="52-despligue-del-algoritmo-de-aprendizaje">5.2. Despligue del algoritmo de aprendizaje</h3>
<p>En algunos casos puede ser necesario continuar aprendiendo con hardware físico real después del despliegue. Algunos entornos pueden ser difíciles de modelar con precisión. Esto quiere decir que una política óptima para el modelo puede que no lo sea para el entorno real. Otra razón puede ser que el entorno cambia lentamente con el tiempo y el agente se debe adaptar a esos cambios.</p>

<p>Es por todo esto que se recomienda desplegar o implementar tanto la política estática como el algoritmo de aprendizaje en el hardware de destino. Con esta configuración, tenemos la opción de ejecutar la política estática, desactivando el aprendizaje; o continuar actualizando la política, activando el aprendizaje.</p>

<div class="row mt-3">
    <div class="col-sm mt-3 mt-md-0">
        <figure>

  <picture>
    
    <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/deploy_alg-480.webp" />
    <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/deploy_alg-800.webp" />
    <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/deploy_alg-1400.webp" />
    

    <!-- Fallback to the original file -->
    <img src="/assets/img/deploy_alg.png" class="img-fluid rounded z-depth-1" width="auto" height="auto" onerror="this.onerror=null; $('.responsive-img-srcset').remove();" />

  </picture>

</figure>

    </div>
</div>
<div class="caption">
    Despliegue del algoritmo RL en hardaware objetivo.
</div>

<div class="row mt-3">
    <div class="col-sm mt-3 mt-md-0">
        <figure>

  <picture>
    
    <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/deploy_alg2-480.webp" />
    <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/deploy_alg2-800.webp" />
    <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/deploy_alg2-1400.webp" />
    

    <!-- Fallback to the original file -->
    <img src="/assets/img/deploy_alg2.png" class="img-fluid rounded z-depth-1" width="auto" height="auto" onerror="this.onerror=null; $('.responsive-img-srcset').remove();" />

  </picture>

</figure>

    </div>
</div>
<div class="caption">
    El aprendizaje puede ser complementario entre el entorno simulado y el entorno real. 
</div>

<h2 id="ix-lo-malo-del-reinforcement-learning">IX. Lo malo del Reinforcement Learning</h2>
<p>Existen dos problemas principales:</p>
<ol>
  <li>¿Cómo sabemos que la solución que entrega RL funciona?</li>
  <li>¿Se puede ajustar manualmente si no es perfecto?</li>
</ol>

<div class="row mt-3">
    <div class="col-sm mt-3 mt-md-0">
        <figure>

  <picture>
    
    <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/mal_rl-480.webp" />
    <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/mal_rl-800.webp" />
    <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/mal_rl-1400.webp" />
    

    <!-- Fallback to the original file -->
    <img src="/assets/img/mal_rl.png" class="img-fluid rounded z-depth-1" width="auto" height="auto" onerror="this.onerror=null; $('.responsive-img-srcset').remove();" />

  </picture>

</figure>

    </div>
</div>
<div class="caption">
    Podemos tener un agente perfecto, un entorno perfecto y un algoritmo RL que converje a una solución, sin embargo, la política puede que no sea tan buena.
</div>

<h3 id="lo-inexplicable-de-la-red-neuronal">Lo inexplicable de la red neuronal</h3>
<p>La política se conforma de una red neuronal con:</p>
<ul>
  <li>Muchos weights</li>
  <li>Muchos bias</li>
  <li>funciones de activación no lineales</li>
  <li>A esto hay que sumarle la estructura de la red neuronal
Todo esto resulta en una función muy compleja!</li>
</ul>

<div class="row mt-3">
    <div class="col-sm mt-3 mt-md-0">
        <figure>

  <picture>
    
    <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/complex_red-480.webp" />
    <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/complex_red-800.webp" />
    <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/complex_red-1400.webp" />
    

    <!-- Fallback to the original file -->
    <img src="/assets/img/complex_red.png" class="img-fluid rounded z-depth-1" width="auto" height="auto" onerror="this.onerror=null; $('.responsive-img-srcset').remove();" />

  </picture>

</figure>

    </div>
</div>
<div class="caption">
    Una red neuronal es muy compleja.
</div>

<blockquote>
  <p>La función que aproxima la red neuronal es una función muy compleja. Se comporta como una caja negra para el diseñador. No conocemos la razón del valor de un weight o bias dado dentro de la red. Si la política no cumple con una especificación o si el entorno operativo cambia, no sabremos como ajustar la política para abordar ese problema.</p>
</blockquote>

<div class="row mt-3">
    <div class="col-sm mt-3 mt-md-0">
        <figure>

  <picture>
    
    <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/blackbox-480.webp" />
    <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/blackbox-800.webp" />
    <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/blackbox-1400.webp" />
    

    <!-- Fallback to the original file -->
    <img src="/assets/img/blackbox.png" class="img-fluid rounded z-depth-1" width="auto" height="auto" onerror="this.onerror=null; $('.responsive-img-srcset').remove();" />

  </picture>

</figure>

    </div>
</div>
<div class="caption">
    La red neuronal es una caja negra.
</div>

<p>No comprendemos el porqué de la solución entregada. En cambio, un sistema de control se puede explicar, dividir, ajustar, aislar las partes conflictivas, repararlas y volver a juntarlas. Una red neuronal <strong>NO</strong>. Por ejemplo, si tenemos un PID con un sistema \(x\) y lo cambiamos al sistema \(y\), simplemente cambiamos las ganancias.</p>

<p>Si el sistema no se comporta como queremos, entonces la política no es del todo correcta. ¿Corregimos la parte defectuosa? No podemos! Tenemos que rediseñar el agente o el modelo del entorno y volver a entrenarlo, lo que puede tomar tiempo.</p>

<h3 id="cómo-verificamos-un-sistema-de-control-tradicional">¿Cómo verificamos un sistema de control tradicional?</h3>
<p>A través de un testeo: simulación + modelo, y, con hardware físico; y verficamos que el sistema cumpla con las especificaciones, es decir, que hace lo correcto en todo el espacio de estados y en presencia de perturbaciones y fallas de hardware.</p>

<p>Debemos hacer este mismo nivel de prueba con una política de RL. Si encontramos un error, se tiene que volver a entrenar la política (previo a un rediseño).</p>

<p>El ciclo es el siguiente:</p>
<ol>
  <li>Rediseño</li>
  <li>Entrenamiento</li>
  <li>Testeo</li>
</ol>

<p>Repetir hasta que se cumplan las especificaciones.</p>

<h3 id="el-problema-de-la-precisión-del-modelo-del-entorno">El problema de la precisión del modelo del entorno</h3>
<p>Es difícil desarrollar un modelo suficientemente realista que tenga en cuenta todas las dinámicas importantes del sistema, además que considere el ruido y las perturbaciones. En algún momento no reflejará la realidad de forma perfecta, por lo que se deben hacer prueba físicas en vez de confiar 100% en la simulación con un modelo.</p>

<div class="row mt-3">
    <div class="col-sm mt-3 mt-md-0">
        <figure>

  <picture>
    
    <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/problema1-480.webp" />
    <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/problema1-800.webp" />
    <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/problema1-1400.webp" />
    

    <!-- Fallback to the original file -->
    <img src="/assets/img/problema1.png" class="img-fluid rounded z-depth-1" width="auto" height="auto" onerror="this.onerror=null; $('.responsive-img-srcset').remove();" />

  </picture>

</figure>

    </div>
</div>
<div class="caption">
    El modelo no es perfecto, entonces el controlador o agente RL tampoco.
</div>

<p>Podríamos ajustar y modificar un controlador en control tradicional, pero una red neuronal no.</p>

<p>Como no podemos construir un modelo 100% realista, todo agente que entrene con ese modelo estará <strong>ligeramente equivocado</strong>.</p>

<blockquote>
  <p>La solución es terminar de entrenar el agente en hardware físico, lo que puede ser desafiante.</p>
</blockquote>

<h3 id="cómo-verificamos-si-la-política-cumple-las-especificaciones">¿Cómo verificamos si la política cumple las especificaciones?</h3>
<p>Con una política aprendida, es difícil predecir cómo se comportará el sistema en un estado en función de su comportamiento en otro. Por ejemplo, entrenamos a un agente para que controle la velocidad de un motor eléctrico haciendo que aprenda a seguir un step input de 0 a 100 RPM.</p>
<ul>
  <li>Entrada 1: step input 0 a 100 RPM</li>
  <li>Entrenamos el agente para que siga la señal de referencia de pasar de 0 a 100 RPM.</li>
  <li>La salida será una curva con el aumento paulatino de RPM hasta llegar a 100.</li>
</ul>

<div class="row mt-3">
    <div class="col-sm mt-3 mt-md-0">
        <figure>

  <picture>
    
    <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/step100-480.webp" />
    <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/step100-800.webp" />
    <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/step100-1400.webp" />
    

    <!-- Fallback to the original file -->
    <img src="/assets/img/step100.png" class="img-fluid rounded z-depth-1" width="auto" height="auto" onerror="this.onerror=null; $('.responsive-img-srcset').remove();" />

  </picture>

</figure>

    </div>
</div>
<div class="caption">
    Step 100 RPM.
</div>

<p>Sin embargo:</p>
<ul>
  <li>Entrada 2: step input 0 a 150 RPM
¿El agente se comportará igual? La política anteriormente aprendida se comportará de forma similar a como se comportó con el step input anterior? No podemos saberlo de antemano, debemos testearlo.</li>
</ul>

<div class="row mt-3">
    <div class="col-sm mt-3 mt-md-0">
        <figure>

  <picture>
    
    <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/step150-480.webp" />
    <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/step150-800.webp" />
    <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/step150-1400.webp" />
    

    <!-- Fallback to the original file -->
    <img src="/assets/img/step150.png" class="img-fluid rounded z-depth-1" width="auto" height="auto" onerror="this.onerror=null; $('.responsive-img-srcset').remove();" />

  </picture>

</figure>

    </div>
</div>
<div class="caption">
    Step 150 RPM.
</div>

<p>¿Y que pasa con un step input de 30-75? ¿o de 80-93? Tendríamos que probar <strong>todas</strong> las combinaciones para demostrar que la política funciona en un 100%. No hay una verificación matemática que cubra todo el rango.</p>

<p>Un cambio ligero puede hacer que se active un conjunto de neuronas completamente diferentes y produzca un resultado no deseado. Debemos probarlo.</p>

<blockquote>
  <p>Probar más implica menos riesgo. Pero, ¿la política es 100% certera? Debemos probar todas las combinaciones de entrada. Pero, ¿y si la entrada es muy grande? Es imposible!</p>
</blockquote>

<h3 id="métodos-formales-de-verificación">Métodos formales de verificación</h3>
<p>Las redes neuronales dificultan la verificación formal. La verificación formal garantiza que se cumpla alguna condición proporcionando una prueba formal en vez de un test.</p>

<p>Ejemplos:</p>
<ol>
  <li>Inspeccionando el código que demuestra que se cumplirá algo siempre, por ejemplo, que la señal sea no negativa.</li>
  <li>Cálculo de factores de estabilidad y robustez, como los márgenes de ganancia y fase.
    <ul>
      <li>Esto es difícil para una red neuronal, ya que no podemos ofrecer garantías sobre cómo se comportará. No hay métodos para determinar su robustez o estabilidad. No podemos expllicar qué hace la función internamente.</li>
    </ul>
  </li>
</ol>

<h3 id="reducción-del-problema">Reducción del problema</h3>
<p>Reducimos el alcance del agente RL para reducir la escala de estos problemas.</p>

<blockquote>
  <p>Solución: RL + control. RL se preocupará de un problema muy especializado, algo muy difícil de resolver con control tradicional. Con respecto al control, los controladores se preocuparán de lo demás.</p>
</blockquote>

<p>Una política más pequeña:</p>
<ul>
  <li>Está más enfocada y es más fácil de entender</li>
  <li>Impacto limitado en todo el sistema</li>
  <li>Menos tiempo de entrenamiento</li>
</ul>

<p>Esta sería la solución, sin embargo, <strong>aún no podemos garantizar estabilidad, cumplimiento de especificaciones o resistencia a incertidumbres</strong>.</p>

<div class="row mt-3">
    <div class="col-sm mt-3 mt-md-0">
        <figure>

  <picture>
    
    <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/reduccion_problema-480.webp" />
    <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/reduccion_problema-800.webp" />
    <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/reduccion_problema-1400.webp" />
    

    <!-- Fallback to the original file -->
    <img src="/assets/img/reduccion_problema.png" class="img-fluid rounded z-depth-1" width="auto" height="auto" onerror="this.onerror=null; $('.responsive-img-srcset').remove();" />

  </picture>

</figure>

    </div>
</div>
<div class="caption">
    Reducimos la política para disminuir la complejidad.
</div>

<h4 id="cómo-lograr-robustez-y-estabilidad">Cómo lograr robustez y estabilidad</h4>
<p>Haciendo la política RL más robusta.</p>

<ul>
  <li>1.- Entrenar el agente en un entorno donde los parámetros importantes del entorno se ajustan cada vez que se ejecute la simulación. 
  Por ejemplo, un robot caminante. Al comienzo de cada episodio cambiamos el valor del torque máximo.
\(\begin{aligned}
&amp; \text {Tabla 3. Configuración de los parámetros del entorno}\\
&amp;\begin{array}{cccccc}
\hline \hline \text { Episodio } &amp; \text { Torque } &amp; \text { Longitud } &amp; \text { Delay } &amp; \text { Referencia } &amp; \text{ ... }\\
\hline
1 &amp; 2 \text{ Nm.} &amp; 1 \text{ Cm.} &amp; 10 \text{ Ms.} &amp; \text{Step} &amp; \text{ ... }\\
2 &amp; 2.5 \text{ Nm.} &amp; 1.3 \text{ Cm.} &amp; 8 \text{ Ms.} &amp; \text{Ramp} &amp; \text{ ... }\\
3 &amp; 2.1 \text{ Nm.} &amp; 1.7 \text{ Cm.} &amp; 14 \text{ Ms.} &amp; \text{Impulse} &amp; \text{ ... }\\
\text{ ... } &amp; \text{ ... } &amp; \text{ ... } &amp; \text{ ... } &amp; \text{ ... } &amp; \text{ ... }\\
\hline
\end{array}
\end{aligned}\)</li>
</ul>

<p>La política será más robusta para esos torques, y si hacemos lo mismo para <code class="language-plaintext highlighter-rouge">longitud</code>, <code class="language-plaintext highlighter-rouge">delay</code>, y <code class="language-plaintext highlighter-rouge">referencia</code>; más robusta será.</p>

<p>La política eventualmente convergerá en algo robusto para esos márgenes, produciendo un diseño robusto en general. El resultado manejará un rango más amplio dentro del espacio de estados operativo.</p>

<ul>
  <li>2.- Para la seguridad, se puede hacer un software que ponga en <em>modo seguro</em> al agente en una situación peligrosa. Esto protegerá al sistema, lo que permite aprender cómo falla y ajustar la recompensa y entrenar para abordar esa falla.</li>
</ul>

<h2 id="x-rl--control">X. RL + Control</h2>
<p>Utilizar el RL como herramienta para optimizar las ganancias del controlador en un sistema de control de arquitectura tradicional. Por ejemplo, un sistema de control con muchos bucles y controladores anidados, cada uno con varias ganancias. En vez de ajustar manualmente cada una de estas ganancias, puedes configurar un agente RL para aprender los mejores valores para todas a la vez.</p>

<div class="row mt-3">
    <div class="col-sm mt-3 mt-md-0">
        <figure>

  <picture>
    
    <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/rl_con_control-480.webp" />
    <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/rl_con_control-800.webp" />
    <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/rl_con_control-1400.webp" />
    

    <!-- Fallback to the original file -->
    <img src="/assets/img/rl_con_control.png" class="img-fluid rounded z-depth-1" width="auto" height="auto" onerror="this.onerror=null; $('.responsive-img-srcset').remove();" />

  </picture>

</figure>

    </div>
</div>
<div class="caption">
    Agente RL aprenderá las ganancias del controlador.
</div>

<h3 id="pasos-de-rl--control">Pasos de RL + Control</h3>
<ol>
  <li>Entorno: Sistema de control y planta</li>
  <li>Recompensa: qué tan bien se desempeña el sistema y cuanto esfuerzo necesitó</li>
  <li>Acciones: Ganancias del controlador</li>
  <li>Después de cada episodio, el algoritmo de aprendizaje modifica la red de manera que las ganancias se mueven en la dirección que aumenta la recompensa (más desempeño y menos esfuerzo).
    <ul>
      <li>Inicialmente, o sea, en el episodio 1, la red se inicializa aleatoriamente y generar los valores aleatorios, los cuales serán las ganancias y se ejecuta la simulación.</li>
    </ul>
  </li>
  <li>Codificamos los valores de ganancia estáticos finales dentro del controlador.</li>
</ol>

<h4 id="ventajas">Ventajas</h4>
<ol>
  <li>Tenemos un sistema de control tradicional</li>
  <li>Sistema de control verificable</li>
  <li>Sistema de control manualmente ajustable en hardware</li>
  <li>Valores de ganancia óptimos gracias a RL</li>
</ol>

<div class="row mt-3">
    <div class="col-sm mt-3 mt-md-0">
        <figure>

  <picture>
    
    <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/rl_con_control2-480.webp" />
    <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/rl_con_control2-800.webp" />
    <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/rl_con_control2-1400.webp" />
    

    <!-- Fallback to the original file -->
    <img src="/assets/img/rl_con_control2.png" class="img-fluid rounded z-depth-1" width="auto" height="auto" onerror="this.onerror=null; $('.responsive-img-srcset').remove();" />

  </picture>

</figure>

    </div>
</div>
<div class="caption">
    El agente RL calculará las ganancias óptimas del controlador.
</div>]]></content><author><name></name></author><category term="algoritmos" /><summary type="html"><![CDATA[Definición y workflow del Aprendizaje por refuerzo, y su aplicación para la resolución de problemas de control.]]></summary></entry><entry><title type="html">K-means clustering algorithm</title><link href="https://squirogar.github.io/blog/2023/clustering-kmeans/" rel="alternate" type="text/html" title="K-means clustering algorithm" /><published>2023-06-09T21:41:00+00:00</published><updated>2023-06-09T21:41:00+00:00</updated><id>https://squirogar.github.io/blog/2023/clustering-kmeans</id><content type="html" xml:base="https://squirogar.github.io/blog/2023/clustering-kmeans/"><![CDATA[<p><em>El siguiente contenido fue aprendido del curso de especialización de Machine Learning de Andrew Ng y deeplearning.ai</em></p>

<p>K-means es un algoritmo de clustering. Antes de ver en qué consiste K-means, es bueno saber qué es un algoritmo de clustering.</p>

<h1 id="clustering">Clustering</h1>
<p>Los algoritmos de clustering son un tipo de algoritmo de Unsupervised Learning (Aprendizaje no supervisado) y se encargan de agrupar los datos en <em>clusters</em> o grupos. Específicamente, el algoritmo mira los datos y automáticamente los agrupa, encontrando así la relación o similitud que hay entre ellos. Los puntos que pertenecen a un mismo cluster son más similares entre sí en comparación con puntos de otros clusters. La similitud entre puntos se basa en la distancia que haya entre ellos, por lo que mientras más juntos los puntos en un cluster, más similares son.</p>

<h1 id="diferencia-con-supervised-learning">Diferencia con supervised learning</h1>
<p>En supervised learning tenemos un dataset compuesto por:
\(\{(x^{(1)},y^{(1)}), (x^{(2)},y^{(2)}), (x^{(3)},y^{(3)}), ..., (x^{(m)},y^{(m)})
\}\)</p>

<p>En este dataset tenemos las input features \(x\) y las true labels \(y\).</p>

<p>Pero en unsupervised learning nuestro dataset no tiene estas respuestas correctas o true labels \(y\), sino que sólo está compuesto por las input features \(x\).
\(\{x^{(1)}, x^{(2)}, x^{(3)}, ..., x^{(m)}
\}\)</p>

<p>Como no tenemos las output targets \(y\), no somos capaces de decirle al algoritmo cuál es la respuesta correcta \(y\) que queremos predecir. En vez de eso, vamos a preguntarle al algoritmo que encuentre alguna estructura interesante sobre esta data, por ejemplo, que la agrupe en clusters, así quizas podemos obtener conocimiento que a simple vista sea difícil de adquirir.</p>

<h1 id="k-means-el-algoritmo-de-clustering-más-utilizado">K-means: el algoritmo de clustering más utilizado</h1>
<p>Para ejecutar K-means necesitamos un dataset sin labels \(y\).</p>

<h2 id="i-procedimiento">I. Procedimiento</h2>
<p>El algoritmo consta de 3 pasos. El primero se realiza una sola vez, mientras que los 2 siguientes se ejecutan varias veces. K-means es un algoritmo iterativo, por lo que dependiendo de la cantidad de iteraciones que le demos podremos obtener mejores resultados.</p>

<ol>
  <li>Lo primero que hace K-means es tomar suposiciones aleatorias de dónde podrían estar los centroids de los clusters que queremos encontrar.
    <ul>
      <li>Podemos decidir cuántos clusters vamos a encontrar. Por ejemplo, 2.</li>
      <li>Si elegimos 2, entonces K-means elegirá aleatoriamente 2 puntos donde podrían estar los centroids de estos dos clusters. Como son suposiciones aleatorias iniciales no son particulamente buenas.</li>
      <li>Un <strong>centroid</strong> es el centro de un cluster. Y un <strong>cluster</strong> es un grupo de puntos de datos relacionados.</li>
    </ul>
  </li>
  <li>
    <p>K-means repetidamente hará estos dos pasos:</p>

    <p>2.1 <strong>Asignar puntos de datos a los centroids:</strong> K-means recorrerá cada dato en nuestro dataset y le asignará el centroid más cercano, ya sea el <strong>centroid1</strong> o al <strong>centroid2</strong> (recordar que para este ejemplo elegimos 2 clusters).</p>

    <p>2.2 <strong>Mover los centroids:</strong> K-means mirará todos los puntos de datos asignados al <strong>centroid1</strong> y tomará un promedio de ellos. Luego, moverá el <strong>centroid1</strong> a la ubicación del promedio de estos puntos de datos.Para el <strong>centroid2</strong> se hace exactamente lo mismo: se toman todos los puntos de datos asignados a este centroid y se calcula el promedio, para luego mover el <strong>centroid2</strong> a esa ubicación.</p>
  </li>
  <li>K-means recorrerá todos los datos de nuestro dataset de nuevo y repetirá el <strong>paso 2</strong> completo. En otras palabras, asociaremos cada punto de dato al centroid más cercano (<strong>paso 2.1</strong>). Algunos datos puede que sean asignados a diferentes centroids con el pasar de las iteraciones, esto es normal, ya que queremos determinar correctamente cuáles son sus centroids más cercanos. Después de esto, recalcularemos los centroids (o los moveremos, que es lo mismo) (<strong>paso 2.2</strong>). Hacemos esto hasta que no hayan más cambios en la asignación de los datos a centroids o que no hayan cambios al mover los centroids. En ese momento, K-means habrá <strong>convergido</strong>. Así, se habrán formado 2 clusters conteniendo cada uno data points similares.</li>
</ol>

<h2 id="ii-definición-formal-de-k-means">II. Definición formal de K-means</h2>

<ol>
  <li>Aleatoriamente inicializar \(K\) centroids \(\mu_1, \mu_2, ..., \mu_K\):
    <ul>
      <li>Elegir aleatoriamente una ubicación para los centroids</li>
      <li>\(\mu_1, \mu_2, ..., \mu_K\) son vectores que tienen la misma dimensión que el dataset de ejemplos de entrenamiento \(x^{(1)}, x^{(2)}, ..., x^{(m)}\). Todos los centroids son listas de \(n\) números o vectores de \(n\)-dimensiones, donde \(n\) es el número de features por cada uno de los ejemplos de entrenamiento. Por ejemplo, si \(n=2\), tenemos 2 features \(x_1\) y \(x_2\), entonces \(\mu_1\) y \(\mu_2\) serán vectores con 2 números en ellos.</li>
    </ul>
  </li>
  <li>
    <p>K-means repetidamente llevará a cabo los dos pasos descritos en la sección <strong>I.</strong>:</p>

    <p>2.1 Asignar cada uno de los puntos de datos \(x^{(i)}\) al centroid \(\mu\) más cercano.</p>

    <p>2.2 Mover los centroids de los clusters. Vamos a actualizar la ubicación de los centroids para ser el promedio o media de los datos asignados a cada cluster. Concretamente, veremos todos los puntos de datos asignados al centroid \(\mu\) y calcularemos el promedio. El promedio se calcula para cada dimensión o feature, entonces: veremos el valor de cada punto en el eje x (feature \(x_1\)) y lo promediamos; luego, veremos el valor de cada punto en el eje y (feature \(x_2\)) y lo promediamos. Con esto, tenemos la media de los puntos que pertenecen al cluster \(\mu\) y que será la nueva ubicación de este último. En python esto sería:</p>
    <div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>     a = np.array([x^(1), x^(5), x^(6), x^(10)])
     mu_1 = np.sum(a) / 4
     # x^(m) es un vector de tam n, con n=num. features
     # mu_1 es un vector de tam n, con n=num. features
     # numpy utiliza vectorización, por lo que las operaciones se aplican al vector completo
</code></pre></div>    </div>
  </li>
</ol>

<h2 id="iii-pseudocódigo">III. Pseudocódigo</h2>

<p>En pseudocódigo el algoritmo es:</p>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>Aleatoriamente inicializar K clusters mu_1, mu_2, ..., mu_K

repeat {
	# asignar cada data point al centroid más cercano
	for i = 1 to m:
		c^(i) := índice (de 1 a K) del centroids más cercano a x^(i).
		# esto es igual a: min_k ||x^(i) - mu_k||^2 (con 1 &lt;= k &lt;= K)
		# ver nota

	# mver los centroids
	for k = 1 to K:
		mu_k := media de los data points asignados al cluster k
}
</code></pre></div></div>

<p>Nota:</p>
<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>c^(i) := índice (de 1 a K) del centroids más cercano a x^(i).
</code></pre></div></div>
<p>Esto es lo mismo que: 
\(min_k ||x^{(i)}-\mu_k||^2\)</p>

<p>Explicación:</p>
<ul>
  <li>\(x^{(i)}\) es el i-ésimo ejemplo de entrenamiento en el dataset</li>
  <li>matemáticamente esto es calcular la distancia entre \(x^{(i)}\) y \(\mu_k\).</li>
  <li>El cálculo de la distancia entre 2 puntos es:
\(||x^{(i)} - \mu_k||\)
A esto se le llama norma \(L2\).</li>
  <li>Queremos encontrar el centroid \(k\) que minimiza esta distancia al cuadrado.</li>
</ul>

<h3 id="1-norma-l2">1. Norma \(L2\)</h3>
<p>Dado el vector 
\(\begin{align}
    x &amp;= \begin{bmatrix}
           x_{1} \\\\
					 x_{2} \\\\
           \vdots \\\\
           x_{n}
         \end{bmatrix}
\end{align}\)</p>

<p>La \(L2\) norm está dada por:</p>

\[||x||=(\sum_{i=1}^{n} {x}_i^2)^{1/2}=\sqrt{\sum_{i=1}^{n} {x}_i^2}\]

<p>Esta es la norma euclidiana que se utiliza para calcular la distancia entre 2 puntos.</p>

<p>En python podemos calcular la norma \(L2\) así:</p>
<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>numpy.linalg.norm(x)
</code></pre></div></div>

<p>Sin embargo, en machine learning se utiliza la norma \(L2\) al cuadrado: \(||x||^2\), ¿por qué?:</p>
<ul>
  <li>Porque se deshace de la raíz cuadrada haciéndola más fácil para operar.</li>
  <li>y terminamos con una simple suma de cada elemento del vector al cuadrado.</li>
  <li>Es ampliamente usada en machine learning porque puede ser calculada con la operación de vector \(x^\text{T}x\), operación que se puede hacer muy rápidamente mediante la vectorización. Así, tenemos mejor rendimiento debido a la optimización (*).</li>
  <li>El centroid con la distancia al cuadrado más pequeña es el mismo que el centroid con la distancia más pequeña sin elevar al cuadrado.</li>
</ul>

\[||x||^2=\sum_{i=1}^{n} {x}_i^2= (x_1)^2+(x_2)^2+...+(x_n)^2\]

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code># l2-norm squared
np.linalg.norm(x)**2
</code></pre></div></div>

<p>(*):</p>
<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code># x^Tx es lo mismo que la l2-norm
x^T = [1,
	2,
	3]
x = [1, 2, 3]
x^T dot product x = (1*1 + 2*2 + 3*3) = 14
</code></pre></div></div>

<h2 id="iv-función-de-costo-para-k-means">IV. Función de costo para K-means</h2>
<p>K-means también optimiza una cost function específica, aunque el algoritmo de optimización que utiliza para optimizar esa función no es gradient descent, en realidad, es el algoritmo que ya vimos anteriormente: el mismo K-means.</p>

<h3 id="1-notación">1. Notación</h3>
<ul>
  <li>\(c^{(i)}\)= índice del cluster \(1, 2, ..., K\) al que se asigna actualmente el ejemplo \(x^{(i)}\)</li>
  <li>\(\mu_k\) = centroid del cluster \(k\) (con \(1 &lt;= k &lt;= K\))</li>
  <li>\(\mu_{c^{(i)}}\)= centroid del cluster al que se le ha asignado el ejemplo \(x^{(i)}\).
    <ul>
      <li>Por ejemplo: training example 10, \(x^{(10)}\). ¿Cuál es la ubicación del centroid al que el décimo training example ha sido asignado? Buscaremos \(c^{(10)}\), entonces \(\mu_{c^{(10)}}\) es la ubicación del centroid del cluster al que se ha asignado \(x^{(10)}\).</li>
    </ul>
  </li>
</ul>

<h3 id="2-cost-function">2. Cost function</h3>
<p>K-means intenta minimizar la siguiente cost function:
\(J(c^{(1)}, ..., c^{(m)}, \mu_1, ..., \mu_K) = \frac{1}{m} \cdot \sum_{i=1}^{m}(||x^{(i)} - \mu_{c^{(i)}}||^2)\)</p>

<ul>
  <li>El nombre de esta cost function es: <strong>Distortion function</strong>.</li>
  <li>\(J\) es función de \(c^{(1)},...,c^{(m)}\) y \(\mu_1,...,\mu_K\).</li>
  <li>\(J\) es el promedio de la distancia al cuadrado entre cada training example \(x^{(i)}\) y la ubicación del centroid \(\mu_{c^{(i)}}\) al que se le asignó el training example \(x^{(i)}\). Por ejemplo, para el décimo ejemplo la distancia al cuadrado sería: \((x^{(10)} - \mu_{c^{(10)}})^2\).</li>
  <li>K-means con esto intenta encontrar asignaciones de puntos a centroids y ubicaciones de centroids que minimizan la distancia al cuadrado.</li>
</ul>

<h3 id="3-explicación-de-la-cost-function">3. Explicación de la cost function</h3>
<p>Una vez asignados los puntos a los centroids, medimos las distancias entre dichos puntos y su repectivo centroid, para después calcular el cuadrado de todas esas distancias y obtener el promedio que finalmente será la cost function \(J\).</p>

<p>En cada iteración, actualizaremos las asignaciones a clusters \(c^{(1)},...,c^{(m)}\); o actualizaremos las posiciones de los centroids \(\mu_1,...,\mu_K\) para seguir reduciendo la función de costo \(J\).</p>

<h3 id="4-por-qué-k-means-intenta-minimizar-la-función-de-costo-j">4. ¿Por qué K-means intenta minimizar la función de costo \(J\)?</h3>
<p>Veamos el algoritmo K-means:</p>
<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>#paso 0: aleatoriamente inicializar K centroids mu_1, mu_2, ..., mu_K

repeat {
	# paso 1: asignar puntos al centroid más cercano
	for i = 1 to m
		c^(i) := índice del centroid más cercano a x^(i)

	# paso 2: mover los centroids
	for k = 1 to K
		mu_k := media de los puntos del cluster k
}
</code></pre></div></div>
<p>K-means intenta minimizar la cost function \(J\) de la siguiente forma:</p>

<ul>
  <li>El <strong>paso 1</strong> busca actualizar \(c^{(1)},...,c^{(m)}\) para minimizar la función de costo \(J\) tanto como sea posible mientras mantiene \(\mu_1,...,\mu_K\) fijos.
    <ul>
      <li>Esto es porque \(c^{(i)}\) es el centroid más cercano a \(x^{(i)}\), lo que hace que la distancia sea lo más pequeña posible.</li>
    </ul>
  </li>
  <li>El <strong>paso 2</strong> busca actualizar \(\mu_1,...,\mu_K\) y dejar \(c^{(1)},...,c^{(m)}\) fijos para minimizar la cost function \(J\) tanto como sea posible.
    <ul>
      <li>Hacer que \(\mu_k\) sea la media de los puntos asignados minimiza la función de costo, ya que minimiza la distancia al cuadrado entre los puntos y el centroid. Por ejemplo:
        <ul>
          <li>distancia <em>punto1</em> a <em>centroid1</em> es \(1\)</li>
          <li>distancia <em>punto2</em> a <em>centroid1</em> es \(9\)</li>
          <li>average: \(J = (1^2 + 9^2) / 2 = 41\)</li>
          <li>Pero, si movemos el centroid donde es el promedio de los puntos: \((1+11) / 2 = 6\), entonces si calculamos \(J\):</li>
          <li>distancia <em>punto1</em> a <em>centroid1</em> es \(5\)</li>
          <li>distancia <em>punto2</em> a <em>centroid1</em> es \(5\)</li>
          <li>average: \(J = (5^2 + 5^2) / 2 = 25\) &lt;- es más pequeña \(J\)!</li>
        </ul>
      </li>
    </ul>
  </li>
</ul>

<h2 id="v-convergencia-de-k-means">V. Convergencia de K-means</h2>
<p>El hecho de que K-means optimice una cost function \(J\) significa que la convergencia está garantizada, es decir, en cada iteracion la distortion cost function debería bajar o mantenerse igual, pero <strong>NUNCA SUBIR!</strong>, si ese es el caso, hay un bug en el código, ya que en cada paso de K-means se está estableciendo el valor \(c^{(i)}\) y \(\mu_k\) para intentar reducir la cost function.</p>

<p>También, si la cost function para de bajar, esto nos da una forma para probar si K-means ha convergido. Una vez que haya una sola iteración que se mantiene igual, eso usualmente significa que K-means ha convergido y deberíamos parar el algoritmo. O en algunos casos, ejecutaremos K-means por un largo tiempo, y la cost function va bajando muy, muy lentamente, esto es parecido al gradient descent, donde quizás ejecutarlo por más tiempo puede mejorar, pero si la tasa a la que la cost function está bajando es muy, muy lenta, también podemos decir que es suficiente y está muy cerca de la convergencia.</p>

<h2 id="vi-inicializando-clusters-de-k-means">VI. Inicializando clusters de K-means</h2>
<p>El uso de múltiples inicializaciones aleatorias diferentes de los centroids dará como resultado la búsqueda de un mejor conjunto de clusters.</p>

<p>El primer paso es elegir ubicaciones aleatorias como suposiciones iniciales para los centroids \(\mu_1,...,\mu_K\).</p>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>#paso 0: aleatoriamente inicializar K centroids mu_1, mu_2, ..., mu_K

repeat {
	# paso 1: asignar puntos al centroid más cercano
	# paso 2: mover los centroids
}
</code></pre></div></div>
<p>¿Cómo implementamos el <strong>paso 0</strong>?</p>

<h3 id="1-a-tener-en-cuenta">1. A tener en cuenta</h3>
<p>Siempre elegir un valor de \(K\) menor a \(m\):</p>

\[K &lt; m\]

<p>\(K\) es el número de clusters y \(m\) es el número de training examples.</p>

<p>No tiene sentido tener más centroids que ejemplos \(m\) porque no habrán suficientes training examples para que cada cluster tenga al menos un ejemplo dentro.</p>

<h3 id="2-inicialización-de-clusters">2. Inicialización de clusters</h3>
<p>La forma de inicializarlos más común es aleatoriamente escoger \(K\) datos de nuestro dataset de entrenamiento y establecer \(\mu_1, ..., \mu_K\) con un valor igual a estos \(K\) datos. Por ejemplo, si \(K=2\), entonces elegimos 2 training examples y ubicamos los 2 centroids, \(\mu_1\) y \(\mu_2\) en el mismo lugar que éstos.</p>

<h3 id="3-a-considerar-en-la-inicialización-de-clusters">3. A considerar en la inicialización de clusters</h3>
<p>La forma anterior de inicializar los cluster centroids tiene algunos puntos a considerar:</p>
<ul>
  <li>
    <p>Con este método hay una posibilidad de terminar con 2 (o más) clusters inicializados muy cerca. Y dependiendo de cómo elijamos las posiciones iniciales aleatorias de los centroids, K-means terminará escogiendo diferentes set de clusters para el dataset que tenemos.</p>
  </li>
  <li>
    <p>Si queremos encontrar 3 clusters y justo tenemos 3 nubes de puntos separadas, el resultado óptimo sería un cluster que agupe a cada nube de puntos. Sin embargo, podemos tener una inicialización diferente que tenga 2 centroids a muy poca distancia (ambos están en la misma nube de puntos), por lo que estos 2 clusters van a tener muy pocos puntos mientras que el tercero tendrá muchos puntos dispersos (las otras 2 nubes de puntos), generando así un resultado subóptimo o mínimo local. Otra situación que puede suceder al inicializar aleatoriamente los centroids es, que un cluster se quede con muchos puntos, otro se quede con unos pocos y finalmente el último se quede sin puntos, generando que K-means se quede estancado en un resultado subóptimo.</p>
  </li>
</ul>

<h3 id="4-solución-al-resultado-subóptimo-de-k-means">4. Solución al resultado subóptimo de k-means</h3>
<p>Si le damos muchas oportunidades o ejecuciones al algoritmo K-means para encontrar el mejor óptimo local se podrá solucionar esto. Así, podemos intentar múltiples inicializaciones aleatorias para tener una mejor chance de encontrar mejores clusters.</p>

<h3 id="5-y-si-hay-un-cluster-que-no-tenga-data-points-asignados">5. ¿Y si hay un cluster que no tenga data points asignados?</h3>
<p>En el <strong>paso 2</strong>, \(\mu_k\) intentaría calcular el promedio de \(0\) training examples, lo cual no está definido.</p>

<p>Hay 2 soluciones:</p>
<ul>
  <li>Solución 1: La más común: eliminar ese cluster, por lo que terminaremos con \(K-1\) clusters</li>
  <li>Solución 2: Si realmente necesitamos los \(K\) clusters, entonces aleatoriamente reinicializar ese cluster y esperar a que se le asignen data points la próxima vez.</li>
</ul>

<h2 id="vii-ejecución-de-k-means-múltiples-veces">VII. Ejecución de K-means múltiples veces</h2>
<p>Si ejecutamos varias veces a K-means, debemos elegir uno de entre varios clustering. Una forma de elegir cuál clustering es el mejor, o sea, qué tirada nos dió mejores resultados, es calcular la cost function \(J\) para <strong>todas</strong> las soluciones, para todas las alternativas de clustering encontradas en las ejecuciones de K-means, y elegir una de éstas de acuerdo a cuál da el menor valor para la cost function \(J\).</p>

<p>Nos damos cuenta que en la mejor solución de clustering, los data points en cada cluster tienen una distancia al cuadrado relativamente pequeña con respecto a su centroid, por lo tanto, la cost function \(J\) será relativamente pequeña para esta solución. En cambio, en las demás soluciones en algunos clusters, habrá una distancia al cuadrado más grande entre los puntos y el centroid correspondiente, por lo cual la función de costo será más grande.</p>

<h3 id="1-formulación-de-k-means-inicializado-aleatoriamente-múltiples-veces">1. Formulación de K-means inicializado aleatoriamente múltiples veces</h3>
<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>for i = 1 to 100 { #100 inicializaciones aleatorias, o sea, ejecutamos 100 veces K-means
	Aleatoriamente inicializar los K centroids
		# Elegir K ejemplos de entrenamiento aleatorios e inicializar los K centroids en esas ubicaciones.

	Ejecutar K-means hasta la convergencia:
        1. Obtener c^(1), ..., c^(m), mu_1, mu_2, ..., mu_K

	    2. Calcular cost function (distortion) J(c^(1), ..., c^(m), mu_1, mu_2, ..., mu_K)
}
</code></pre></div></div>

<blockquote>
  <p>Elegir el set de clusters que dan la cost function \(J\) más baja.</p>
</blockquote>

<p>Después de ejecutar este bucle 100 veces (el número es arbitrario), escogeremos el set de clusters que nos dan el costo \(J\) más bajo. Si hacemos esto, a menudo obtendremos un mejor set de clusters que si ejecutaramos K-means una sola vez.</p>

<h4 id="sobre-el-número-de-veces-que-tenemos-que-ejecutar-k-means">Sobre el número de veces que tenemos que ejecutar K-means</h4>
<p>Se recomienda un número entre 50 a 1000 (en el ejemplo pusimos 100). Si ejecutamos K-means más de 1000 veces tiende a ser computacionalmente caro y obtendremos muy pocas mejoras si lo ejecutamos más de esas veces.</p>

<h2 id="viii-cómo-elegir-el-número-de-clusters-k">VIII. Cómo elegir el número de clusters K</h2>
<p>Para muchos problemas de clustering, el valor correcto de \(K\) es ambigüo. Hay muchas aplicaciones donde los datos no dan un indicador claro de cuántos clusters hay en ellos.</p>

<h3 id="1-método-1-elbow-method">1. Método 1: Elbow method</h3>
<p>Técnica para encontrar automáticamente el número de clusters para usar en una aplicación.</p>

<p>Ejecutamos K-means con un variedad de valores de \(K\) y graficamos la función de costo o la <strong>distortion function</strong> \(J\) (eje \(y\)) como una función del número de clusters (eje \(x\)).</p>

<p>Lo que encontramos es que cuando tienes muy pocos clusters, la distorion function o la cost function \(J\) será alta y a medida de que aumentes el número de clusters, bajará. Veremos que la cost function va disminuyendo rápidamente hasta que lleguemos a un punto determinado, luego de ese punto, la función de costo disminuirá más lentamente (la curva se va aplanando). Básicamente se elige \(K\) en el punto donde la curva se dobla (se forma como un codo), o sea, donde termina el descenso rápido y empieza a descender más lentamente.</p>

<h4 id="el-problema-del-elbow-method">El problema del elbow method</h4>
<p>El problema es que en muchas aplicaciones la cost function tiene un descenso suavizado, por lo que paulatinamente va descendiendo y no se forma un codo notorio para elgir un valor de \(K\).</p>

<h3 id="2-método-2-elegir-el-k-que-más-hace-disminuir-j">2. Método 2: elegir el K que más hace disminuir J</h3>
<p>Elegir \(K\) que minimiza la función de costo \(J\) no sirve porque implicaría elegir el valor de \(K\) más grande posible, ya que tener más clusters siempre reducirá la cost function.</p>

<h3 id="3-método-3-recomendado-decidir-qué-tiene-sentido-para-la-aplicación">3. Método 3 (recomendado): decidir qué tiene sentido para la aplicación</h3>
<p>A menudo, ejecutamos K-means para obtener clusters y usarlos para algún propósito posterior. Lo que es recomendable, es evaluar K-means en función de qué tan bien se desempeñe para ese propósito posterior. Por ejemplo, tenemos una nube de puntos dispersos y los ejes son: <em>altura</em> y <em>peso</em>. Dado estas medidas de personas, queremos agruparlas en clusters para determinar cómo medir las poleras <em>S (small)</em>, <em>M (medium)</em> y <em>L (large)</em>. Elegimos \(K=3\). Sin embargo, podemos ejecutar K-means con 5 clusters \((K=5)\) y así podemos medir las poleras de acuerdo a <em>XS (extra small), S, M, L y XL (extra large)</em>. Entonces, Ambas opciones son completamente válidas, por lo que la elección dependerá de lo que tenga sentido para el negocio de las poleras.</p>

<p>Hay un trade-off entre qué tan bien las poleras entran o se ajustan a las personas, dependiendo si tenemos 3 tallas o 5 tallas, pero habrá un costo extra asociado a la manufactura y al envío de las poleras. ¿Qué hacemos? Ejecutar K-means con \(K=3\) y \(K=5\) y con estas 2 soluciones elegir, basandonos en el trade-off entre fabricar las poleras con más tamaños implicando un mejor ajuste, y el costo extra de hacer más poleras.</p>

<h2 id="ix-por-qué-normalizar-las-features-en-k-means">IX. ¿Por qué normalizar las features en K-means?</h2>
<p>Es siempre mejor normalizar las features cuando usamos K-means porque K-means es basado en distancia, lo que significa que es sensible a la diferencia de las escalas de las features.</p>

<p>Por ejemplo, tenemos 2 features:</p>
<ol>
  <li>Peso (lbs)</li>
  <li>Altura (feet)</li>
</ol>

<p>Utilizamos éstas para predecir si una persona necesita una polera <em>S</em> o una <em>L</em>.</p>

<p>En nuestro training set tenemos 2 personas ya en los clusters:</p>
<ol>
  <li>Adam (175 lbs, 5.9 ft) en <em>L</em>.</li>
  <li>Lucy (115 lbs, 5.2 ft) en <em>S</em>.</li>
</ol>

<p>Tenemos una nueva persona: Alan (140 lbs, 6.1 ft).</p>

<p>Nuestro algoritmo de clustering lo pondrá en el cluster más cercano.</p>
<ul>
  <li>
    <p>Cálculo distancia en cluster 1: \((175-140)^2 + (5.9-6.1)^2 = \sqrt{(1225 + 0.04)} = 35\)</p>
  </li>
  <li>
    <p>Cálculo distancia en cluster 2: \((115-140)^2 + (5.2-6.1)^2 = \sqrt{(625 + 0.81)} = 25\)</p>
  </li>
</ul>

<p>Notar el gran impacto de la feature 1 (<em>Peso</em>) en el cálculo de la distancia. Además, notar el pequeñísimo impacto de la feature 2 (<em>altura</em>) en el cálculo de la distancia.</p>

<p>Esto impactará el rendimiento de todos los modelos basados en distancia, ya que otorgará mayor peso a las variables que tienen una mayor magnitud (<em>Peso</em> en este caso). Entonces, el algoritmo está sesgado hacia una variable con mayor magnitud. Para superar este problema, podemos reducir todas las variables a la misma escala. Si no escalamos las features, la variable <em>altura</em> no tiene mucho efecto y a Alan se le asignará en el grupo <em>S</em>, lo que no tiene sentido, ya que es muy alto.</p>

<h3 id="en-resumen">En resumen</h3>
<p>El problema está en que en K-means, si las features están a escalas muy diferentes, por ejemplo, edad vs sueldo, entonces va a haber una feature que va a tener demasiado impacto sobre el cálculo de la distancia, mientras que la otra va a tener poco o casi nulo.</p>

<h2 id="x-algunas-qa">X. Algunas Q&amp;A</h2>
<ol>
  <li>¿Por qué con K-means resultan clusters tan diferentes?
    <ul>
      <li>Tiene que ver con la data. Si nuestros clusters tienen un límite bien definido entre ellos siempre encontraremos los mismos centroids y solo dependerá del parámetro \(K\). Si nuestra data no tiene un límite claro entre cada cluster (datos más dispersos) entonces inicializar los centroids aleatoriamente darán diferentes resultados dependiendo de la inicialización.</li>
    </ul>
    <ul>
      <li>Una sugerencia es usar el parámetro <code class="language-plaintext highlighter-rouge">random_state</code> cuando usemos la implementación de K-means de la librería <code class="language-plaintext highlighter-rouge">scikit-learn</code>, ya que si lo establecemos con un valor podemos garantizar que tendremos los mismos centroids cada vez que usemos el modelo (que lo entrenemos).
        <div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code> from sklearn.cluster import KMeans
 kmeans = KMeans(n_clusters=3, random_state=42)
</code></pre></div>        </div>
      </li>
    </ul>
  </li>
  <li>¿Por qué hay clusters sin puntos asignados?
    <ul>
      <li>A medida que aumenta \(K\) (más centroids) existe más probabilidad de que un centroid no sea el más cercano de ningún punto, o sea, no se le asigne ningún data point. También existe el caso que si hay 2 centroids muy cerca y le asignamos correctamente de forma aleatoria sus ubicaciones, pero tenemos datos con ejemplos repetidos; al momento de asignar los centroids más cercanos a cada uno de los puntos, puede pasar que uno le “robe” el dato a otro, quedandose con dos datos asignados y el otro sin nada, esto pasa en el caso de aplicar K-means para comprimir imágenes, en donde dos pixeles pueden tener exactamente el mismo valor.</li>
    </ul>
  </li>
</ol>]]></content><author><name></name></author><category term="algoritmos" /><summary type="html"><![CDATA[Definición, análisis, implementación y consejos del algoritmo K-means]]></summary></entry><entry><title type="html">Compresión de una imagen utilizando k-means</title><link href="https://squirogar.github.io/blog/2023/compresor-imagenes-kmeans/" rel="alternate" type="text/html" title="Compresión de una imagen utilizando k-means" /><published>2023-06-09T21:41:00+00:00</published><updated>2023-06-09T21:41:00+00:00</updated><id>https://squirogar.github.io/blog/2023/compresor-imagenes-kmeans</id><content type="html" xml:base="https://squirogar.github.io/blog/2023/compresor-imagenes-kmeans/"><![CDATA[<p>Para representar una imagen RGB podemos usar hasta: 255 (Canal R) * 255 (Canal G) * 255 (Canal B) = 16.581.375 colores (combinaciones posibles). Además, se requieren 24 bits (8 bits por cada canal) para cada uno de los pixeles de la imagen (128x128 pixeles) dando así un tamaño total de imagen de 128 x 128 x 24 = 393.216 bits.</p>

<p>Sin embargo, para una imagen cualquiera no se utiliza todo este espectro de colores. No obstante, el número de colores a utilizar es aún grande. Entonces, si reducimos este número de colores a algo como 16, o sea, elegimos 16 colores para representar toda la imagen.</p>

<p>Esto sería muy bueno porque solo necesitaríamos recordar los 16 colores, y en cada pixel, ya no se requerirá tener 3 números entre 0-255 para representar el color, sino que solamente el índice en el vector de 16 colores que corresponde al color que pintamos en el pixel.</p>

<p>Esta nueva representación requiere un leve overhead, ya que guardamos el vector de 16 colores. Cada uno de estos colores en el vector requiere 24 bits. Sin embargo, para la imagen en sí misma sólo se requieren 4 bits por ubicación de pixel (los 4 bits son para representar un número entre 0-15).</p>

<p>El tamaño total de la imagen es: (16x24) + (128x128x4) = 65.920 bits</p>]]></content><author><name></name></author><category term="algoritmos" /><summary type="html"><![CDATA[Definición, análisis, implementación y consejos del algoritmo K-means]]></summary></entry><entry><title type="html">a post with custom blockquotes</title><link href="https://squirogar.github.io/blog/2023/custom-blockquotes/" rel="alternate" type="text/html" title="a post with custom blockquotes" /><published>2023-05-12T19:53:00+00:00</published><updated>2023-05-12T19:53:00+00:00</updated><id>https://squirogar.github.io/blog/2023/custom-blockquotes</id><content type="html" xml:base="https://squirogar.github.io/blog/2023/custom-blockquotes/"><![CDATA[<p>This post shows how to add custom styles for blockquotes. Based on <a href="https://github.com/sighingnow/jekyll-gitbook">jekyll-gitbook</a> implementation.</p>

<p>We decided to support the same custom blockquotes as in <a href="https://sighingnow.github.io/jekyll-gitbook/jekyll/2022-06-30-tips_warnings_dangers.html">jekyll-gitbook</a>, which are also found in a lot of other sites’ styles. The styles definitions can be found on the <a href="https://github.com/alshedivat/al-folio/blob/master/_sass/_base.scss">_base.scss</a> file, more specifically:</p>

<div class="language-scss highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="cm">/* Tips, warnings, and dangers */</span>
<span class="nc">.post</span> <span class="nc">.post-content</span> <span class="nt">blockquote</span> <span class="p">{</span>
    <span class="k">&amp;</span><span class="nc">.block-tip</span> <span class="p">{</span>
    <span class="nl">border-color</span><span class="p">:</span> <span class="nf">var</span><span class="p">(</span><span class="o">--</span><span class="n">global-tip-block</span><span class="p">);</span>
    <span class="nl">background-color</span><span class="p">:</span> <span class="nf">var</span><span class="p">(</span><span class="o">--</span><span class="n">global-tip-block-bg</span><span class="p">);</span>

    <span class="nt">p</span> <span class="p">{</span>
      <span class="nl">color</span><span class="p">:</span> <span class="nf">var</span><span class="p">(</span><span class="o">--</span><span class="n">global-tip-block-text</span><span class="p">);</span>
    <span class="p">}</span>

    <span class="nt">h1</span><span class="o">,</span> <span class="nt">h2</span><span class="o">,</span> <span class="nt">h3</span><span class="o">,</span> <span class="nt">h4</span><span class="o">,</span> <span class="nt">h5</span><span class="o">,</span> <span class="nt">h6</span> <span class="p">{</span>
      <span class="nl">color</span><span class="p">:</span> <span class="nf">var</span><span class="p">(</span><span class="o">--</span><span class="n">global-tip-block-title</span><span class="p">);</span>
    <span class="p">}</span>
  <span class="p">}</span>

  <span class="k">&amp;</span><span class="nc">.block-warning</span> <span class="p">{</span>
    <span class="nl">border-color</span><span class="p">:</span> <span class="nf">var</span><span class="p">(</span><span class="o">--</span><span class="n">global-warning-block</span><span class="p">);</span>
    <span class="nl">background-color</span><span class="p">:</span> <span class="nf">var</span><span class="p">(</span><span class="o">--</span><span class="n">global-warning-block-bg</span><span class="p">);</span>

    <span class="nt">p</span> <span class="p">{</span>
      <span class="nl">color</span><span class="p">:</span> <span class="nf">var</span><span class="p">(</span><span class="o">--</span><span class="n">global-warning-block-text</span><span class="p">);</span>
    <span class="p">}</span>

    <span class="nt">h1</span><span class="o">,</span> <span class="nt">h2</span><span class="o">,</span> <span class="nt">h3</span><span class="o">,</span> <span class="nt">h4</span><span class="o">,</span> <span class="nt">h5</span><span class="o">,</span> <span class="nt">h6</span> <span class="p">{</span>
      <span class="nl">color</span><span class="p">:</span> <span class="nf">var</span><span class="p">(</span><span class="o">--</span><span class="n">global-warning-block-title</span><span class="p">);</span>
    <span class="p">}</span>
  <span class="p">}</span>

  <span class="k">&amp;</span><span class="nc">.block-danger</span> <span class="p">{</span>
    <span class="nl">border-color</span><span class="p">:</span> <span class="nf">var</span><span class="p">(</span><span class="o">--</span><span class="n">global-danger-block</span><span class="p">);</span>
    <span class="nl">background-color</span><span class="p">:</span> <span class="nf">var</span><span class="p">(</span><span class="o">--</span><span class="n">global-danger-block-bg</span><span class="p">);</span>

    <span class="nt">p</span> <span class="p">{</span>
      <span class="nl">color</span><span class="p">:</span> <span class="nf">var</span><span class="p">(</span><span class="o">--</span><span class="n">global-danger-block-text</span><span class="p">);</span>
    <span class="p">}</span>

    <span class="nt">h1</span><span class="o">,</span> <span class="nt">h2</span><span class="o">,</span> <span class="nt">h3</span><span class="o">,</span> <span class="nt">h4</span><span class="o">,</span> <span class="nt">h5</span><span class="o">,</span> <span class="nt">h6</span> <span class="p">{</span>
      <span class="nl">color</span><span class="p">:</span> <span class="nf">var</span><span class="p">(</span><span class="o">--</span><span class="n">global-danger-block-title</span><span class="p">);</span>
    <span class="p">}</span>
  <span class="p">}</span>
<span class="p">}</span>
</code></pre></div></div>

<p>A regular blockquote can be used as following:</p>

<div class="language-markdown highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="gt">&gt; This is a regular blockquote</span>
<span class="gt">&gt; and it can be used as usual</span>
</code></pre></div></div>

<blockquote>
  <p>This is a regular blockquote
and it can be used as usual</p>
</blockquote>

<p>These custom styles can be used by adding the specific class to the blockquote, as follows:</p>

<div class="language-markdown highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="gt">&gt; ##### TIP</span>
<span class="gt">&gt;</span>
<span class="gt">&gt; A tip can be used when you want to give advice</span>
<span class="gt">&gt; related to a certain content.</span>
{: .block-tip }
</code></pre></div></div>

<blockquote class="block-tip">
  <h5 id="tip">TIP</h5>

  <p>A tip can be used when you want to give advice
related to a certain content.</p>
</blockquote>

<div class="language-markdown highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="gt">&gt; ##### WARNING</span>
<span class="gt">&gt;</span>
<span class="gt">&gt; This is a warning, and thus should</span>
<span class="gt">&gt; be used when you want to warn the user</span>
{: .block-warning }
</code></pre></div></div>

<blockquote class="block-warning">
  <h5 id="warning">WARNING</h5>

  <p>This is a warning, and thus should
be used when you want to warn the user</p>
</blockquote>

<div class="language-markdown highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="gt">&gt; ##### DANGER</span>
<span class="gt">&gt;</span>
<span class="gt">&gt; This is a danger zone, and thus should</span>
<span class="gt">&gt; be used carefully</span>
{: .block-danger }
</code></pre></div></div>

<blockquote class="block-danger">
  <h5 id="danger">DANGER</h5>

  <p>This is a danger zone, and thus should
be used carefully</p>
</blockquote>]]></content><author><name></name></author><category term="sample-posts" /><category term="blockquotes" /><summary type="html"><![CDATA[an example of a blog post with custom blockquotes]]></summary></entry><entry><title type="html">a post with table of contents on a sidebar</title><link href="https://squirogar.github.io/blog/2023/sidebar-table-of-contents/" rel="alternate" type="text/html" title="a post with table of contents on a sidebar" /><published>2023-04-25T14:14:00+00:00</published><updated>2023-04-25T14:14:00+00:00</updated><id>https://squirogar.github.io/blog/2023/sidebar-table-of-contents</id><content type="html" xml:base="https://squirogar.github.io/blog/2023/sidebar-table-of-contents/"><![CDATA[<p>This post shows how to add a table of contents as a sidebar.</p>

<h2 id="adding-a-table-of-contents">Adding a Table of Contents</h2>

<p>To add a table of contents to a post as a sidebar, simply add</p>
<div class="language-yml highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="na">toc</span><span class="pi">:</span>
  <span class="na">sidebar</span><span class="pi">:</span> <span class="s">left</span>
</code></pre></div></div>
<p>to the front matter of the post. The table of contents will be automatically generated from the headings in the post. If you wish to display the sidebar to the right, simply change <code class="language-plaintext highlighter-rouge">left</code> to <code class="language-plaintext highlighter-rouge">right</code>.</p>

<h3 id="example-of-sub-heading-1">Example of Sub-Heading 1</h3>

<p>Jean shorts raw denim Vice normcore, art party High Life PBR skateboard stumptown vinyl kitsch. Four loko meh 8-bit, tousled banh mi tilde forage Schlitz dreamcatcher twee 3 wolf moon. Chambray asymmetrical paleo salvia, sartorial umami four loko master cleanse drinking vinegar brunch. <a href="https://www.pinterest.com">Pinterest</a> DIY authentic Schlitz, hoodie Intelligentsia butcher trust fund brunch shabby chic Kickstarter forage flexitarian. Direct trade <a href="https://en.wikipedia.org/wiki/Cold-pressed_juice">cold-pressed</a> meggings stumptown plaid, pop-up taxidermy. Hoodie XOXO fingerstache scenester Echo Park. Plaid ugh Wes Anderson, freegan pug selvage fanny pack leggings pickled food truck DIY irony Banksy.</p>

<h3 id="example-of-another-sub-heading-1">Example of another Sub-Heading 1</h3>

<p>Jean shorts raw denim Vice normcore, art party High Life PBR skateboard stumptown vinyl kitsch. Four loko meh 8-bit, tousled banh mi tilde forage Schlitz dreamcatcher twee 3 wolf moon. Chambray asymmetrical paleo salvia, sartorial umami four loko master cleanse drinking vinegar brunch. <a href="https://www.pinterest.com">Pinterest</a> DIY authentic Schlitz, hoodie Intelligentsia butcher trust fund brunch shabby chic Kickstarter forage flexitarian. Direct trade <a href="https://en.wikipedia.org/wiki/Cold-pressed_juice">cold-pressed</a> meggings stumptown plaid, pop-up taxidermy. Hoodie XOXO fingerstache scenester Echo Park. Plaid ugh Wes Anderson, freegan pug selvage fanny pack leggings pickled food truck DIY irony Banksy.</p>

<h2 data-toc-text="Customizing" id="customizing-your-table-of-contents">Customizing Your Table of Contents</h2>

<p>If you want to learn more about how to customize the table of contents of your sidebar, you can check the <a href="https://afeld.github.io/bootstrap-toc/">bootstrap-toc</a> documentation. Notice that you can even customize the text of the heading that will be displayed on the sidebar.</p>

<h3 id="example-of-sub-heading-2">Example of Sub-Heading 2</h3>

<p>Jean shorts raw denim Vice normcore, art party High Life PBR skateboard stumptown vinyl kitsch. Four loko meh 8-bit, tousled banh mi tilde forage Schlitz dreamcatcher twee 3 wolf moon. Chambray asymmetrical paleo salvia, sartorial umami four loko master cleanse drinking vinegar brunch. <a href="https://www.pinterest.com">Pinterest</a> DIY authentic Schlitz, hoodie Intelligentsia butcher trust fund brunch shabby chic Kickstarter forage flexitarian. Direct trade <a href="https://en.wikipedia.org/wiki/Cold-pressed_juice">cold-pressed</a> meggings stumptown plaid, pop-up taxidermy. Hoodie XOXO fingerstache scenester Echo Park. Plaid ugh Wes Anderson, freegan pug selvage fanny pack leggings pickled food truck DIY irony Banksy.</p>

<h3 id="example-of-another-sub-heading-2">Example of another Sub-Heading 2</h3>

<p>Jean shorts raw denim Vice normcore, art party High Life PBR skateboard stumptown vinyl kitsch. Four loko meh 8-bit, tousled banh mi tilde forage Schlitz dreamcatcher twee 3 wolf moon. Chambray asymmetrical paleo salvia, sartorial umami four loko master cleanse drinking vinegar brunch. <a href="https://www.pinterest.com">Pinterest</a> DIY authentic Schlitz, hoodie Intelligentsia butcher trust fund brunch shabby chic Kickstarter forage flexitarian. Direct trade <a href="https://en.wikipedia.org/wiki/Cold-pressed_juice">cold-pressed</a> meggings stumptown plaid, pop-up taxidermy. Hoodie XOXO fingerstache scenester Echo Park. Plaid ugh Wes Anderson, freegan pug selvage fanny pack leggings pickled food truck DIY irony Banksy.</p>]]></content><author><name></name></author><category term="sample-posts" /><category term="toc" /><category term="sidebar" /><summary type="html"><![CDATA[an example of a blog post with table of contents on a sidebar]]></summary></entry><entry><title type="html">a post with audios</title><link href="https://squirogar.github.io/blog/2023/audios/" rel="alternate" type="text/html" title="a post with audios" /><published>2023-04-25T10:25:00+00:00</published><updated>2023-04-25T10:25:00+00:00</updated><id>https://squirogar.github.io/blog/2023/audios</id><content type="html" xml:base="https://squirogar.github.io/blog/2023/audios/"><![CDATA[<p>This is an example post with audios. It supports local audio files.</p>

<div class="row mt-3">
    <div class="col-sm mt-3 mt-md-0">
        <figure>

  <audio src="/assets/audio/epicaly-short-113909.mp3" controls="" />

</figure>

    </div>
    <div class="col-sm mt-3 mt-md-0">
        <figure>

  <audio src="https://cdn.pixabay.com/download/audio/2022/06/25/audio_69a61cd6d6.mp3" controls="" />

</figure>

    </div>
</div>
<div class="caption">
    A simple, elegant caption looks good between video rows, after each row, or doesn't have to be there at all.
</div>]]></content><author><name></name></author><category term="sample-posts" /><category term="including" /><category term="audios" /><summary type="html"><![CDATA[this is what included audios could look like]]></summary></entry><entry><title type="html">a post with videos</title><link href="https://squirogar.github.io/blog/2023/videos/" rel="alternate" type="text/html" title="a post with videos" /><published>2023-04-24T21:01:00+00:00</published><updated>2023-04-24T21:01:00+00:00</updated><id>https://squirogar.github.io/blog/2023/videos</id><content type="html" xml:base="https://squirogar.github.io/blog/2023/videos/"><![CDATA[<p>This is an example post with videos. It supports local video files.</p>

<div class="row mt-3">
    <div class="col-sm mt-3 mt-md-0">
        

<figure>

  

  <video src="/assets/video/pexels-engin-akyurt-6069112-960x540-30fps.mp4" class="img-fluid rounded z-depth-1" width="auto" height="auto" autoplay="" controls="" />

  

</figure>

    </div>
    <div class="col-sm mt-3 mt-md-0">
        

<figure>

  

  <video src="/assets/video/pexels-engin-akyurt-6069112-960x540-30fps.mp4" class="img-fluid rounded z-depth-1" width="auto" height="auto" controls="" />

  

</figure>

    </div>
</div>
<div class="caption">
    A simple, elegant caption looks good between video rows, after each row, or doesn't have to be there at all.
</div>

<p>It does also support embedding videos from different sources. Here are some examples:</p>

<div class="row mt-3">
    <div class="col-sm mt-3 mt-md-0">
        

<figure>

  

  <iframe src="https://www.youtube.com/embed/jNQXAC9IVRw" class="img-fluid rounded z-depth-1" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share" allowfullscreen="" width="auto" height="auto" />

  

</figure>

    </div>
    <div class="col-sm mt-3 mt-md-0">
        

<figure>

  

  <iframe src="https://player.vimeo.com/video/524933864?h=1ac4fd9fb4&amp;title=0&amp;byline=0&amp;portrait=0" class="img-fluid rounded z-depth-1" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share" allowfullscreen="" width="auto" height="auto" />

  

</figure>

    </div>
</div>]]></content><author><name></name></author><category term="sample-posts" /><category term="including" /><category term="videos" /><summary type="html"><![CDATA[this is what included videos could look like]]></summary></entry><entry><title type="html">displaying beautiful tables with Bootstrap Tables</title><link href="https://squirogar.github.io/blog/2023/tables/" rel="alternate" type="text/html" title="displaying beautiful tables with Bootstrap Tables" /><published>2023-03-20T18:37:00+00:00</published><updated>2023-03-20T18:37:00+00:00</updated><id>https://squirogar.github.io/blog/2023/tables</id><content type="html" xml:base="https://squirogar.github.io/blog/2023/tables/"><![CDATA[<p>Using markdown to display tables is easy. Just use the following syntax:</p>

<div class="language-markdown highlighter-rouge"><div class="highlight"><pre class="highlight"><code>| Left aligned | Center aligned | Right aligned |
| :----------- | :------------: | ------------: |
| Left 1       | center 1       | right 1       |
| Left 2       | center 2       | right 2       |
| Left 3       | center 3       | right 3       |
</code></pre></div></div>

<p>That will generate:</p>

<table>
  <thead>
    <tr>
      <th style="text-align: left">Left aligned</th>
      <th style="text-align: center">Center aligned</th>
      <th style="text-align: right">Right aligned</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td style="text-align: left">Left 1</td>
      <td style="text-align: center">center 1</td>
      <td style="text-align: right">right 1</td>
    </tr>
    <tr>
      <td style="text-align: left">Left 2</td>
      <td style="text-align: center">center 2</td>
      <td style="text-align: right">right 2</td>
    </tr>
    <tr>
      <td style="text-align: left">Left 3</td>
      <td style="text-align: center">center 3</td>
      <td style="text-align: right">right 3</td>
    </tr>
  </tbody>
</table>

<p></p>

<p>It is also possible to use HTML to display tables. For example, the following HTML code will display a table with <a href="https://bootstrap-table.com/">Bootstrap Table</a>, loaded from a JSON file:</p>

<div class="language-html highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="nt">&lt;table</span>
  <span class="na">id=</span><span class="s">"table"</span>
  <span class="na">data-toggle=</span><span class="s">"table"</span>
  <span class="na">data-url=</span><span class="s">"{{ '/assets/json/table_data.json' | relative_url }}"</span><span class="nt">&gt;</span>
  <span class="nt">&lt;thead&gt;</span>
    <span class="nt">&lt;tr&gt;</span>
      <span class="nt">&lt;th</span> <span class="na">data-field=</span><span class="s">"id"</span><span class="nt">&gt;</span>ID<span class="nt">&lt;/th&gt;</span>
      <span class="nt">&lt;th</span> <span class="na">data-field=</span><span class="s">"name"</span><span class="nt">&gt;</span>Item Name<span class="nt">&lt;/th&gt;</span>
      <span class="nt">&lt;th</span> <span class="na">data-field=</span><span class="s">"price"</span><span class="nt">&gt;</span>Item Price<span class="nt">&lt;/th&gt;</span>
    <span class="nt">&lt;/tr&gt;</span>
  <span class="nt">&lt;/thead&gt;</span>
<span class="nt">&lt;/table&gt;</span>
</code></pre></div></div>

<table data-toggle="table" data-url="/assets/json/table_data.json">
  <thead>
    <tr>
      <th data-field="id">ID</th>
      <th data-field="name">Item Name</th>
      <th data-field="price">Item Price</th>
    </tr>
  </thead>
</table>

<p></p>

<p>By using <a href="https://bootstrap-table.com/">Bootstrap Table</a> it is possible to create pretty complex tables, with pagination, search, and more. For example, the following HTML code will display a table, loaded from a JSON file, with pagination, search, checkboxes, and header/content alignment. For more information, check the <a href="https://examples.bootstrap-table.com/index.html">documentation</a>.</p>

<div class="language-html highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="nt">&lt;table</span>
  <span class="na">data-click-to-select=</span><span class="s">"true"</span>
  <span class="na">data-height=</span><span class="s">"460"</span>
  <span class="na">data-pagination=</span><span class="s">"true"</span>
  <span class="na">data-search=</span><span class="s">"true"</span>
  <span class="na">data-toggle=</span><span class="s">"table"</span>
  <span class="na">data-url=</span><span class="s">"{{ '/assets/json/table_data.json' | relative_url }}"</span><span class="nt">&gt;</span>
  <span class="nt">&lt;thead&gt;</span>
    <span class="nt">&lt;tr&gt;</span>
      <span class="nt">&lt;th</span> <span class="na">data-checkbox=</span><span class="s">"true"</span><span class="nt">&gt;&lt;/th&gt;</span>
      <span class="nt">&lt;th</span> <span class="na">data-field=</span><span class="s">"id"</span> <span class="na">data-halign=</span><span class="s">"left"</span> <span class="na">data-align=</span><span class="s">"center"</span> <span class="na">data-sortable=</span><span class="s">"true"</span><span class="nt">&gt;</span>ID<span class="nt">&lt;/th&gt;</span>
      <span class="nt">&lt;th</span> <span class="na">data-field=</span><span class="s">"name"</span> <span class="na">data-halign=</span><span class="s">"center"</span> <span class="na">data-align=</span><span class="s">"right"</span> <span class="na">data-sortable=</span><span class="s">"true"</span><span class="nt">&gt;</span>Item Name<span class="nt">&lt;/th&gt;</span>
      <span class="nt">&lt;th</span> <span class="na">data-field=</span><span class="s">"price"</span> <span class="na">data-halign=</span><span class="s">"right"</span> <span class="na">data-align=</span><span class="s">"left"</span> <span class="na">data-sortable=</span><span class="s">"true"</span><span class="nt">&gt;</span>Item Price<span class="nt">&lt;/th&gt;</span>
    <span class="nt">&lt;/tr&gt;</span>
  <span class="nt">&lt;/thead&gt;</span>
<span class="nt">&lt;/table&gt;</span>
</code></pre></div></div>

<table data-click-to-select="true" data-height="460" data-pagination="true" data-search="true" data-toggle="table" data-url="/assets/json/table_data.json">
  <thead>
    <tr>
      <th data-checkbox="true"></th>
      <th data-field="id" data-halign="left" data-align="center" data-sortable="true">ID</th>
      <th data-field="name" data-halign="center" data-align="right" data-sortable="true">Item Name</th>
      <th data-field="price" data-halign="right" data-align="left" data-sortable="true">Item Price</th>
    </tr>
  </thead>
</table>]]></content><author><name></name></author><category term="sample-posts" /><summary type="html"><![CDATA[an example of how to use Bootstrap Tables]]></summary></entry><entry><title type="html">a post with table of contents</title><link href="https://squirogar.github.io/blog/2023/table-of-contents/" rel="alternate" type="text/html" title="a post with table of contents" /><published>2023-03-20T15:59:00+00:00</published><updated>2023-03-20T15:59:00+00:00</updated><id>https://squirogar.github.io/blog/2023/table-of-contents</id><content type="html" xml:base="https://squirogar.github.io/blog/2023/table-of-contents/"><![CDATA[<p>This post shows how to add a table of contents in the beginning of the post.</p>

<h2 id="adding-a-table-of-contents">Adding a Table of Contents</h2>

<p>To add a table of contents to a post, simply add</p>
<div class="language-yml highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="na">toc</span><span class="pi">:</span>
  <span class="na">beginning</span><span class="pi">:</span> <span class="kc">true</span>
</code></pre></div></div>
<p>to the front matter of the post. The table of contents will be automatically generated from the headings in the post.</p>

<h3 id="example-of-sub-heading-1">Example of Sub-Heading 1</h3>

<p>Jean shorts raw denim Vice normcore, art party High Life PBR skateboard stumptown vinyl kitsch. Four loko meh 8-bit, tousled banh mi tilde forage Schlitz dreamcatcher twee 3 wolf moon. Chambray asymmetrical paleo salvia, sartorial umami four loko master cleanse drinking vinegar brunch. <a href="https://www.pinterest.com">Pinterest</a> DIY authentic Schlitz, hoodie Intelligentsia butcher trust fund brunch shabby chic Kickstarter forage flexitarian. Direct trade <a href="https://en.wikipedia.org/wiki/Cold-pressed_juice">cold-pressed</a> meggings stumptown plaid, pop-up taxidermy. Hoodie XOXO fingerstache scenester Echo Park. Plaid ugh Wes Anderson, freegan pug selvage fanny pack leggings pickled food truck DIY irony Banksy.</p>

<h3 id="example-of-another-sub-heading-1">Example of another Sub-Heading 1</h3>

<p>Jean shorts raw denim Vice normcore, art party High Life PBR skateboard stumptown vinyl kitsch. Four loko meh 8-bit, tousled banh mi tilde forage Schlitz dreamcatcher twee 3 wolf moon. Chambray asymmetrical paleo salvia, sartorial umami four loko master cleanse drinking vinegar brunch. <a href="https://www.pinterest.com">Pinterest</a> DIY authentic Schlitz, hoodie Intelligentsia butcher trust fund brunch shabby chic Kickstarter forage flexitarian. Direct trade <a href="https://en.wikipedia.org/wiki/Cold-pressed_juice">cold-pressed</a> meggings stumptown plaid, pop-up taxidermy. Hoodie XOXO fingerstache scenester Echo Park. Plaid ugh Wes Anderson, freegan pug selvage fanny pack leggings pickled food truck DIY irony Banksy.</p>

<h2 id="table-of-contents-options">Table of Contents Options</h2>

<p>If you want to learn more about how to customize the table of contents, you can check the <a href="https://github.com/toshimaru/jekyll-toc">jekyll-toc</a> repository.</p>

<h3 id="example-of-sub-heading-2">Example of Sub-Heading 2</h3>

<p>Jean shorts raw denim Vice normcore, art party High Life PBR skateboard stumptown vinyl kitsch. Four loko meh 8-bit, tousled banh mi tilde forage Schlitz dreamcatcher twee 3 wolf moon. Chambray asymmetrical paleo salvia, sartorial umami four loko master cleanse drinking vinegar brunch. <a href="https://www.pinterest.com">Pinterest</a> DIY authentic Schlitz, hoodie Intelligentsia butcher trust fund brunch shabby chic Kickstarter forage flexitarian. Direct trade <a href="https://en.wikipedia.org/wiki/Cold-pressed_juice">cold-pressed</a> meggings stumptown plaid, pop-up taxidermy. Hoodie XOXO fingerstache scenester Echo Park. Plaid ugh Wes Anderson, freegan pug selvage fanny pack leggings pickled food truck DIY irony Banksy.</p>

<h3 id="example-of-another-sub-heading-2">Example of another Sub-Heading 2</h3>

<p>Jean shorts raw denim Vice normcore, art party High Life PBR skateboard stumptown vinyl kitsch. Four loko meh 8-bit, tousled banh mi tilde forage Schlitz dreamcatcher twee 3 wolf moon. Chambray asymmetrical paleo salvia, sartorial umami four loko master cleanse drinking vinegar brunch. <a href="https://www.pinterest.com">Pinterest</a> DIY authentic Schlitz, hoodie Intelligentsia butcher trust fund brunch shabby chic Kickstarter forage flexitarian. Direct trade <a href="https://en.wikipedia.org/wiki/Cold-pressed_juice">cold-pressed</a> meggings stumptown plaid, pop-up taxidermy. Hoodie XOXO fingerstache scenester Echo Park. Plaid ugh Wes Anderson, freegan pug selvage fanny pack leggings pickled food truck DIY irony Banksy.</p>]]></content><author><name></name></author><category term="sample-posts" /><category term="toc" /><summary type="html"><![CDATA[an example of a blog post with table of contents]]></summary></entry><entry><title type="html">a post with giscus comments</title><link href="https://squirogar.github.io/blog/2022/giscus-comments/" rel="alternate" type="text/html" title="a post with giscus comments" /><published>2022-12-10T15:59:00+00:00</published><updated>2022-12-10T15:59:00+00:00</updated><id>https://squirogar.github.io/blog/2022/giscus-comments</id><content type="html" xml:base="https://squirogar.github.io/blog/2022/giscus-comments/"><![CDATA[<p>This post shows how to add GISCUS comments.</p>]]></content><author><name></name></author><category term="sample-posts" /><category term="external-services" /><summary type="html"><![CDATA[an example of a blog post with giscus comments]]></summary></entry></feed>