<?xml version="1.0" encoding="utf-8"?><feed xmlns="http://www.w3.org/2005/Atom" xml:lang="en"><generator uri="https://jekyllrb.com/" version="4.3.2">Jekyll</generator><link href="squirogar.github.io/feed.xml" rel="self" type="application/atom+xml" /><link href="squirogar.github.io/" rel="alternate" type="text/html" hreflang="en" /><updated>2023-06-10T21:39:50+00:00</updated><id>squirogar.github.io/feed.xml</id><title type="html">blank</title><subtitle>A simple, whitespace theme for academics. Based on [*folio](https://github.com/bogoli/-folio) design.
</subtitle><entry><title type="html">K-means clustering algorithm</title><link href="squirogar.github.io/blog/2023/clustering-kmeans/" rel="alternate" type="text/html" title="K-means clustering algorithm" /><published>2023-06-09T21:41:00+00:00</published><updated>2023-06-09T21:41:00+00:00</updated><id>squirogar.github.io/blog/2023/clustering-kmeans</id><content type="html" xml:base="squirogar.github.io/blog/2023/clustering-kmeans/"><![CDATA[<p>K-means es un algoritmo de clustering. Antes de ver en qué consiste K-means, es bueno saber qué es un algoritmo de clustering.</p>

<h1 id="clustering">Clustering</h1>
<p>Los algoritmos de clustering son un tipo de algoritmo de Unsupervised Learning (Aprendizaje no supervisado) y se encargan de agrupar los datos en <em>clusters</em> o grupos. Específicamente, el algoritmo mira los datos y automáticamente los agrupa, encontrando así la relación o similitud que hay entre ellos. Los puntos que pertenecen a un mismo cluster son más similares entre sí en comparación con puntos de otros clusters. La similitud entre puntos se basa en la distancia que haya entre ellos, por lo que mientras más juntos los puntos en un cluster, más similares son.</p>

<h1 id="diferencia-con-supervised-learning">Diferencia con supervised learning</h1>
<p>En supervised learning tenemos un dataset compuesto por:
\(\{(x^{(1)},y^{(1)}), (x^{(2)},y^{(2)}), (x^{(3)},y^{(3)}), ..., (x^{(m)},y^{(m)})
\}\)</p>

<p>En este dataset tenemos las input features \(x\) y las true labels \(y\).</p>

<p>Pero en unsupervised learning nuestro dataset no tiene estas respuestas correctas o true labels \(y\), sino que sólo está compuesto por las input features \(x\).
\(\{x^{(1)}, x^{(2)}, x^{(3)}, ..., x^{(m)}
\}\)</p>

<p>Como no tenemos las output targets \(y\), no somos capaces de decirle al algoritmo cuál es la respuesta correcta \(y\) que queremos predecir. En vez de eso, vamos a preguntarle al algoritmo que encuentre alguna estructura interesante sobre esta data, por ejemplo, que la agrupe en clusters, así quizas podemos obtener conocimiento que a simple vista sea difícil de adquirir.</p>

<h1 id="k-means-el-algoritmo-de-clustering-más-utilizado">K-means: el algoritmo de clustering más utilizado</h1>
<p>Para ejecutar K-means necesitamos un dataset sin labels \(y\).</p>

<h2 id="i-procedimiento">I. Procedimiento</h2>
<p>El algoritmo consta de 3 pasos. El primero se realiza una sola vez, mientras que los 2 siguientes se ejecutan varias veces. K-means es un algoritmo iterativo, por lo que dependiendo de la cantidad de iteraciones que le demos podremos obtener mejores resultados.</p>

<ol>
  <li>Lo primero que hace K-means es tomar suposiciones aleatorias de dónde podrían estar los centroids de los clusters que queremos encontrar.
    <ul>
      <li>Podemos decidir cuántos clusters vamos a encontrar. Por ejemplo, 2.</li>
      <li>Si elegimos 2, entonces K-means elegirá aleatoriamente 2 puntos donde podrían estar los centroids de estos dos clusters. Como son suposiciones aleatorias iniciales no son particulamente buenas.</li>
      <li>Un <strong>centroid</strong> es el centro de un cluster. Y un <strong>cluster</strong> es un grupo de puntos de datos relacionados.</li>
    </ul>
  </li>
  <li>
    <p>K-means repetidamente hará estos dos pasos:</p>

    <p>2.1 <strong>Asignar puntos de datos a los centroids:</strong> K-means recorrerá cada dato en nuestro dataset y le asignará el centroid más cercano, ya sea el <strong>centroid1</strong> o al <strong>centroid2</strong> (recordar que para este ejemplo elegimos 2 clusters).</p>

    <p>2.2 <strong>Mover los centroids:</strong> K-means mirará todos los puntos de datos asignados al <strong>centroid1</strong> y tomará un promedio de ellos. Luego, moverá el <strong>centroid1</strong> a la ubicación del promedio de estos puntos de datos.Para el <strong>centroid2</strong> se hace exactamente lo mismo: se toman todos los puntos de datos asignados a este centroid y se calcula el promedio, para luego mover el <strong>centroid2</strong> a esa ubicación.</p>
  </li>
  <li>K-means recorrerá todos los datos de nuestro dataset de nuevo y repetirá el <strong>paso 2</strong> completo. En otras palabras, asociaremos cada punto de dato al centroid más cercano (<strong>paso 2.1</strong>). Algunos datos puede que sean asignados a diferentes centroids con el pasar de las iteraciones, esto es normal, ya que queremos determinar correctamente cuáles son sus centroids más cercanos. Después de esto, recalcularemos los centroids (o los moveremos, que es lo mismo) (<strong>paso 2.2</strong>). Hacemos esto hasta que no hayan más cambios en la asignación de los datos a centroids o que no hayan cambios al mover los centroids. En ese momento, K-means habrá <strong>convergido</strong>. Así, se habrán formado 2 clusters conteniendo cada uno data points similares.</li>
</ol>

<h2 id="ii-definición-formal-de-k-means">II. Definición formal de K-means</h2>

<ol>
  <li>Aleatoriamente inicializar \(K\) centroids \(\mu_1, \mu_2, ..., \mu_K\):
    <ul>
      <li>Elegir aleatoriamente una ubicación para los centroids</li>
      <li>\(\mu_1, \mu_2, ..., \mu_K\) son vectores que tienen la misma dimensión que el dataset de ejemplos de entrenamiento \(x^{(1)}, x^{(2)}, ..., x^{(m)}\). Todos los centroids son listas de \(n\) números o vectores de \(n\)-dimensiones, donde \(n\) es el número de features por cada uno de los ejemplos de entrenamiento. Por ejemplo, si \(n=2\), tenemos 2 features \(x_1\) y \(x_2\), entonces \(\mu_1\) y \(\mu_2\) serán vectores con 2 números en ellos.</li>
    </ul>
  </li>
  <li>
    <p>K-means repetidamente llevará a cabo los dos pasos descritos en la sección <strong>I.</strong>:</p>

    <p>2.1 Asignar cada uno de los puntos de datos \(x^{(i)}\) al centroid \(\mu\) más cercano.</p>

    <p>2.2 Mover los centroids de los clusters. Vamos a actualizar la ubicación de los centroids para ser el promedio o media de los datos asignados a cada cluster. Concretamente, veremos todos los puntos de datos asignados al centroid \(\mu\) y calcularemos el promedio. El promedio se calcula para cada dimensión o feature, entonces: veremos el valor de cada punto en el eje x (feature \(x_1\)) y lo promediamos; luego, veremos el valor de cada punto en el eje y (feature \(x_2\)) y lo promediamos. Con esto, tenemos la media de los puntos que pertenecen al cluster \(\mu\) y que será la nueva ubicación de este último. En python esto sería:</p>
    <div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>     a = np.array([x^(1), x^(5), x^(6), x^(10)])
     mu_1 = np.sum(a) / 4
     # x^(m) es un vector de tam n, con n=num. features
     # mu_1 es un vector de tam n, con n=num. features
     # numpy utiliza vectorización, por lo que las operaciones se aplican al vector completo
</code></pre></div>    </div>
  </li>
</ol>

<h2 id="iii-pseudocódigo">III. Pseudocódigo</h2>

<p>En pseudocódigo el algoritmo es:</p>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>Aleatoriamente inicializar K clusters mu_1, mu_2, ..., mu_K

repeat {
	# asignar cada data point al centroid más cercano
	for i = 1 to m:
		c^(i) := índice (de 1 a K) del centroids más cercano a x^(i).
		# esto es igual a: min_k ||x^(i) - mu_k||^2 (con 1 &lt;= k &lt;= K)
		# ver nota

	# mver los centroids
	for k = 1 to K:
		mu_k := media de los data points asignados al cluster k
}
</code></pre></div></div>

<p>Nota:</p>
<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>c^(i) := índice (de 1 a K) del centroids más cercano a x^(i).
</code></pre></div></div>
<p>Esto es lo mismo que: 
\(min_k ||x^{(i)}-\mu_k||^2\)</p>

<p>Explicación:</p>
<ul>
  <li>\(x^{(i)}\) es el i-ésimo ejemplo de entrenamiento en el dataset</li>
  <li>matemáticamente esto es calcular la distancia entre \(x^{(i)}\) y \(\mu_k\).</li>
  <li>El cálculo de la distancia entre 2 puntos es:
\(||x^{(i)} - \mu_k||\)
A esto se le llama norma \(L2\).</li>
  <li>Queremos encontrar el centroid \(k\) que minimiza esta distancia al cuadrado.</li>
</ul>

<h3 id="1-norma-l2">1. Norma \(L2\)</h3>
<p>Dado el vector 
\(\begin{align}
    x &amp;= \begin{bmatrix}
           x_{1} \\\\
					 x_{2} \\\\
           \vdots \\\\
           x_{n}
         \end{bmatrix}
\end{align}\)</p>

<p>La \(L2\) norm está dada por:</p>

\[||x||=(\sum_{i=1}^{n} {x}_i^2)^{1/2}=\sqrt{\sum_{i=1}^{n} {x}_i^2}\]

<p>Esta es la norma euclidiana que se utiliza para calcular la distancia entre 2 puntos.</p>

<p>En python podemos calcular la norma \(L2\) así:</p>
<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>numpy.linalg.norm(x)
</code></pre></div></div>

<p>Sin embargo, en machine learning se utiliza la norma \(L2\) al cuadrado: \(||x||^2\), ¿por qué?:</p>
<ul>
  <li>Porque se deshace de la raíz cuadrada haciéndola más fácil para operar.</li>
  <li>y terminamos con una simple suma de cada elemento del vector al cuadrado.</li>
  <li>Es ampliamente usada en machine learning porque puede ser calculada con la operación de vector \(x^\text{T}x\), operación que se puede hacer muy rápidamente mediante la vectorización. Así, tenemos mejor rendimiento debido a la optimización (*).</li>
  <li>El centroid con la distancia al cuadrado más pequeña es el mismo que el centroid con la distancia más pequeña sin elevar al cuadrado.</li>
</ul>

\[||x||^2=\sum_{i=1}^{n} {x}_i^2= (x_1)^2+(x_2)^2+...+(x_n)^2\]

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code># l2-norm squared
np.linalg.norm(x)**2
</code></pre></div></div>

<p>(*):</p>
<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code># x^Tx es lo mismo que la l2-norm
x^T = [1,
	2,
	3]
x = [1, 2, 3]
x^T dot product x = (1*1 + 2*2 + 3*3) = 14
</code></pre></div></div>

<h2 id="iv-función-de-costo-para-k-means">IV. Función de costo para K-means</h2>
<p>K-means también optimiza una cost function específica, aunque el algoritmo de optimización que utiliza para optimizar esa función no es gradient descent, en realidad, es el algoritmo que ya vimos anteriormente: el mismo K-means.</p>

<h3 id="1-notación">1. Notación</h3>
<ul>
  <li>\(c^{(i)}\)= índice del cluster \(1, 2, ..., K\) al que se asigna actualmente el ejemplo \(x^{(i)}\)</li>
  <li>\(\mu_k\) = centroid del cluster \(k\) (con \(1 &lt;= k &lt;= K\))</li>
  <li>\(\mu_{c^{(i)}}\)= centroid del cluster al que se le ha asignado el ejemplo \(x^{(i)}\).
    <ul>
      <li>Por ejemplo: training example 10, \(x^{(10)}\). ¿Cuál es la ubicación del centroid al que el décimo training example ha sido asignado? Buscaremos \(c^{(10)}\), entonces \(\mu_{c^{(10)}}\) es la ubicación del centroid del cluster al que se ha asignado \(x^{(10)}\).</li>
    </ul>
  </li>
</ul>

<h3 id="2-cost-function">2. Cost function</h3>
<p>K-means intenta minimizar la siguiente cost function:
\(J(c^{(1)}, ..., c^{(m)}, \mu_1, ..., \mu_K) = \frac{1}{m} \cdot \sum_{i=1}^{m}(||x^{(i)} - \mu_{c^{(i)}}||^2)\)</p>

<ul>
  <li>El nombre de esta cost function es: <strong>Distortion function</strong>.</li>
  <li>\(J\) es función de \(c^{(1)},...,c^{(m)}\) y \(\mu_1,...,\mu_K\).</li>
  <li>\(J\) es el promedio de la distancia al cuadrado entre cada training example \(x^{(i)}\) y la ubicación del centroid \(\mu_{c^{(i)}}\) al que se le asignó el training example \(x^{(i)}\). Por ejemplo, para el décimo ejemplo la distancia al cuadrado sería: \((x^{(10)} - \mu_{c^{(10)}})^2\).</li>
  <li>K-means con esto intenta encontrar asignaciones de puntos a centroids y ubicaciones de centroids que minimizan la distancia al cuadrado.</li>
</ul>

<h3 id="3-explicación-de-la-cost-function">3. Explicación de la cost function</h3>
<p>Una vez asignados los puntos a los centroids, medimos las distancias entre dichos puntos y su repectivo centroid, para después calcular el cuadrado de todas esas distancias y obtener el promedio que finalmente será la cost function \(J\).</p>

<p>En cada iteración, actualizaremos las asignaciones a clusters \(c^{(1)},...,c^{(m)}\); o actualizaremos las posiciones de los centroids \(\mu_1,...,\mu_K\) para seguir reduciendo la función de costo \(J\).</p>

<h3 id="4-por-qué-k-means-intenta-minimizar-la-función-de-costo-j">4. ¿Por qué K-means intenta minimizar la función de costo \(J\)?</h3>
<p>Veamos el algoritmo K-means:</p>
<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>#paso 0: aleatoriamente inicializar K centroids mu_1, mu_2, ..., mu_K

repeat {
	# paso 1: asignar puntos al centroid más cercano
	for i = 1 to m
		c^(i) := índice del centroid más cercano a x^(i)

	# paso 2: mover los centroids
	for k = 1 to K
		mu_k := media de los puntos del cluster k
}
</code></pre></div></div>
<p>K-means intenta minimizar la cost function \(J\) de la siguiente forma:</p>

<ul>
  <li>El <strong>paso 1</strong> busca actualizar \(c^{(1)},...,c^{(m)}\) para minimizar la función de costo \(J\) tanto como sea posible mientras mantiene \(\mu_1,...,\mu_K\) fijos.
    <ul>
      <li>Esto es porque \(c^{(i)}\) es el centroid más cercano a \(x^{(i)}\), lo que hace que la distancia sea lo más pequeña posible.</li>
    </ul>
  </li>
  <li>El <strong>paso 2</strong> busca actualizar \(\mu_1,...,\mu_K\) y dejar \(c^{(1)},...,c^{(m)}\) fijos para minimizar la cost function \(J\) tanto como sea posible.
    <ul>
      <li>Hacer que \(\mu_k\) sea la media de los puntos asignados minimiza la función de costo, ya que minimiza la distancia al cuadrado entre los puntos y el centroid. Por ejemplo:
        <ul>
          <li>distancia <em>punto1</em> a <em>centroid1</em> es \(1\)</li>
          <li>distancia <em>punto2</em> a <em>centroid1</em> es \(9\)</li>
          <li>average: \(J = (1^2 + 9^2) / 2 = 41\)</li>
          <li>Pero, si movemos el centroid donde es el promedio de los puntos: \((1+11) / 2 = 6\), entonces si calculamos \(J\):</li>
          <li>distancia <em>punto1</em> a <em>centroid1</em> es \(5\)</li>
          <li>distancia <em>punto2</em> a <em>centroid1</em> es \(5\)</li>
          <li>average: \(J = (5^2 + 5^2) / 2 = 25\) &lt;- es más pequeña \(J\)!</li>
        </ul>
      </li>
    </ul>
  </li>
</ul>

<h2 id="v-convergencia-de-k-means">V. Convergencia de K-means</h2>
<p>El hecho de que K-means optimice una cost function \(J\) significa que la convergencia está garantizada, es decir, en cada iteracion la distortion cost function debería bajar o mantenerse igual, pero <strong>NUNCA SUBIR!</strong>, si ese es el caso, hay un bug en el código, ya que en cada paso de K-means se está estableciendo el valor \(c^{(i)}\) y \(\mu_k\) para intentar reducir la cost function.</p>

<p>También, si la cost function para de bajar, esto nos da una forma para probar si K-means ha convergido. Una vez que haya una sola iteración que se mantiene igual, eso usualmente significa que K-means ha convergido y deberíamos parar el algoritmo. O en algunos casos, ejecutaremos K-means por un largo tiempo, y la cost function va bajando muy, muy lentamente, esto es parecido al gradient descent, donde quizás ejecutarlo por más tiempo puede mejorar, pero si la tasa a la que la cost function está bajando es muy, muy lenta, también podemos decir que es suficiente y está muy cerca de la convergencia.</p>

<h2 id="vi-inicializando-clusters-de-k-means">VI. Inicializando clusters de K-means</h2>
<p>El uso de múltiples inicializaciones aleatorias diferentes de los centroids dará como resultado la búsqueda de un mejor conjunto de clusters.</p>

<p>El primer paso es elegir ubicaciones aleatorias como suposiciones iniciales para los centroids \(\mu_1,...,\mu_K\).</p>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>#paso 0: aleatoriamente inicializar K centroids mu_1, mu_2, ..., mu_K

repeat {
	# paso 1: asignar puntos al centroid más cercano
	# paso 2: mover los centroids
}
</code></pre></div></div>
<p>¿Cómo implementamos el <strong>paso 0</strong>?</p>

<h3 id="1-a-tener-en-cuenta">1. A tener en cuenta</h3>
<p>Siempre elegir un valor de \(K\) menor a \(m\):</p>

\[K &lt; m\]

<p>\(K\) es el número de clusters y \(m\) es el número de training examples.</p>

<p>No tiene sentido tener más centroids que ejemplos \(m\) porque no habrán suficientes training examples para que cada cluster tenga al menos un ejemplo dentro.</p>

<h3 id="2-inicialización-de-clusters">2. Inicialización de clusters</h3>
<p>La forma de inicializarlos más común es aleatoriamente escoger \(K\) datos de nuestro dataset de entrenamiento y establecer \(\mu_1, ..., \mu_K\) con un valor igual a estos \(K\) datos. Por ejemplo, si \(K=2\), entonces elegimos 2 training examples y ubicamos los 2 centroids, \(\mu_1\) y \(\mu_2\) en el mismo lugar que éstos.</p>

<h3 id="3-a-considerar-en-la-inicialización-de-clusters">3. A considerar en la inicialización de clusters</h3>
<p>La forma anterior de inicializar los cluster centroids tiene algunos puntos a considerar:</p>
<ul>
  <li>
    <p>Con este método hay una posibilidad de terminar con 2 (o más) clusters inicializados muy cerca. Y dependiendo de cómo elijamos las posiciones iniciales aleatorias de los centroids, K-means terminará escogiendo diferentes set de clusters para el dataset que tenemos.</p>
  </li>
  <li>
    <p>Si queremos encontrar 3 clusters y justo tenemos 3 nubes de puntos separadas, el resultado óptimo sería un cluster que agupe a cada nube de puntos. Sin embargo, podemos tener una inicialización diferente que tenga 2 centroids a muy poca distancia (ambos están en la misma nube de puntos), por lo que estos 2 clusters van a tener muy pocos puntos mientras que el tercero tendrá muchos puntos dispersos (las otras 2 nubes de puntos), generando así un resultado subóptimo o mínimo local. Otra situación que puede suceder al inicializar aleatoriamente los centroids es, que un cluster se quede con muchos puntos, otro se quede con unos pocos y finalmente el último se quede sin puntos, generando que K-means se quede estancado en un resultado subóptimo.</p>
  </li>
</ul>

<h3 id="4-solución-al-resultado-subóptimo-de-k-means">4. Solución al resultado subóptimo de k-means</h3>
<p>Si le damos muchas oportunidades o ejecuciones al algoritmo K-means para encontrar el mejor óptimo local se podrá solucionar esto. Así, podemos intentar múltiples inicializaciones aleatorias para tener una mejor chance de encontrar mejores clusters.</p>

<h3 id="5-y-si-hay-un-cluster-que-no-tenga-data-points-asignados">5. ¿Y si hay un cluster que no tenga data points asignados?</h3>
<p>En el <strong>paso 2</strong>, \(\mu_k\) intentaría calcular el promedio de \(0\) training examples, lo cual no está definido.</p>

<p>Hay 2 soluciones:</p>
<ul>
  <li>Solución 1: La más común: eliminar ese cluster, por lo que terminaremos con \(K-1\) clusters</li>
  <li>Solución 2: Si realmente necesitamos los \(K\) clusters, entonces aleatoriamente reinicializar ese cluster y esperar a que se le asignen data points la próxima vez.</li>
</ul>

<h2 id="vii-ejecución-de-k-means-múltiples-veces">VII. Ejecución de K-means múltiples veces</h2>
<p>Si ejecutamos varias veces a K-means, debemos elegir uno de entre varios clustering. Una forma de elegir cuál clustering es el mejor, o sea, qué tirada nos dió mejores resultados, es calcular la cost function \(J\) para <strong>todas</strong> las soluciones, para todas las alternativas de clustering encontradas en las ejecuciones de K-means, y elegir una de éstas de acuerdo a cuál da el menor valor para la cost function \(J\).</p>

<p>Nos damos cuenta que en la mejor solución de clustering, los data points en cada cluster tienen una distancia al cuadrado relativamente pequeña con respecto a su centroid, por lo tanto, la cost function \(J\) será relativamente pequeña para esta solución. En cambio, en las demás soluciones en algunos clusters, habrá una distancia al cuadrado más grande entre los puntos y el centroid correspondiente, por lo cual la función de costo será más grande.</p>

<h3 id="1-formulación-de-k-means-inicializado-aleatoriamente-múltiples-veces">1. Formulación de K-means inicializado aleatoriamente múltiples veces</h3>
<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>for i = 1 to 100 { #100 inicializaciones aleatorias, o sea, ejecutamos 100 veces K-means
	Aleatoriamente inicializar los K centroids
		# Elegir K ejemplos de entrenamiento aleatorios e inicializar los K centroids en esas ubicaciones.

	Ejecutar K-means hasta la convergencia:
        1. Obtener c^(1), ..., c^(m), mu_1, mu_2, ..., mu_K

	    2. Calcular cost function (distortion) J(c^(1), ..., c^(m), mu_1, mu_2, ..., mu_K)
}
</code></pre></div></div>

<blockquote>
  <p>Elegir el set de clusters que dan la cost function \(J\) más baja.</p>
</blockquote>

<p>Después de ejecutar este bucle 100 veces (el número es arbitrario), escogeremos el set de clusters que nos dan el costo \(J\) más bajo. Si hacemos esto, a menudo obtendremos un mejor set de clusters que si ejecutaramos K-means una sola vez.</p>

<h4 id="sobre-el-número-de-veces-que-tenemos-que-ejecutar-k-means">Sobre el número de veces que tenemos que ejecutar K-means</h4>
<p>Se recomienda un número entre 50 a 1000 (en el ejemplo pusimos 100). Si ejecutamos K-means más de 1000 veces tiende a ser computacionalmente caro y obtendremos muy pocas mejoras si lo ejecutamos más de esas veces.</p>

<h2 id="viii-cómo-elegir-el-número-de-clusters-k">VIII. Cómo elegir el número de clusters K</h2>
<p>Para muchos problemas de clustering, el valor correcto de \(K\) es ambigüo. Hay muchas aplicaciones donde los datos no dan un indicador claro de cuántos clusters hay en ellos.</p>

<h3 id="1-método-1-elbow-method">1. Método 1: Elbow method</h3>
<p>Técnica para encontrar automáticamente el número de clusters para usar en una aplicación.</p>

<p>Ejecutamos K-means con un variedad de valores de \(K\) y graficamos la función de costo o la <strong>distortion function</strong> \(J\) (eje \(y\)) como una función del número de clusters (eje \(x\)).</p>

<p>Lo que encontramos es que cuando tienes muy pocos clusters, la distorion function o la cost function \(J\) será alta y a medida de que aumentes el número de clusters, bajará. Veremos que la cost function va disminuyendo rápidamente hasta que lleguemos a un punto determinado, luego de ese punto, la función de costo disminuirá más lentamente (la curva se va aplanando). Básicamente se elige \(K\) en el punto donde la curva se dobla (se forma como un codo), o sea, donde termina el descenso rápido y empieza a descender más lentamente.</p>

<h4 id="el-problema-del-elbow-method">El problema del elbow method</h4>
<p>El problema es que en muchas aplicaciones la cost function tiene un descenso suavizado, por lo que paulatinamente va descendiendo y no se forma un codo notorio para elgir un valor de \(K\).</p>

<h3 id="2-método-2-elegir-el-k-que-más-hace-disminuir-j">2. Método 2: elegir el K que más hace disminuir J</h3>
<p>Elegir \(K\) que minimiza la función de costo \(J\) no sirve porque implicaría elegir el valor de \(K\) más grande posible, ya que tener más clusters siempre reducirá la cost function.</p>

<h3 id="3-método-3-recomendado-decidir-qué-tiene-sentido-para-la-aplicación">3. Método 3 (recomendado): decidir qué tiene sentido para la aplicación</h3>
<p>A menudo, ejecutamos K-means para obtener clusters y usarlos para algún propósito posterior. Lo que es recomendable, es evaluar K-means en función de qué tan bien se desempeñe para ese propósito posterior. Por ejemplo, tenemos una nube de puntos dispersos y los ejes son: <em>altura</em> y <em>peso</em>. Dado estas medidas de personas, queremos agruparlas en clusters para determinar cómo medir las poleras <em>S (small)</em>, <em>M (medium)</em> y <em>L (large)</em>. Elegimos \(K=3\). Sin embargo, podemos ejecutar K-means con 5 clusters \((K=5)\) y así podemos medir las poleras de acuerdo a <em>XS (extra small), S, M, L y XL (extra large)</em>. Entonces, Ambas opciones son completamente válidas, por lo que la elección dependerá de lo que tenga sentido para el negocio de las poleras.</p>

<p>Hay un trade-off entre qué tan bien las poleras entran o se ajustan a las personas, dependiendo si tenemos 3 tallas o 5 tallas, pero habrá un costo extra asociado a la manufactura y al envío de las poleras. ¿Qué hacemos? Ejecutar K-means con \(K=3\) y \(K=5\) y con estas 2 soluciones elegir, basandonos en el trade-off entre fabricar las poleras con más tamaños implicando un mejor ajuste, y el costo extra de hacer más poleras.</p>

<h2 id="ix-por-qué-normalizar-las-features-en-k-means">IX. ¿Por qué normalizar las features en K-means?</h2>
<p>Es siempre mejor normalizar las features cuando usamos K-means porque K-means es basado en distancia, lo que significa que es sensible a la diferencia de las escalas de las features.</p>

<p>Por ejemplo, tenemos 2 features:</p>
<ol>
  <li>Peso (lbs)</li>
  <li>Altura (feet)</li>
</ol>

<p>Utilizamos éstas para predecir si una persona necesita una polera <em>S</em> o una <em>L</em>.</p>

<p>En nuestro training set tenemos 2 personas ya en los clusters:</p>
<ol>
  <li>Adam (175 lbs, 5.9 ft) en <em>L</em>.</li>
  <li>Lucy (115 lbs, 5.2 ft) en <em>S</em>.</li>
</ol>

<p>Tenemos una nueva persona: Alan (140 lbs, 6.1 ft).</p>

<p>Nuestro algoritmo de clustering lo pondrá en el cluster más cercano.</p>
<ul>
  <li>
    <p>Cálculo distancia en cluster 1: \((175-140)^2 + (5.9-6.1)^2 = \sqrt{(1225 + 0.04)} = 35\)</p>
  </li>
  <li>
    <p>Cálculo distancia en cluster 2: \((115-140)^2 + (5.2-6.1)^2 = \sqrt{(625 + 0.81)} = 25\)</p>
  </li>
</ul>

<p>Notar el gran impacto de la feature 1 (<em>Peso</em>) en el cálculo de la distancia. Además, notar el pequeñísimo impacto de la feature 2 (<em>altura</em>) en el cálculo de la distancia.</p>

<p>Esto impactará el rendimiento de todos los modelos basados en distancia, ya que otorgará mayor peso a las variables que tienen una mayor magnitud (<em>Peso</em> en este caso). Entonces, el algoritmo está sesgado hacia una variable con mayor magnitud. Para superar este problema, podemos reducir todas las variables a la misma escala. Si no escalamos las features, la variable <em>altura</em> no tiene mucho efecto y a Alan se le asignará en el grupo <em>S</em>, lo que no tiene sentido, ya que es muy alto.</p>

<h3 id="en-resumen">En resumen</h3>
<p>El problema está en que en K-means, si las features están a escalas muy diferentes, por ejemplo, edad vs sueldo, entonces va a haber una feature que va a tener demasiado impacto sobre el cálculo de la distancia, mientras que la otra va a tener poco o casi nulo.</p>

<h2 id="x-algunas-qa">X. Algunas Q&amp;A</h2>
<ol>
  <li>¿Por qué con K-means resultan clusters tan diferentes?
    <ul>
      <li>Tiene que ver con la data. Si nuestros clusters tienen un límite bien definido entre ellos siempre encontraremos los mismos centroids y solo dependerá del parámetro \(K\). Si nuestra data no tiene un límite claro entre cada cluster (datos más dispersos) entonces inicializar los centroids aleatoriamente darán diferentes resultados dependiendo de la inicialización.</li>
    </ul>
    <ul>
      <li>Una sugerencia es usar el parámetro <code class="language-plaintext highlighter-rouge">random_state</code> cuando usemos la implementación de K-means de la librería <code class="language-plaintext highlighter-rouge">scikit-learn</code>, ya que si lo establecemos con un valor podemos garantizar que tendremos los mismos centroids cada vez que usemos el modelo (que lo entrenemos).
        <div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code> from sklearn.cluster import KMeans
 kmeans = KMeans(n_clusters=3, random_state=42)
</code></pre></div>        </div>
      </li>
    </ul>
  </li>
  <li>¿Por qué hay clusters sin puntos asignados?
    <ul>
      <li>A medida que aumenta \(K\) (más centroids) existe más probabilidad de que un centroid no sea el más cercano de ningún punto, o sea, no se le asigne ningún data point. También existe el caso que si hay 2 centroids muy cerca y le asignamos correctamente de forma aleatoria sus ubicaciones, pero tenemos datos con ejemplos repetidos; al momento de asignar los centroids más cercanos a cada uno de los puntos, puede pasar que uno le “robe” el dato a otro, quedandose con dos datos asignados y el otro sin nada, esto pasa en el caso de aplicar K-means para comprimir imágenes, en donde dos pixeles pueden tener exactamente el mismo valor.</li>
    </ul>
  </li>
</ol>]]></content><author><name></name></author><category term="algoritmos" /><summary type="html"><![CDATA[Definición, análisis, implementación y consejos del algoritmo K-means]]></summary></entry><entry><title type="html">a post with custom blockquotes</title><link href="squirogar.github.io/blog/2023/custom-blockquotes/" rel="alternate" type="text/html" title="a post with custom blockquotes" /><published>2023-05-12T19:53:00+00:00</published><updated>2023-05-12T19:53:00+00:00</updated><id>squirogar.github.io/blog/2023/custom-blockquotes</id><content type="html" xml:base="squirogar.github.io/blog/2023/custom-blockquotes/"><![CDATA[<p>This post shows how to add custom styles for blockquotes. Based on <a href="https://github.com/sighingnow/jekyll-gitbook">jekyll-gitbook</a> implementation.</p>

<p>We decided to support the same custom blockquotes as in <a href="https://sighingnow.github.io/jekyll-gitbook/jekyll/2022-06-30-tips_warnings_dangers.html">jekyll-gitbook</a>, which are also found in a lot of other sites’ styles. The styles definitions can be found on the <a href="https://github.com/alshedivat/al-folio/blob/master/_sass/_base.scss">_base.scss</a> file, more specifically:</p>

<div class="language-scss highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="cm">/* Tips, warnings, and dangers */</span>
<span class="nc">.post</span> <span class="nc">.post-content</span> <span class="nt">blockquote</span> <span class="p">{</span>
    <span class="k">&amp;</span><span class="nc">.block-tip</span> <span class="p">{</span>
    <span class="nl">border-color</span><span class="p">:</span> <span class="nf">var</span><span class="p">(</span><span class="o">--</span><span class="n">global-tip-block</span><span class="p">);</span>
    <span class="nl">background-color</span><span class="p">:</span> <span class="nf">var</span><span class="p">(</span><span class="o">--</span><span class="n">global-tip-block-bg</span><span class="p">);</span>

    <span class="nt">p</span> <span class="p">{</span>
      <span class="nl">color</span><span class="p">:</span> <span class="nf">var</span><span class="p">(</span><span class="o">--</span><span class="n">global-tip-block-text</span><span class="p">);</span>
    <span class="p">}</span>

    <span class="nt">h1</span><span class="o">,</span> <span class="nt">h2</span><span class="o">,</span> <span class="nt">h3</span><span class="o">,</span> <span class="nt">h4</span><span class="o">,</span> <span class="nt">h5</span><span class="o">,</span> <span class="nt">h6</span> <span class="p">{</span>
      <span class="nl">color</span><span class="p">:</span> <span class="nf">var</span><span class="p">(</span><span class="o">--</span><span class="n">global-tip-block-title</span><span class="p">);</span>
    <span class="p">}</span>
  <span class="p">}</span>

  <span class="k">&amp;</span><span class="nc">.block-warning</span> <span class="p">{</span>
    <span class="nl">border-color</span><span class="p">:</span> <span class="nf">var</span><span class="p">(</span><span class="o">--</span><span class="n">global-warning-block</span><span class="p">);</span>
    <span class="nl">background-color</span><span class="p">:</span> <span class="nf">var</span><span class="p">(</span><span class="o">--</span><span class="n">global-warning-block-bg</span><span class="p">);</span>

    <span class="nt">p</span> <span class="p">{</span>
      <span class="nl">color</span><span class="p">:</span> <span class="nf">var</span><span class="p">(</span><span class="o">--</span><span class="n">global-warning-block-text</span><span class="p">);</span>
    <span class="p">}</span>

    <span class="nt">h1</span><span class="o">,</span> <span class="nt">h2</span><span class="o">,</span> <span class="nt">h3</span><span class="o">,</span> <span class="nt">h4</span><span class="o">,</span> <span class="nt">h5</span><span class="o">,</span> <span class="nt">h6</span> <span class="p">{</span>
      <span class="nl">color</span><span class="p">:</span> <span class="nf">var</span><span class="p">(</span><span class="o">--</span><span class="n">global-warning-block-title</span><span class="p">);</span>
    <span class="p">}</span>
  <span class="p">}</span>

  <span class="k">&amp;</span><span class="nc">.block-danger</span> <span class="p">{</span>
    <span class="nl">border-color</span><span class="p">:</span> <span class="nf">var</span><span class="p">(</span><span class="o">--</span><span class="n">global-danger-block</span><span class="p">);</span>
    <span class="nl">background-color</span><span class="p">:</span> <span class="nf">var</span><span class="p">(</span><span class="o">--</span><span class="n">global-danger-block-bg</span><span class="p">);</span>

    <span class="nt">p</span> <span class="p">{</span>
      <span class="nl">color</span><span class="p">:</span> <span class="nf">var</span><span class="p">(</span><span class="o">--</span><span class="n">global-danger-block-text</span><span class="p">);</span>
    <span class="p">}</span>

    <span class="nt">h1</span><span class="o">,</span> <span class="nt">h2</span><span class="o">,</span> <span class="nt">h3</span><span class="o">,</span> <span class="nt">h4</span><span class="o">,</span> <span class="nt">h5</span><span class="o">,</span> <span class="nt">h6</span> <span class="p">{</span>
      <span class="nl">color</span><span class="p">:</span> <span class="nf">var</span><span class="p">(</span><span class="o">--</span><span class="n">global-danger-block-title</span><span class="p">);</span>
    <span class="p">}</span>
  <span class="p">}</span>
<span class="p">}</span>
</code></pre></div></div>

<p>A regular blockquote can be used as following:</p>

<div class="language-markdown highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="gt">&gt; This is a regular blockquote</span>
<span class="gt">&gt; and it can be used as usual</span>
</code></pre></div></div>

<blockquote>
  <p>This is a regular blockquote
and it can be used as usual</p>
</blockquote>

<p>These custom styles can be used by adding the specific class to the blockquote, as follows:</p>

<div class="language-markdown highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="gt">&gt; ##### TIP</span>
<span class="gt">&gt;</span>
<span class="gt">&gt; A tip can be used when you want to give advice</span>
<span class="gt">&gt; related to a certain content.</span>
{: .block-tip }
</code></pre></div></div>

<blockquote class="block-tip">
  <h5 id="tip">TIP</h5>

  <p>A tip can be used when you want to give advice
related to a certain content.</p>
</blockquote>

<div class="language-markdown highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="gt">&gt; ##### WARNING</span>
<span class="gt">&gt;</span>
<span class="gt">&gt; This is a warning, and thus should</span>
<span class="gt">&gt; be used when you want to warn the user</span>
{: .block-warning }
</code></pre></div></div>

<blockquote class="block-warning">
  <h5 id="warning">WARNING</h5>

  <p>This is a warning, and thus should
be used when you want to warn the user</p>
</blockquote>

<div class="language-markdown highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="gt">&gt; ##### DANGER</span>
<span class="gt">&gt;</span>
<span class="gt">&gt; This is a danger zone, and thus should</span>
<span class="gt">&gt; be used carefully</span>
{: .block-danger }
</code></pre></div></div>

<blockquote class="block-danger">
  <h5 id="danger">DANGER</h5>

  <p>This is a danger zone, and thus should
be used carefully</p>
</blockquote>]]></content><author><name></name></author><category term="sample-posts" /><category term="blockquotes" /><summary type="html"><![CDATA[an example of a blog post with custom blockquotes]]></summary></entry><entry><title type="html">a post with table of contents on a sidebar</title><link href="squirogar.github.io/blog/2023/sidebar-table-of-contents/" rel="alternate" type="text/html" title="a post with table of contents on a sidebar" /><published>2023-04-25T14:14:00+00:00</published><updated>2023-04-25T14:14:00+00:00</updated><id>squirogar.github.io/blog/2023/sidebar-table-of-contents</id><content type="html" xml:base="squirogar.github.io/blog/2023/sidebar-table-of-contents/"><![CDATA[<p>This post shows how to add a table of contents as a sidebar.</p>

<h2 id="adding-a-table-of-contents">Adding a Table of Contents</h2>

<p>To add a table of contents to a post as a sidebar, simply add</p>
<div class="language-yml highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="na">toc</span><span class="pi">:</span>
  <span class="na">sidebar</span><span class="pi">:</span> <span class="s">left</span>
</code></pre></div></div>
<p>to the front matter of the post. The table of contents will be automatically generated from the headings in the post. If you wish to display the sidebar to the right, simply change <code class="language-plaintext highlighter-rouge">left</code> to <code class="language-plaintext highlighter-rouge">right</code>.</p>

<h3 id="example-of-sub-heading-1">Example of Sub-Heading 1</h3>

<p>Jean shorts raw denim Vice normcore, art party High Life PBR skateboard stumptown vinyl kitsch. Four loko meh 8-bit, tousled banh mi tilde forage Schlitz dreamcatcher twee 3 wolf moon. Chambray asymmetrical paleo salvia, sartorial umami four loko master cleanse drinking vinegar brunch. <a href="https://www.pinterest.com">Pinterest</a> DIY authentic Schlitz, hoodie Intelligentsia butcher trust fund brunch shabby chic Kickstarter forage flexitarian. Direct trade <a href="https://en.wikipedia.org/wiki/Cold-pressed_juice">cold-pressed</a> meggings stumptown plaid, pop-up taxidermy. Hoodie XOXO fingerstache scenester Echo Park. Plaid ugh Wes Anderson, freegan pug selvage fanny pack leggings pickled food truck DIY irony Banksy.</p>

<h3 id="example-of-another-sub-heading-1">Example of another Sub-Heading 1</h3>

<p>Jean shorts raw denim Vice normcore, art party High Life PBR skateboard stumptown vinyl kitsch. Four loko meh 8-bit, tousled banh mi tilde forage Schlitz dreamcatcher twee 3 wolf moon. Chambray asymmetrical paleo salvia, sartorial umami four loko master cleanse drinking vinegar brunch. <a href="https://www.pinterest.com">Pinterest</a> DIY authentic Schlitz, hoodie Intelligentsia butcher trust fund brunch shabby chic Kickstarter forage flexitarian. Direct trade <a href="https://en.wikipedia.org/wiki/Cold-pressed_juice">cold-pressed</a> meggings stumptown plaid, pop-up taxidermy. Hoodie XOXO fingerstache scenester Echo Park. Plaid ugh Wes Anderson, freegan pug selvage fanny pack leggings pickled food truck DIY irony Banksy.</p>

<h2 data-toc-text="Customizing" id="customizing-your-table-of-contents">Customizing Your Table of Contents</h2>

<p>If you want to learn more about how to customize the table of contents of your sidebar, you can check the <a href="https://afeld.github.io/bootstrap-toc/">bootstrap-toc</a> documentation. Notice that you can even customize the text of the heading that will be displayed on the sidebar.</p>

<h3 id="example-of-sub-heading-2">Example of Sub-Heading 2</h3>

<p>Jean shorts raw denim Vice normcore, art party High Life PBR skateboard stumptown vinyl kitsch. Four loko meh 8-bit, tousled banh mi tilde forage Schlitz dreamcatcher twee 3 wolf moon. Chambray asymmetrical paleo salvia, sartorial umami four loko master cleanse drinking vinegar brunch. <a href="https://www.pinterest.com">Pinterest</a> DIY authentic Schlitz, hoodie Intelligentsia butcher trust fund brunch shabby chic Kickstarter forage flexitarian. Direct trade <a href="https://en.wikipedia.org/wiki/Cold-pressed_juice">cold-pressed</a> meggings stumptown plaid, pop-up taxidermy. Hoodie XOXO fingerstache scenester Echo Park. Plaid ugh Wes Anderson, freegan pug selvage fanny pack leggings pickled food truck DIY irony Banksy.</p>

<h3 id="example-of-another-sub-heading-2">Example of another Sub-Heading 2</h3>

<p>Jean shorts raw denim Vice normcore, art party High Life PBR skateboard stumptown vinyl kitsch. Four loko meh 8-bit, tousled banh mi tilde forage Schlitz dreamcatcher twee 3 wolf moon. Chambray asymmetrical paleo salvia, sartorial umami four loko master cleanse drinking vinegar brunch. <a href="https://www.pinterest.com">Pinterest</a> DIY authentic Schlitz, hoodie Intelligentsia butcher trust fund brunch shabby chic Kickstarter forage flexitarian. Direct trade <a href="https://en.wikipedia.org/wiki/Cold-pressed_juice">cold-pressed</a> meggings stumptown plaid, pop-up taxidermy. Hoodie XOXO fingerstache scenester Echo Park. Plaid ugh Wes Anderson, freegan pug selvage fanny pack leggings pickled food truck DIY irony Banksy.</p>]]></content><author><name></name></author><category term="sample-posts" /><category term="toc" /><category term="sidebar" /><summary type="html"><![CDATA[an example of a blog post with table of contents on a sidebar]]></summary></entry><entry><title type="html">a post with audios</title><link href="squirogar.github.io/blog/2023/audios/" rel="alternate" type="text/html" title="a post with audios" /><published>2023-04-25T10:25:00+00:00</published><updated>2023-04-25T10:25:00+00:00</updated><id>squirogar.github.io/blog/2023/audios</id><content type="html" xml:base="squirogar.github.io/blog/2023/audios/"><![CDATA[<p>This is an example post with audios. It supports local audio files.</p>

<div class="row mt-3">
    <div class="col-sm mt-3 mt-md-0">
        <figure>

  <audio src="/assets/audio/epicaly-short-113909.mp3" controls="" />

</figure>

    </div>
    <div class="col-sm mt-3 mt-md-0">
        <figure>

  <audio src="https://cdn.pixabay.com/download/audio/2022/06/25/audio_69a61cd6d6.mp3" controls="" />

</figure>

    </div>
</div>
<div class="caption">
    A simple, elegant caption looks good between video rows, after each row, or doesn't have to be there at all.
</div>]]></content><author><name></name></author><category term="sample-posts" /><category term="including" /><category term="audios" /><summary type="html"><![CDATA[this is what included audios could look like]]></summary></entry><entry><title type="html">a post with videos</title><link href="squirogar.github.io/blog/2023/videos/" rel="alternate" type="text/html" title="a post with videos" /><published>2023-04-24T21:01:00+00:00</published><updated>2023-04-24T21:01:00+00:00</updated><id>squirogar.github.io/blog/2023/videos</id><content type="html" xml:base="squirogar.github.io/blog/2023/videos/"><![CDATA[<p>This is an example post with videos. It supports local video files.</p>

<div class="row mt-3">
    <div class="col-sm mt-3 mt-md-0">
        

<figure>

  

  <video src="/assets/video/pexels-engin-akyurt-6069112-960x540-30fps.mp4" class="img-fluid rounded z-depth-1" width="auto" height="auto" autoplay="" controls="" />

  

</figure>

    </div>
    <div class="col-sm mt-3 mt-md-0">
        

<figure>

  

  <video src="/assets/video/pexels-engin-akyurt-6069112-960x540-30fps.mp4" class="img-fluid rounded z-depth-1" width="auto" height="auto" controls="" />

  

</figure>

    </div>
</div>
<div class="caption">
    A simple, elegant caption looks good between video rows, after each row, or doesn't have to be there at all.
</div>

<p>It does also support embedding videos from different sources. Here are some examples:</p>

<div class="row mt-3">
    <div class="col-sm mt-3 mt-md-0">
        

<figure>

  

  <iframe src="https://www.youtube.com/embed/jNQXAC9IVRw" class="img-fluid rounded z-depth-1" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share" allowfullscreen="" width="auto" height="auto" />

  

</figure>

    </div>
    <div class="col-sm mt-3 mt-md-0">
        

<figure>

  

  <iframe src="https://player.vimeo.com/video/524933864?h=1ac4fd9fb4&amp;title=0&amp;byline=0&amp;portrait=0" class="img-fluid rounded z-depth-1" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share" allowfullscreen="" width="auto" height="auto" />

  

</figure>

    </div>
</div>]]></content><author><name></name></author><category term="sample-posts" /><category term="including" /><category term="videos" /><summary type="html"><![CDATA[this is what included videos could look like]]></summary></entry><entry><title type="html">displaying beautiful tables with Bootstrap Tables</title><link href="squirogar.github.io/blog/2023/tables/" rel="alternate" type="text/html" title="displaying beautiful tables with Bootstrap Tables" /><published>2023-03-20T18:37:00+00:00</published><updated>2023-03-20T18:37:00+00:00</updated><id>squirogar.github.io/blog/2023/tables</id><content type="html" xml:base="squirogar.github.io/blog/2023/tables/"><![CDATA[<p>Using markdown to display tables is easy. Just use the following syntax:</p>

<div class="language-markdown highlighter-rouge"><div class="highlight"><pre class="highlight"><code>| Left aligned | Center aligned | Right aligned |
| :----------- | :------------: | ------------: |
| Left 1       | center 1       | right 1       |
| Left 2       | center 2       | right 2       |
| Left 3       | center 3       | right 3       |
</code></pre></div></div>

<p>That will generate:</p>

<table>
  <thead>
    <tr>
      <th style="text-align: left">Left aligned</th>
      <th style="text-align: center">Center aligned</th>
      <th style="text-align: right">Right aligned</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td style="text-align: left">Left 1</td>
      <td style="text-align: center">center 1</td>
      <td style="text-align: right">right 1</td>
    </tr>
    <tr>
      <td style="text-align: left">Left 2</td>
      <td style="text-align: center">center 2</td>
      <td style="text-align: right">right 2</td>
    </tr>
    <tr>
      <td style="text-align: left">Left 3</td>
      <td style="text-align: center">center 3</td>
      <td style="text-align: right">right 3</td>
    </tr>
  </tbody>
</table>

<p></p>

<p>It is also possible to use HTML to display tables. For example, the following HTML code will display a table with <a href="https://bootstrap-table.com/">Bootstrap Table</a>, loaded from a JSON file:</p>

<div class="language-html highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="nt">&lt;table</span>
  <span class="na">id=</span><span class="s">"table"</span>
  <span class="na">data-toggle=</span><span class="s">"table"</span>
  <span class="na">data-url=</span><span class="s">"{{ '/assets/json/table_data.json' | relative_url }}"</span><span class="nt">&gt;</span>
  <span class="nt">&lt;thead&gt;</span>
    <span class="nt">&lt;tr&gt;</span>
      <span class="nt">&lt;th</span> <span class="na">data-field=</span><span class="s">"id"</span><span class="nt">&gt;</span>ID<span class="nt">&lt;/th&gt;</span>
      <span class="nt">&lt;th</span> <span class="na">data-field=</span><span class="s">"name"</span><span class="nt">&gt;</span>Item Name<span class="nt">&lt;/th&gt;</span>
      <span class="nt">&lt;th</span> <span class="na">data-field=</span><span class="s">"price"</span><span class="nt">&gt;</span>Item Price<span class="nt">&lt;/th&gt;</span>
    <span class="nt">&lt;/tr&gt;</span>
  <span class="nt">&lt;/thead&gt;</span>
<span class="nt">&lt;/table&gt;</span>
</code></pre></div></div>

<table data-toggle="table" data-url="/assets/json/table_data.json">
  <thead>
    <tr>
      <th data-field="id">ID</th>
      <th data-field="name">Item Name</th>
      <th data-field="price">Item Price</th>
    </tr>
  </thead>
</table>

<p></p>

<p>By using <a href="https://bootstrap-table.com/">Bootstrap Table</a> it is possible to create pretty complex tables, with pagination, search, and more. For example, the following HTML code will display a table, loaded from a JSON file, with pagination, search, checkboxes, and header/content alignment. For more information, check the <a href="https://examples.bootstrap-table.com/index.html">documentation</a>.</p>

<div class="language-html highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="nt">&lt;table</span>
  <span class="na">data-click-to-select=</span><span class="s">"true"</span>
  <span class="na">data-height=</span><span class="s">"460"</span>
  <span class="na">data-pagination=</span><span class="s">"true"</span>
  <span class="na">data-search=</span><span class="s">"true"</span>
  <span class="na">data-toggle=</span><span class="s">"table"</span>
  <span class="na">data-url=</span><span class="s">"{{ '/assets/json/table_data.json' | relative_url }}"</span><span class="nt">&gt;</span>
  <span class="nt">&lt;thead&gt;</span>
    <span class="nt">&lt;tr&gt;</span>
      <span class="nt">&lt;th</span> <span class="na">data-checkbox=</span><span class="s">"true"</span><span class="nt">&gt;&lt;/th&gt;</span>
      <span class="nt">&lt;th</span> <span class="na">data-field=</span><span class="s">"id"</span> <span class="na">data-halign=</span><span class="s">"left"</span> <span class="na">data-align=</span><span class="s">"center"</span> <span class="na">data-sortable=</span><span class="s">"true"</span><span class="nt">&gt;</span>ID<span class="nt">&lt;/th&gt;</span>
      <span class="nt">&lt;th</span> <span class="na">data-field=</span><span class="s">"name"</span> <span class="na">data-halign=</span><span class="s">"center"</span> <span class="na">data-align=</span><span class="s">"right"</span> <span class="na">data-sortable=</span><span class="s">"true"</span><span class="nt">&gt;</span>Item Name<span class="nt">&lt;/th&gt;</span>
      <span class="nt">&lt;th</span> <span class="na">data-field=</span><span class="s">"price"</span> <span class="na">data-halign=</span><span class="s">"right"</span> <span class="na">data-align=</span><span class="s">"left"</span> <span class="na">data-sortable=</span><span class="s">"true"</span><span class="nt">&gt;</span>Item Price<span class="nt">&lt;/th&gt;</span>
    <span class="nt">&lt;/tr&gt;</span>
  <span class="nt">&lt;/thead&gt;</span>
<span class="nt">&lt;/table&gt;</span>
</code></pre></div></div>

<table data-click-to-select="true" data-height="460" data-pagination="true" data-search="true" data-toggle="table" data-url="/assets/json/table_data.json">
  <thead>
    <tr>
      <th data-checkbox="true"></th>
      <th data-field="id" data-halign="left" data-align="center" data-sortable="true">ID</th>
      <th data-field="name" data-halign="center" data-align="right" data-sortable="true">Item Name</th>
      <th data-field="price" data-halign="right" data-align="left" data-sortable="true">Item Price</th>
    </tr>
  </thead>
</table>]]></content><author><name></name></author><category term="sample-posts" /><summary type="html"><![CDATA[an example of how to use Bootstrap Tables]]></summary></entry><entry><title type="html">a post with table of contents</title><link href="squirogar.github.io/blog/2023/table-of-contents/" rel="alternate" type="text/html" title="a post with table of contents" /><published>2023-03-20T15:59:00+00:00</published><updated>2023-03-20T15:59:00+00:00</updated><id>squirogar.github.io/blog/2023/table-of-contents</id><content type="html" xml:base="squirogar.github.io/blog/2023/table-of-contents/"><![CDATA[<p>This post shows how to add a table of contents in the beginning of the post.</p>

<h2 id="adding-a-table-of-contents">Adding a Table of Contents</h2>

<p>To add a table of contents to a post, simply add</p>
<div class="language-yml highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="na">toc</span><span class="pi">:</span>
  <span class="na">beginning</span><span class="pi">:</span> <span class="kc">true</span>
</code></pre></div></div>
<p>to the front matter of the post. The table of contents will be automatically generated from the headings in the post.</p>

<h3 id="example-of-sub-heading-1">Example of Sub-Heading 1</h3>

<p>Jean shorts raw denim Vice normcore, art party High Life PBR skateboard stumptown vinyl kitsch. Four loko meh 8-bit, tousled banh mi tilde forage Schlitz dreamcatcher twee 3 wolf moon. Chambray asymmetrical paleo salvia, sartorial umami four loko master cleanse drinking vinegar brunch. <a href="https://www.pinterest.com">Pinterest</a> DIY authentic Schlitz, hoodie Intelligentsia butcher trust fund brunch shabby chic Kickstarter forage flexitarian. Direct trade <a href="https://en.wikipedia.org/wiki/Cold-pressed_juice">cold-pressed</a> meggings stumptown plaid, pop-up taxidermy. Hoodie XOXO fingerstache scenester Echo Park. Plaid ugh Wes Anderson, freegan pug selvage fanny pack leggings pickled food truck DIY irony Banksy.</p>

<h3 id="example-of-another-sub-heading-1">Example of another Sub-Heading 1</h3>

<p>Jean shorts raw denim Vice normcore, art party High Life PBR skateboard stumptown vinyl kitsch. Four loko meh 8-bit, tousled banh mi tilde forage Schlitz dreamcatcher twee 3 wolf moon. Chambray asymmetrical paleo salvia, sartorial umami four loko master cleanse drinking vinegar brunch. <a href="https://www.pinterest.com">Pinterest</a> DIY authentic Schlitz, hoodie Intelligentsia butcher trust fund brunch shabby chic Kickstarter forage flexitarian. Direct trade <a href="https://en.wikipedia.org/wiki/Cold-pressed_juice">cold-pressed</a> meggings stumptown plaid, pop-up taxidermy. Hoodie XOXO fingerstache scenester Echo Park. Plaid ugh Wes Anderson, freegan pug selvage fanny pack leggings pickled food truck DIY irony Banksy.</p>

<h2 id="table-of-contents-options">Table of Contents Options</h2>

<p>If you want to learn more about how to customize the table of contents, you can check the <a href="https://github.com/toshimaru/jekyll-toc">jekyll-toc</a> repository.</p>

<h3 id="example-of-sub-heading-2">Example of Sub-Heading 2</h3>

<p>Jean shorts raw denim Vice normcore, art party High Life PBR skateboard stumptown vinyl kitsch. Four loko meh 8-bit, tousled banh mi tilde forage Schlitz dreamcatcher twee 3 wolf moon. Chambray asymmetrical paleo salvia, sartorial umami four loko master cleanse drinking vinegar brunch. <a href="https://www.pinterest.com">Pinterest</a> DIY authentic Schlitz, hoodie Intelligentsia butcher trust fund brunch shabby chic Kickstarter forage flexitarian. Direct trade <a href="https://en.wikipedia.org/wiki/Cold-pressed_juice">cold-pressed</a> meggings stumptown plaid, pop-up taxidermy. Hoodie XOXO fingerstache scenester Echo Park. Plaid ugh Wes Anderson, freegan pug selvage fanny pack leggings pickled food truck DIY irony Banksy.</p>

<h3 id="example-of-another-sub-heading-2">Example of another Sub-Heading 2</h3>

<p>Jean shorts raw denim Vice normcore, art party High Life PBR skateboard stumptown vinyl kitsch. Four loko meh 8-bit, tousled banh mi tilde forage Schlitz dreamcatcher twee 3 wolf moon. Chambray asymmetrical paleo salvia, sartorial umami four loko master cleanse drinking vinegar brunch. <a href="https://www.pinterest.com">Pinterest</a> DIY authentic Schlitz, hoodie Intelligentsia butcher trust fund brunch shabby chic Kickstarter forage flexitarian. Direct trade <a href="https://en.wikipedia.org/wiki/Cold-pressed_juice">cold-pressed</a> meggings stumptown plaid, pop-up taxidermy. Hoodie XOXO fingerstache scenester Echo Park. Plaid ugh Wes Anderson, freegan pug selvage fanny pack leggings pickled food truck DIY irony Banksy.</p>]]></content><author><name></name></author><category term="sample-posts" /><category term="toc" /><summary type="html"><![CDATA[an example of a blog post with table of contents]]></summary></entry><entry><title type="html">a post with giscus comments</title><link href="squirogar.github.io/blog/2022/giscus-comments/" rel="alternate" type="text/html" title="a post with giscus comments" /><published>2022-12-10T15:59:00+00:00</published><updated>2022-12-10T15:59:00+00:00</updated><id>squirogar.github.io/blog/2022/giscus-comments</id><content type="html" xml:base="squirogar.github.io/blog/2022/giscus-comments/"><![CDATA[<p>This post shows how to add GISCUS comments.</p>]]></content><author><name></name></author><category term="sample-posts" /><category term="external-services" /><summary type="html"><![CDATA[an example of a blog post with giscus comments]]></summary></entry><entry><title type="html">Displaying External Posts on Your al-folio Blog</title><link href="squirogar.github.io/blog/2022/displaying-external-posts-on-your-al-folio-blog/" rel="alternate" type="text/html" title="Displaying External Posts on Your al-folio Blog" /><published>2022-04-23T23:20:09+00:00</published><updated>2022-04-23T23:20:09+00:00</updated><id>squirogar.github.io/blog/2022/displaying-external-posts-on-your-al-folio-blog</id><content type="html" xml:base="squirogar.github.io/blog/2022/displaying-external-posts-on-your-al-folio-blog/"><![CDATA[]]></content><author><name></name></author></entry><entry><title type="html">a post with redirect</title><link href="squirogar.github.io/blog/2022/redirect/" rel="alternate" type="text/html" title="a post with redirect" /><published>2022-02-01T17:39:00+00:00</published><updated>2022-02-01T17:39:00+00:00</updated><id>squirogar.github.io/blog/2022/redirect</id><content type="html" xml:base="squirogar.github.io/blog/2022/redirect/"><![CDATA[<p>Redirecting to another page.</p>]]></content><author><name></name></author><summary type="html"><![CDATA[you can also redirect to assets like pdf]]></summary></entry></feed>